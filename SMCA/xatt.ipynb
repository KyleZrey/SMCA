{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for module imports\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import custom modules\n",
    "from modules.cross_attention import CrossAttention\n",
    "from modules.cross_attentionb import CrossAttentionB\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # # Print missing files after checking all\n",
    "        # if self.missing_files:\n",
    "        #     print(\"Missing files:\")\n",
    "        #     for item in self.missing_files:\n",
    "        #         print(f\"IMDBid: {item['IMDBid']}\")\n",
    "        #     print(f\"Total IMDB IDs with missing files: {len(self.missing_files)}\")\n",
    "        # else:\n",
    "        #     print(\"No missing files.\")\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 0, 'green': 1} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "        \n",
    "        # Debugging output\n",
    "        if label_data.shape[0] == 0:\n",
    "            print(f\"Empty target for IMDBid {imdbid} at index {idx}\")\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Text'\n",
    "audio_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Audio'\n",
    "video_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Video'\n",
    "\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMCA Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 1 of SMCA\n",
    "def SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttention(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 2 of SMCA - Model A: Stage 1 Output as Query\n",
    "def SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttention(modalityAlphaBeta.shape[-1], modalityGamma.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    multimodal_representation = cross_attn(modalityAlphaBeta, modalityGamma)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMCAModelA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelA(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v, device):\n",
    "        super(SMCAModelA, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma, device):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityAlphaBeta (as query) and modalityGamma (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Flatten the output\n",
    "        return torch.flatten(multimodal_representation, start_dim=1)  # Flatten all dimensions except batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "\n",
    "# # Stage 2 of SMCA - Model B: Stage 1 Output as Key-Value\n",
    "# def SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v, device):\n",
    "#     cross_attn = CrossAttention(modalityGamma.shape[-1], modalityAlphaBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "#     multimodal_representation = cross_attn(modalityGamma, modalityAlphaBeta)\n",
    "#     return multimodal_representation\n",
    "\n",
    "def SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(\n",
    "        d_in_query=modalityGamma.shape[-1],\n",
    "        d_in_kv=modalityAlphaBeta.shape[-1],\n",
    "        d_out_kq=d_out_kq,\n",
    "        d_out_v=d_out_v\n",
    "    ).to(device)\n",
    "    multimodal_representation = cross_attn(modalityGamma, modalityAlphaBeta)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelB(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v, device):\n",
    "        super(SMCAModelB, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma, device):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityGamma (as query) and modalityAlphaBeta (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Flatten the output\n",
    "        return torch.flatten(multimodal_representation, start_dim=1)  # Flatten all dimensions except batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model (For Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "--------------------------------------------------\n",
      "SMCA Output Shape: torch.Size([8, 276480])\n"
     ]
    }
   ],
   "source": [
    "# Test the SMCA model using the items from dataloader as input\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Determine the output dimensions\n",
    "d_out_kq = 512\n",
    "d_out_v = 256\n",
    "\n",
    "# Initialize the SMCA model A\n",
    "model = SMCAModelB(d_out_kq, d_out_v, device)\n",
    "model.to(device)\n",
    "\n",
    "# Use DataLoader to get a batch of data\n",
    "for batch in train_dataloader:  # You can use any DataLoader (train_dataloader, val_dataloader, etc.)\n",
    "    text_data, audio_data, video_data, labels = batch\n",
    "    \n",
    "    text_data = text_data.to(device)\n",
    "    audio_data = audio_data.to(device)\n",
    "    video_data = video_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Feed the entire batch to the GMU model\n",
    "    with torch.no_grad():\n",
    "        output = model(modalityAlpha=audio_data, modalityBeta=text_data, modalityGamma=video_data, device=device)\n",
    "        \n",
    "    # Print the output shape\n",
    "    print('-'*50)\n",
    "    print(\"SMCA Output Shape:\", output.shape)    \n",
    "    # Break after the first batch for testing purposes\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:\n",
      "Text file: tt0021814.npy\n",
      "Audio file: feature_tt0021814.npy\n",
      "Video file: tt0021814_features.npy\n",
      "--------------------------------------------------\n",
      "Text Feature Shape: torch.Size([1, 1024])\n",
      "Audio Feature Shape: torch.Size([1, 197, 768])\n",
      "Video Feature Shape: torch.Size([1, 95, 768])\n",
      "--------------------------------------------------\n",
      "Model output shape: torch.Size([1, 50432]) ###[batch_size, output_dim]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the SMCA model using a single file from each feature directory\n",
    "\n",
    "from modules.dataloader import load_npy_files\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Text'\n",
    "audio_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Audio'\n",
    "video_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Video'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Select the first file from each modality directories (for testing)\n",
    "text_file_name, text_features = text_features[index]\n",
    "audio_file_name, audio_features = audio_features[index]\n",
    "video_file_name, video_features = video_features[index]\n",
    "\n",
    "print(\"Selected File:\")\n",
    "print(\"Text file:\", os.path.basename(text_file_name))\n",
    "print(\"Audio file:\", os.path.basename(audio_file_name))\n",
    "print(\"Video file:\", os.path.basename(video_file_name))\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Determine the output dimensions\n",
    "d_out_kq = 512 \n",
    "d_out_v = 256\n",
    "\n",
    "# Initialize the SMCA model A\n",
    "model = SMCAModelB(d_out_kq, d_out_v, device)\n",
    "\n",
    "# Move model to the same device as your data (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Reshape features\n",
    "video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "print(\"Text Feature Shape:\", text_features.shape)\n",
    "print(\"Audio Feature Shape:\", audio_features.shape)\n",
    "print(\"Video Feature Shape:\", video_features.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Pass the data through the SMCA model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    output = model(modalityAlpha=text_features.to(device), modalityBeta=video_features.to(device), modalityGamma=audio_features.to(device), device=device)\n",
    "\n",
    "# Print the output shape and the output itself\n",
    "print(\"Model output shape:\", output.shape, \"###[batch_size, output_dim]\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected File Names:\n",
      "Audio file: D:\\\\Projects\\\\Thesis\\\\Audio\\feature_tt0021814.npy\n",
      "Video file: D:\\\\Projects\\\\Thesis\\\\Video\\tt0021814_features.npy\n",
      "Text file: D:\\\\Projects\\\\Thesis\\\\Text\\tt0021814.npy\n",
      "Modality Alpha Shape: torch.Size([1, 197, 768])\n",
      "Modality Beta Shape: torch.Size([1, 768])\n",
      "Modality Gamma Shape: torch.Size([1, 95, 768])\n",
      "Stage 1 Bimodal Representation Shape: torch.Size([1, 197, 768])\n",
      "Final Multimodal Representation (Model A) Shape: torch.Size([1, 197, 768])\n",
      "Final Multimodal Representation (Model B) Shape: torch.Size([1, 95, 768])\n"
     ]
    }
   ],
   "source": [
    "# Original Sample Test for the SMCA  (Non-Classes)\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load .npy files\n",
    "    video_features = load_npy_files(r'D:\\\\Projects\\\\Thesis\\\\Video')\n",
    "    audio_features = load_npy_files(r'D:\\\\Projects\\\\Thesis\\\\Audio')\n",
    "    text_features = load_npy_files(r'D:\\\\Projects\\\\Thesis\\\\Text')\n",
    "    \n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    text_file_name, text_features = text_features[0]\n",
    "\n",
    "    # Print the file names\n",
    "    print(\"\\nSelected File Names:\")\n",
    "    print(\"Audio file:\", audio_file_name)\n",
    "    print(\"Video file:\", video_file_name)\n",
    "    print(\"Text file:\", text_file_name)\n",
    "    \n",
    "    # Reshape features\n",
    "    video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    # # Randomize assignment of Alpha, Beta, Gamma\n",
    "    # modalityAlpha, modalityBeta, modalityGamma = randomize_modalities(audio_features, video_features, text_features)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Manual assignment of modalities\n",
    "    modalityAlpha = audio_features.to(device)\n",
    "    modalityBeta = text_features.to(device)\n",
    "    modalityGamma = video_features.to(device)\n",
    "\n",
    "    # Apply linear transformation to match dimensions\n",
    "    linear_transform_Alpha = LinearTransformations(modalityAlpha.shape[-1], 768).to(device)\n",
    "    linear_transform_Beta = LinearTransformations(modalityBeta.shape[-1], 768).to(device)\n",
    "    linear_transform_Gamma = LinearTransformations(modalityGamma.shape[-1], 768).to(device)\n",
    "\n",
    "    modalityAlpha = linear_transform_Alpha(modalityAlpha)\n",
    "    modalityBeta = linear_transform_Beta(modalityBeta)\n",
    "    modalityGamma = linear_transform_Gamma(modalityGamma)\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    d_out_kq = 768  # Final transformed dimension\n",
    "    d_out_v = 768\n",
    "\n",
    "    # Stage 1: Bimodal Representation\n",
    "    modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v, device)\n",
    "    \n",
    "    # Stage 2, Model A: Multimodal Representation (using AlphaBeta as Query)\n",
    "    final_representation_A = SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v, device)\n",
    "    \n",
    "    # Stage 2, Model B: Multimodal Representation (using AlphaBeta as Key-Value)\n",
    "    final_representation_B = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v, device)\n",
    "\n",
    "    print(\"Modality Alpha Shape:\", modalityAlpha.shape)\n",
    "    print(\"Modality Beta Shape:\", modalityBeta.shape)\n",
    "    print(\"Modality Gamma Shape:\", modalityGamma.shape)\n",
    "    print(\"Stage 1 Bimodal Representation Shape:\", modalityAlphaBeta.shape)\n",
    "    print(\"Final Multimodal Representation (Model A) Shape:\", final_representation_A.shape)\n",
    "    print(\"Final Multimodal Representation (Model B) Shape:\", final_representation_B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-4):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dense_layer, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    dense_layer.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device).view(-1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass inputs through SMCA model\n",
    "        \n",
    "        outputs = model(modalityAlpha=text_features, modalityBeta=audio_features, modalityGamma=video_features, device=device)\n",
    "        # outputs = model(modalityAlpha=text_features, modalityBeta=video_features, modalityGamma=audio_features, device=device)\n",
    "        # outputs = model(modalityAlpha=audio_features, modalityBeta=text_features, modalityGamma=video_features, device=device)\n",
    "        # outputs = model(modalityAlpha=audio_features, modalityBeta=video_features, modalityGamma=text_features, device=device)\n",
    "        # outputs = model(modalityAlpha=video_features, modalityBeta=audio_features, modalityGamma=text_features, device=device)\n",
    "        # outputs = model(modalityAlpha=video_features, modalityBeta=text_features, modalityGamma=audio_features, device=device)\n",
    "        \n",
    "        # Check if padding is necessary\n",
    "        output_size = outputs.size(1)\n",
    "        dense_input_size = dense_layer.fc.in_features\n",
    "        \n",
    "        if output_size < dense_input_size:\n",
    "            # Pad the outputs if they are smaller than the expected size for the dense layer\n",
    "            padding_size = dense_input_size - output_size\n",
    "            # Pad on the second dimension (feature dimension)\n",
    "            outputs = torch.nn.functional.pad(outputs, (0, padding_size))\n",
    "        elif output_size > dense_input_size:\n",
    "            # In case outputs are larger (though unlikely, we trim)\n",
    "            outputs = outputs[:, :dense_input_size]\n",
    "        \n",
    "        # Pass the fused features through the dense layer\n",
    "        predictions = dense_layer(outputs).view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Pass inputs through SMCA model\n",
    "        \n",
    "            outputs = model(modalityAlpha=text_features, modalityBeta=audio_features, modalityGamma=video_features, device=device)\n",
    "            # outputs = model(modalityAlpha=text_features, modalityBeta=video_features, modalityGamma=audio_features, device=device)\n",
    "            # outputs = model(modalityAlpha=audio_features, modalityBeta=text_features, modalityGamma=video_features, device=device)\n",
    "            # outputs = model(modalityAlpha=audio_features, modalityBeta=video_features, modalityGamma=text_features, device=device)\n",
    "            # outputs = model(modalityAlpha=video_features, modalityBeta=audio_features, modalityGamma=text_features, device=device)\n",
    "            # outputs = model(modalityAlpha=video_features, modalityBeta=text_features, modalityGamma=audio_features, device=device)\n",
    "\n",
    "            # Check if padding is necessary\n",
    "            output_size = outputs.size(1)\n",
    "            dense_input_size = dense_layer.fc.in_features\n",
    "            \n",
    "            if output_size < dense_input_size:\n",
    "                # Pad the outputs if they are smaller than the expected size for the dense layer\n",
    "                padding_size = dense_input_size - output_size\n",
    "                # Pad on the second dimension (feature dimension)\n",
    "                outputs = torch.nn.functional.pad(outputs, (0, padding_size))\n",
    "            elif output_size > dense_input_size:\n",
    "                # In case outputs are larger (though unlikely, we trim)\n",
    "                outputs = outputs[:, :dense_input_size]\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1) \n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Evaluation Loss: {average_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return average_loss, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Pass inputs through SMCA model\n",
    "            outputs = model(modalityAlpha=text_features.to(device), modalityBeta=audio_features.to(device), modalityGamma=video_features.to(device), device=device)\n",
    "\n",
    "            # Check if padding is necessary\n",
    "            output_size = outputs.size(1)\n",
    "            dense_input_size = dense_layer.fc.in_features\n",
    "            \n",
    "            if output_size < dense_input_size:\n",
    "                # Pad the outputs if they are smaller than the expected size for the dense layer\n",
    "                padding_size = dense_input_size - output_size\n",
    "                # Pad on the second dimension (feature dimension)\n",
    "                outputs = torch.nn.functional.pad(outputs, (0, padding_size))\n",
    "            elif output_size > dense_input_size:\n",
    "                # In case outputs are larger (though unlikely, we trim)\n",
    "                outputs = outputs[:, :dense_input_size]\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = torch.sigmoid(dense_layer(outputs)).view(-1)  \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    return average_loss, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest output size from the SMCA model\n",
    "def find_largest_output_size(model, dataloader, device):\n",
    "    max_output_size = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            # Move features to device\n",
    "            text_features = text_features.to(device)\n",
    "            audio_features = audio_features.to(device)\n",
    "            video_features = video_features.to(device)\n",
    "\n",
    "            # Pass inputs through the SMCA model\n",
    "            outputs = model(\n",
    "                modalityAlpha=audio_features, \n",
    "                modalityBeta=text_features, \n",
    "                modalityGamma=video_features,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # Compare and store the maximum output size\n",
    "            if outputs.size(1) > max_output_size:\n",
    "                max_output_size = outputs.size(1)\n",
    "\n",
    "    return max_output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/10\n",
      "Evaluation Loss: 0.6329\n",
      "Precision: 0.7460\n",
      "Recall: 0.9724\n",
      "F1 Score: 0.8443\n",
      "Training Loss: 0.7268, Validation Loss: 0.6329\n",
      "------------------------------\n",
      "Epoch 2/10\n",
      "Evaluation Loss: 0.6759\n",
      "Precision: 0.7360\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8480\n",
      "Training Loss: 0.6602, Validation Loss: 0.6759\n",
      "------------------------------\n",
      "Epoch 3/10\n",
      "Evaluation Loss: 0.6916\n",
      "Precision: 0.7394\n",
      "Recall: 0.8414\n",
      "F1 Score: 0.7871\n",
      "Training Loss: 0.6358, Validation Loss: 0.6916\n",
      "------------------------------\n",
      "Epoch 4/10\n",
      "Evaluation Loss: 0.6592\n",
      "Precision: 0.7212\n",
      "Recall: 0.8207\n",
      "F1 Score: 0.7677\n",
      "Training Loss: 0.6235, Validation Loss: 0.6592\n",
      "------------------------------\n",
      "Epoch 5/10\n",
      "Evaluation Loss: 0.6339\n",
      "Precision: 0.7293\n",
      "Recall: 0.9103\n",
      "F1 Score: 0.8098\n",
      "Training Loss: 0.6419, Validation Loss: 0.6339\n",
      "------------------------------\n",
      "Epoch 6/10\n",
      "Evaluation Loss: 0.6463\n",
      "Precision: 0.7348\n",
      "Recall: 0.9172\n",
      "F1 Score: 0.8160\n",
      "Training Loss: 0.6483, Validation Loss: 0.6463\n",
      "------------------------------\n",
      "Epoch 7/10\n",
      "Evaluation Loss: 0.6298\n",
      "Precision: 0.7293\n",
      "Recall: 0.9103\n",
      "F1 Score: 0.8098\n",
      "Training Loss: 0.6313, Validation Loss: 0.6298\n",
      "------------------------------\n",
      "Epoch 8/10\n",
      "Evaluation Loss: 0.6853\n",
      "Precision: 0.7348\n",
      "Recall: 0.9172\n",
      "F1 Score: 0.8160\n",
      "Training Loss: 0.6538, Validation Loss: 0.6853\n",
      "------------------------------\n",
      "Epoch 9/10\n",
      "Evaluation Loss: 0.6448\n",
      "Precision: 0.7399\n",
      "Recall: 0.8828\n",
      "F1 Score: 0.8050\n",
      "Training Loss: 0.6527, Validation Loss: 0.6448\n",
      "------------------------------\n",
      "Epoch 10/10\n",
      "Evaluation Loss: 0.6200\n",
      "Precision: 0.7460\n",
      "Recall: 0.9724\n",
      "F1 Score: 0.8443\n",
      "Training Loss: 0.6523, Validation Loss: 0.6200\n",
      "------------------------------\n",
      "Testing the model on the test set...\n",
      "Test Loss: 0.5841\n",
      "Test Precision: 0.7313\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.8448\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelA(512, 256, device) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = find_largest_output_size(model, train_dataloader, device)\n",
    "    max_output_size_val = find_largest_output_size(model, val_dataloader, device)\n",
    "    max_output_size_test = find_largest_output_size(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=512).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(list(dense_layer.parameters())+ list(model.parameters()))  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/10\n",
      "Evaluation Loss: 1.3887\n",
      "Precision: 0.7667\n",
      "Recall: 0.3172\n",
      "F1 Score: 0.4488\n",
      "Training Loss: 1.1935, Validation Loss: 1.3887\n",
      "------------------------------\n",
      "Epoch 2/10\n",
      "Evaluation Loss: 0.7448\n",
      "Precision: 0.7626\n",
      "Recall: 0.7310\n",
      "F1 Score: 0.7465\n",
      "Training Loss: 1.4990, Validation Loss: 0.7448\n",
      "------------------------------\n",
      "Epoch 3/10\n",
      "Evaluation Loss: 1.2374\n",
      "Precision: 0.7525\n",
      "Recall: 0.5241\n",
      "F1 Score: 0.6179\n",
      "Training Loss: 1.2258, Validation Loss: 1.2374\n",
      "------------------------------\n",
      "Epoch 4/10\n",
      "Evaluation Loss: 1.0247\n",
      "Precision: 0.7586\n",
      "Recall: 0.6069\n",
      "F1 Score: 0.6743\n",
      "Training Loss: 1.2109, Validation Loss: 1.0247\n",
      "------------------------------\n",
      "Epoch 5/10\n",
      "Evaluation Loss: 0.9895\n",
      "Precision: 0.7007\n",
      "Recall: 0.7103\n",
      "F1 Score: 0.7055\n",
      "Training Loss: 1.1509, Validation Loss: 0.9895\n",
      "------------------------------\n",
      "Epoch 6/10\n",
      "Evaluation Loss: 1.0092\n",
      "Precision: 0.7706\n",
      "Recall: 0.5793\n",
      "F1 Score: 0.6614\n",
      "Training Loss: 1.0474, Validation Loss: 1.0092\n",
      "------------------------------\n",
      "Epoch 7/10\n",
      "Evaluation Loss: 1.1865\n",
      "Precision: 0.7564\n",
      "Recall: 0.4069\n",
      "F1 Score: 0.5291\n",
      "Training Loss: 1.0591, Validation Loss: 1.1865\n",
      "------------------------------\n",
      "Epoch 8/10\n",
      "Evaluation Loss: 1.2388\n",
      "Precision: 0.6897\n",
      "Recall: 0.4138\n",
      "F1 Score: 0.5172\n",
      "Training Loss: 1.0772, Validation Loss: 1.2388\n",
      "------------------------------\n",
      "Epoch 9/10\n",
      "Evaluation Loss: 0.9972\n",
      "Precision: 0.7215\n",
      "Recall: 0.3931\n",
      "F1 Score: 0.5089\n",
      "Training Loss: 1.1377, Validation Loss: 0.9972\n",
      "------------------------------\n",
      "Epoch 10/10\n",
      "Evaluation Loss: 0.9489\n",
      "Precision: 0.7244\n",
      "Recall: 0.6345\n",
      "F1 Score: 0.6765\n",
      "Training Loss: 1.2052, Validation Loss: 0.9489\n",
      "------------------------------\n",
      "Testing the model on the test set...\n",
      "Test Loss: 0.6184\n",
      "Test Precision: 0.7313\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.8448\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelB(512, 256, device) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = find_largest_output_size(model, train_dataloader, device)\n",
    "    max_output_size_val = find_largest_output_size(model, val_dataloader, device)\n",
    "    max_output_size_test = find_largest_output_size(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=512).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(dense_layer.parameters())  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
