{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for module imports\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import custom modules\n",
    "from modules.cross_attention import CrossAttention\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 0, 'green': 1} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "        \n",
    "        # Debugging output\n",
    "        if label_data.shape[0] == 0:\n",
    "            print(f\"Empty target for IMDBid {imdbid} at index {idx}\")\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMCA Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 1 of SMCA\n",
    "def SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v):\n",
    "    cross_attn = CrossAttention(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 2 of SMCA - Model A: Stage 1 Output as Query\n",
    "def SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v):\n",
    "    cross_attn = CrossAttention(modalityAlphaBeta.shape[-1], modalityGamma.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    multimodal_representation = cross_attn(modalityAlphaBeta, modalityGamma)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 2 of SMCA - Model B: Stage 1 Output as Key-Value\n",
    "def SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v):\n",
    "    cross_attn = CrossAttention(modalityGamma.shape[-1], modalityAlphaBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    multimodal_representation = cross_attn(modalityGamma, modalityAlphaBeta)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelA(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v):\n",
    "        super(SMCAModelA, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "    \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityAlphaBeta (as query) and modalityGamma (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, self.d_out_kq, self.d_out_v)\n",
    "        \n",
    "        # Apply global average pooling on the sequence dimension (e.g., dim=1 for sequence length)\n",
    "        return torch.mean(multimodal_representation, dim=1)  # [batch_size, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelB(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v):\n",
    "        super(SMCAModelB, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "    \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityGamma (as query) and modalityAlphaBeta (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, self.d_out_kq, self.d_out_v)\n",
    "        \n",
    "        # Apply global average pooling on the sequence dimension (e.g., dim=1 for sequence length)\n",
    "        return torch.mean(multimodal_representation, dim=1)  # [batch_size, feature_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model (For Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Text Feature Shape: torch.Size([8, 1024])\n",
      "Audio Feature Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Feature Shape: torch.Size([8, 524, 768])\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "SMCA Output Shape: torch.Size([8, 524, 768])\n",
      "SMCA Output:  tensor([[[-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         ...,\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181]],\n",
      "\n",
      "        [[-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         ...,\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181]],\n",
      "\n",
      "        [[-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         ...,\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         ...,\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181]],\n",
      "\n",
      "        [[-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         ...,\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181]],\n",
      "\n",
      "        [[-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         ...,\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181],\n",
      "         [-668.6876, -659.5363, -694.2010,  ..., -705.6362, -778.1246,\n",
      "          -674.3181]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the SMCA model using the items from dataloader as input\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Determine the output dimensions\n",
    "output_dim = 768\n",
    "\n",
    "# Initialize the SMCA model A\n",
    "model = SMCAModelB(output_dim, output_dim) # Dimension for d_out_kq and d_out_v\n",
    "model.to(device)  # Move the model to the correct device\n",
    "\n",
    "# Use DataLoader to get a batch of data\n",
    "for batch in train_dataloader:  # You can use any DataLoader (train_dataloader, val_dataloader, etc.)\n",
    "    text_data, audio_data, video_data, labels = batch\n",
    "    \n",
    "    text_data = text_data.to(device)\n",
    "    audio_data = audio_data.to(device)\n",
    "    video_data = video_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    print(\"Text Feature Shape:\", text_data.shape)\n",
    "    print(\"Audio Feature Shape:\", audio_data.shape)\n",
    "    print(\"Video Feature Shape:\", video_data.shape)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Feed the entire batch to the GMU model\n",
    "    with torch.no_grad():\n",
    "        output = model(modalityAlpha=audio_data, modalityBeta=text_data, modalityGamma=video_data)\n",
    "        \n",
    "    # Print the output shape\n",
    "    print('-'*50)\n",
    "    print(\"SMCA Output Shape:\", output.shape)\n",
    "    print(\"SMCA Output: \", output)\n",
    "    \n",
    "    # Break after the first batch for testing purposes\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:\n",
      "Text file: tt0082766.npy\n",
      "Audio file: feature_tt0082766.npy\n",
      "Video file: tt0082766_features.npy\n",
      "--------------------------------------------------\n",
      "Text Feature Shape: torch.Size([1, 1024])\n",
      "Audio Feature Shape: torch.Size([1, 197, 768])\n",
      "Video Feature Shape: torch.Size([1, 244, 768])\n",
      "--------------------------------------------------\n",
      "Model output shape: torch.Size([1, 768]) ###[batch_size, output_dim]\n",
      "--------------------------------------------------\n",
      "Model output: tensor([[ -7.5661,  -5.7842,  -7.2111,  -7.1210,  -6.5655,  -8.0369,  -6.7916,\n",
      "          -6.2260,  -6.3291,  -5.2037,  -3.2735,  -5.7887,  -7.7017,  -4.5259,\n",
      "          -6.4231,  -4.1904,  -5.8694,  -6.7259,  -6.5397,  -3.3662,  -5.1554,\n",
      "          -5.1319,  -6.3771,  -3.8463,  -5.5759,  -5.7581,  -6.1207,  -8.6624,\n",
      "          -5.0021,  -7.5447,  -2.3135,  -5.6687,  -3.4267,  -4.6449,  -7.8818,\n",
      "          -3.9250,  -7.2540,  -6.4369,  -5.2256,  -4.9827,  -6.6402,  -5.1723,\n",
      "          -5.5409,  -5.3132,  -6.4971,  -4.9146,  -6.6334,  -4.8258,  -3.6824,\n",
      "          -4.6984,  -5.3308,  -5.9229,  -8.5051,  -4.8940,  -8.2040,  -4.6388,\n",
      "          -6.1185,  -3.9144,  -5.0867,  -8.0892,  -6.1563,  -7.3603,  -4.8989,\n",
      "          -5.6438,  -6.4355,  -6.8627,  -5.0007,  -8.6842,  -6.0108,  -2.9340,\n",
      "          -7.3365,  -6.2989,  -5.1233,  -6.6517,  -6.3663,  -6.6534,  -8.6449,\n",
      "          -2.8768,  -7.6365,  -8.5708,  -5.0560,  -5.1035,  -7.5768,  -8.2977,\n",
      "          -6.0569,  -4.4433,  -6.0979,  -6.4853,  -5.8955,  -7.0865,  -5.5863,\n",
      "          -7.5954,  -5.3877,  -4.0249,  -6.8860,  -6.6991,  -5.4869,  -4.3415,\n",
      "          -7.2413,  -6.6483,  -5.9781,  -5.8842,  -4.9825,  -5.9736,  -7.3623,\n",
      "          -6.7127,  -3.1872,  -7.8850,  -5.4089,  -6.6208,  -5.1685,  -7.5604,\n",
      "          -6.5532,  -5.0398,  -4.1486,  -4.6760,  -7.2580,  -6.0583,  -6.1604,\n",
      "          -8.0917,  -5.7753,  -5.3732,  -4.6952,  -7.4056,  -5.0741,  -8.0848,\n",
      "          -7.3099,  -7.2510,  -6.3031,  -3.3108,  -5.6081,  -7.5341,  -5.8819,\n",
      "          -8.2587,  -6.5193,  -5.1636,  -4.2336,  -7.4579,  -6.3994,  -5.9011,\n",
      "          -5.4972,  -5.8483,  -6.7257,  -8.9005,  -5.5261,  -6.3086,  -3.5535,\n",
      "          -5.4599,  -4.3883,  -5.5782,  -6.3136,  -6.6488,  -5.2540,  -6.8815,\n",
      "          -5.5164,  -3.6483,  -6.7832,  -6.4638,  -2.7421,  -6.7898,  -4.9106,\n",
      "          -5.4890,  -6.5458,  -8.4863,  -7.1327,  -6.3502,  -5.1354,  -5.1353,\n",
      "          -7.2738,  -4.9614,  -6.7405,  -7.5610,  -3.8120,  -6.4716,  -5.5872,\n",
      "          -4.7304,  -4.9573,  -5.3022,  -5.9422,  -6.4025,  -6.4826,  -4.8574,\n",
      "          -8.0251,  -6.1989,  -3.3816,  -3.7390,  -8.2848,  -6.3982,  -3.8959,\n",
      "          -5.5126,  -5.6472,  -5.7998,  -5.9733,  -6.1328,  -8.4630,  -6.8577,\n",
      "          -5.9593,  -6.7240,  -7.2676,  -6.2719,  -6.9344,  -7.6948,  -7.0972,\n",
      "          -6.5539,  -5.5464,  -6.6744,  -4.1106,  -7.2406,  -2.8172,  -5.7492,\n",
      "          -4.4465,  -6.9025,  -6.5158,  -6.2676,  -6.3226,  -6.3840,  -5.9707,\n",
      "          -6.4966,  -5.3116,  -5.5911,  -4.1974,  -7.1845,  -5.0993,  -7.3454,\n",
      "          -6.4028,  -5.3677,  -5.5908,  -5.9106,  -8.4637,  -6.4509,  -7.8267,\n",
      "          -6.6199,  -9.1560,  -5.9867,  -5.6440,  -5.8375,  -6.3279,  -5.8962,\n",
      "          -6.2738,  -4.1915,  -4.3173,  -7.2886,  -7.2451,  -6.3396,  -7.1575,\n",
      "          -4.2487,  -6.1369,  -8.9992,  -5.1790,  -4.3571,  -3.9029,  -6.8334,\n",
      "          -5.1710,  -3.2930,  -8.0967,  -8.1618,  -4.7950,  -7.6694,  -8.6169,\n",
      "          -5.9969,  -6.5966,  -6.2769,  -2.8097,  -5.8769,  -2.2655,  -8.3756,\n",
      "          -6.5502,  -5.7161,  -4.5943,  -5.9632,  -6.6191,  -5.3866,  -4.7311,\n",
      "          -6.4771,  -8.4265,  -6.5242,  -4.6690,  -5.6756,  -5.6359,  -4.5619,\n",
      "          -4.5515,  -8.1582,  -8.5334,  -6.6823,  -4.8430,  -8.6319,  -5.2616,\n",
      "          -6.2904,  -5.7880,  -4.5319,  -4.0357,  -5.2711,  -7.4877,  -8.7315,\n",
      "          -8.7522,  -6.4382,  -4.4250,  -5.3183,  -7.5209,  -5.5471,  -4.1520,\n",
      "          -7.9784,  -5.9315,  -4.2776,  -5.5902,  -5.3590,  -8.3457,  -6.9079,\n",
      "          -5.4998,  -6.5152,  -7.0626,  -5.3290,  -6.3402,  -7.2650,  -5.9310,\n",
      "          -5.6595,  -7.6142,  -4.8526,  -6.6308,  -8.4856,  -5.6745,  -5.7491,\n",
      "          -6.9077,  -7.2712,  -4.5013,  -6.3578,  -5.0389,  -7.5399,  -7.1408,\n",
      "          -7.0064,  -5.3832,  -5.4760,  -5.5178,  -4.0744,  -4.6244,  -9.9091,\n",
      "          -6.1866,  -6.2211,  -8.2598,  -5.7158,  -6.6195,  -3.4342,  -7.3062,\n",
      "          -5.8199,  -5.0541,  -5.0463,  -4.6125,  -5.9374,  -6.9358,  -5.0773,\n",
      "          -3.6968,  -8.0117,  -4.9434,  -5.5009,  -5.1612,  -7.8422,  -8.7614,\n",
      "          -5.3669,  -5.7670,  -7.2925,  -6.4067,  -2.6816,  -8.2460,  -5.3493,\n",
      "          -5.6837,  -5.7430,  -4.3332,  -6.7254,  -2.1490,  -8.1460,  -5.4682,\n",
      "          -8.0595,  -4.6103,  -8.3118,  -6.2243,  -7.9142,  -2.6687,  -4.9279,\n",
      "          -8.2843,  -8.0880,  -4.4075,  -6.9511,  -9.2550,  -4.9376,  -6.5671,\n",
      "          -6.1537,  -7.2541,  -3.3789,  -6.5167,  -4.1626,  -5.7973,  -6.9020,\n",
      "          -6.4008,  -7.8534,  -8.2707,  -5.6961,  -7.0601,  -6.5446,  -6.1471,\n",
      "          -4.4645,  -6.8621,  -5.0530,  -5.6669,  -6.5035,  -6.9263,  -4.2485,\n",
      "          -5.5288,  -7.1682,  -5.6022,  -4.2935,  -7.4053,  -6.5975,  -4.8677,\n",
      "          -3.8873,  -5.8348,  -7.2611,  -5.0903,  -6.6986,  -4.3389,  -3.8376,\n",
      "          -5.8661,  -7.2213,  -6.2453,  -5.2631,  -5.6423,  -7.0331,  -5.0511,\n",
      "          -6.1118,  -4.5056,  -6.7108,  -6.2332,  -6.2034,  -4.8245,  -5.5637,\n",
      "          -4.9141,  -6.2226,  -5.4699,  -5.9238,  -5.1710,  -5.3570,  -6.3066,\n",
      "          -5.5599,  -6.3535,  -7.4293,  -7.3915,  -7.6041,  -5.7090,  -6.6038,\n",
      "          -5.1313,  -6.1175,  -4.8599,  -5.0605, -10.1595,  -5.9673,  -5.4143,\n",
      "          -6.1788,  -6.0468,  -6.0335,  -7.1214,  -7.0168,  -5.1741,  -7.7203,\n",
      "          -8.0394,  -6.5886,  -4.9454,  -4.4993,  -5.5310,  -6.4365,  -8.1775,\n",
      "          -5.2172,  -7.1501,  -6.7765,  -5.9799,  -6.5398,  -4.8723,  -5.4346,\n",
      "          -7.7863,  -7.6282,  -7.1275,  -6.0309,  -6.5014,  -3.3167,  -3.6878,\n",
      "          -7.9806,  -6.6792,  -4.8236,  -6.7645,  -6.3724,  -7.2077,  -4.9151,\n",
      "          -3.8931,  -4.2730,  -4.7459,  -4.4136,  -7.0008,  -4.9575,  -4.4838,\n",
      "          -4.0391,  -7.3648,  -5.2424,  -5.1974,  -8.2388,  -6.7394,  -5.0438,\n",
      "          -5.8178,  -8.0689,  -5.6400,  -7.3580,  -5.0456,  -7.3261,  -6.4587,\n",
      "          -3.5824,  -5.1480,  -4.2898,  -7.0408,  -8.2151, -10.0438,  -6.1643,\n",
      "          -5.4855,  -7.2213,  -5.9581,  -5.2740,  -6.1168,  -6.0329,  -8.2198,\n",
      "          -8.4816,  -5.5293,  -7.4408,  -8.9002,  -4.0529,  -4.4846,  -9.1689,\n",
      "          -5.3818,  -6.4819,  -6.2122,  -4.8300,  -3.9469,  -4.7883,  -6.3835,\n",
      "          -5.4623,  -6.8709,  -5.2678,  -3.3513,  -6.9040,  -6.0994,  -6.3572,\n",
      "          -6.0435,  -8.5139,  -5.7176,  -5.3623,  -4.4919,  -4.4807,  -5.0165,\n",
      "          -5.8801,  -3.1965,  -8.8817,  -4.6906,  -6.8056,  -4.4291,  -5.1565,\n",
      "          -3.9416,  -4.9596,  -2.8881,  -5.8706,  -4.1237,  -5.5424,  -7.8439,\n",
      "          -4.6315,  -4.2772,  -5.1596,  -6.4565,  -6.2825,  -6.6097,  -4.9826,\n",
      "          -8.1633,  -8.0723,  -3.7338,  -4.6048,  -6.6727,  -6.0082,  -5.6870,\n",
      "          -6.2627,  -4.4509,  -3.0717, -10.6774,  -7.4107,  -7.6708,  -5.6441,\n",
      "          -5.5040,  -7.4698,  -7.2008,  -6.5784,  -6.7369,  -6.8181,  -6.2823,\n",
      "          -3.9882,  -8.7861,  -5.0863,  -6.9464,  -7.2403,  -5.1651,  -5.8128,\n",
      "          -7.6300,  -5.3102,  -5.1853,  -6.3139,  -8.7344,  -7.3164,  -5.8390,\n",
      "          -4.3497,  -4.2222,  -5.4759,  -6.9204,  -3.5163,  -5.4940,  -5.1533,\n",
      "          -5.5856,  -4.8679,  -5.4173,  -4.1543,  -5.5391,  -7.7106,  -6.6269,\n",
      "          -5.3904,  -5.1156,  -6.5417,  -6.5067,  -6.3994,  -6.2582,  -6.8574,\n",
      "          -8.8443,  -6.2799,  -4.6463,  -6.7376,  -5.0675,  -2.1788,  -6.7825,\n",
      "          -5.2710,  -4.6829,  -7.1225,  -4.3437,  -5.4417,  -9.9201,  -3.8577,\n",
      "          -3.6480,  -6.1950,  -5.4414,  -5.0340,  -5.6098,  -6.4731,  -7.4380,\n",
      "          -8.5178,  -4.6582,  -6.7476,  -5.8486,  -5.4945,  -4.3532,  -9.2404,\n",
      "          -7.5830,  -3.0024,  -7.5757,  -4.1934,  -6.3749,  -5.0251,  -4.3028,\n",
      "          -5.4613,  -6.9449,  -6.4025,  -6.0853,  -5.0191,  -7.1719,  -5.3886,\n",
      "          -8.8912,  -5.5412,  -5.6819,  -4.4482,  -7.7792,  -5.6153,  -5.2794,\n",
      "          -6.3864,  -8.0400,  -5.9636,  -5.1269,  -8.4340,  -6.9664,  -6.8859,\n",
      "          -5.3252,  -5.7620,  -6.5512,  -4.0904,  -3.8939,  -8.9343,  -4.9530,\n",
      "          -7.9238,  -3.4601,  -3.7467,  -8.2276,  -7.2387,  -6.6661,  -4.8803,\n",
      "          -4.8348,  -6.8186,  -4.7465,  -6.6994,  -4.9609,  -7.8089,  -4.6870,\n",
      "          -9.3699,  -6.3171,  -8.3599,  -5.0177,  -6.5629,  -4.8723,  -8.5008,\n",
      "          -5.8744,  -6.9031,  -5.6983,  -4.0597,  -9.0197,  -7.1887,  -5.6849,\n",
      "          -5.8411,  -5.1504,  -7.1146,  -7.1496,  -6.0420,  -6.3078,  -4.7147,\n",
      "          -8.7186,  -4.5973,  -6.4062,  -5.2511,  -7.7053,  -2.3502,  -1.1413,\n",
      "          -6.3825,  -6.6744,  -7.1098,  -7.1470,  -7.4992,  -5.0101,  -4.9407,\n",
      "          -6.9513,  -7.7786,  -4.8964,  -4.1061,  -4.6884,  -6.2380,  -5.4932,\n",
      "          -6.4164,  -5.6863,  -4.6925,  -4.4408,  -7.5563,  -3.9363,  -6.5369,\n",
      "          -6.1383,  -8.6241,  -6.9130,  -6.5548,  -8.3054,  -4.6635,  -5.6138,\n",
      "          -4.0460,  -6.2118,  -6.7570,  -8.4204,  -8.0970]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the SMCA model using a single file from each feature directory\n",
    "\n",
    "from modules.dataloader import load_npy_files\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "index = 30\n",
    "\n",
    "# Select the first file from each modality directories (for testing)\n",
    "text_file_name, text_features = text_features[index]\n",
    "audio_file_name, audio_features = audio_features[index]\n",
    "video_file_name, video_features = video_features[index]\n",
    "\n",
    "print(\"Selected File:\")\n",
    "print(\"Text file:\", os.path.basename(text_file_name))\n",
    "print(\"Audio file:\", os.path.basename(audio_file_name))\n",
    "print(\"Video file:\", os.path.basename(video_file_name))\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Determine the output dimensions\n",
    "d_out_kq = 768 \n",
    "d_out_v = 768\n",
    "\n",
    "# Initialize the SMCA model A\n",
    "model = SMCAModelA(d_out_kq, d_out_v)\n",
    "\n",
    "# Move model to the same device as your data (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Reshape features\n",
    "video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "print(\"Text Feature Shape:\", text_features.shape)\n",
    "print(\"Audio Feature Shape:\", audio_features.shape)\n",
    "print(\"Video Feature Shape:\", video_features.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Pass the data through the SMCA model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    output = model(modalityAlpha=audio_features.to(device), modalityBeta=text_features.to(device), modalityGamma=video_features.to(device))\n",
    "\n",
    "# Print the output shape and the output itself\n",
    "print(\"Model output shape:\", output.shape, \"###[batch_size, output_dim]\")\n",
    "print(\"-\"*50)\n",
    "print(\"Model output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected File Names:\n",
      "Audio file: C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms\\feature_tt0021814.npy\n",
      "Video file: C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors\\tt0021814_features.npy\n",
      "Text file: C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors\\tt0021814.npy\n",
      "Modality Alpha Shape: torch.Size([1, 197, 768])\n",
      "Modality Beta Shape: torch.Size([1, 768])\n",
      "Modality Gamma Shape: torch.Size([1, 95, 768])\n",
      "Stage 1 Bimodal Representation Shape: torch.Size([1, 197, 768])\n",
      "Final Multimodal Representation (Model A) Shape: torch.Size([1, 197, 768])\n",
      "Final Multimodal Representation (Model B) Shape: torch.Size([1, 95, 768])\n"
     ]
    }
   ],
   "source": [
    "# Original Sample Test for the SMCA  (Non-Classes)\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load .npy files\n",
    "    video_features = load_npy_files(r'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors')\n",
    "    audio_features = load_npy_files(r'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms')\n",
    "    text_features = load_npy_files(r'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors')\n",
    "    \n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    text_file_name, text_features = text_features[0]\n",
    "\n",
    "    # Print the file names\n",
    "    print(\"\\nSelected File Names:\")\n",
    "    print(\"Audio file:\", audio_file_name)\n",
    "    print(\"Video file:\", video_file_name)\n",
    "    print(\"Text file:\", text_file_name)\n",
    "    \n",
    "    # Reshape features\n",
    "    video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    # # Randomize assignment of Alpha, Beta, Gamma\n",
    "    # modalityAlpha, modalityBeta, modalityGamma = randomize_modalities(audio_features, video_features, text_features)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Manual assignment of modalities\n",
    "    modalityAlpha = audio_features.to(device)\n",
    "    modalityBeta = text_features.to(device)\n",
    "    modalityGamma = video_features.to(device)\n",
    "\n",
    "    # Apply linear transformation to match dimensions\n",
    "    linear_transform_Alpha = LinearTransformations(modalityAlpha.shape[-1], 768).to(device)\n",
    "    linear_transform_Beta = LinearTransformations(modalityBeta.shape[-1], 768).to(device)\n",
    "    linear_transform_Gamma = LinearTransformations(modalityGamma.shape[-1], 768).to(device)\n",
    "\n",
    "    modalityAlpha = linear_transform_Alpha(modalityAlpha)\n",
    "    modalityBeta = linear_transform_Beta(modalityBeta)\n",
    "    modalityGamma = linear_transform_Gamma(modalityGamma)\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    d_out_kq = 768  # Final transformed dimension\n",
    "    d_out_v = 768\n",
    "\n",
    "    # Stage 1: Bimodal Representation\n",
    "    modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v)\n",
    "    \n",
    "    # Stage 2, Model A: Multimodal Representation (using AlphaBeta as Query)\n",
    "    final_representation_A = SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v)\n",
    "    \n",
    "    # Stage 2, Model B: Multimodal Representation (using AlphaBeta as Key-Value)\n",
    "    final_representation_B = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v)\n",
    "\n",
    "    print(\"Modality Alpha Shape:\", modalityAlpha.shape)\n",
    "    print(\"Modality Beta Shape:\", modalityBeta.shape)\n",
    "    print(\"Modality Gamma Shape:\", modalityGamma.shape)\n",
    "    print(\"Stage 1 Bimodal Representation Shape:\", modalityAlphaBeta.shape)\n",
    "    print(\"Final Multimodal Representation (Model A) Shape:\", final_representation_A.shape)\n",
    "    print(\"Final Multimodal Representation (Model B) Shape:\", final_representation_B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-3):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dense_layer, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    dense_layer.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device).view(-1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass inputs through SMCA model\n",
    "        outputs = model(modalityAlpha=audio_features, modalityBeta=text_features, modalityGamma=video_features)\n",
    "        \n",
    "        # Check if padding is necessary\n",
    "        output_size = outputs.size(1)\n",
    "        dense_input_size = dense_layer.fc.in_features\n",
    "        \n",
    "        if output_size < dense_input_size:\n",
    "            # Pad the outputs if they are smaller than the expected size for the dense layer\n",
    "            padding_size = dense_input_size - output_size\n",
    "            # Pad on the second dimension (feature dimension)\n",
    "            outputs = torch.nn.functional.pad(outputs, (0, padding_size))\n",
    "        elif output_size > dense_input_size:\n",
    "            # In case outputs are larger (though unlikely, we trim)\n",
    "            outputs = outputs[:, :dense_input_size]\n",
    "        \n",
    "        # Pass the fused features through the dense layer\n",
    "        predictions = dense_layer(outputs).view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Pass inputs through SMCA model\n",
    "            outputs = model(modalityAlpha=audio_features, modalityBeta=text_features, modalityGamma=video_features)\n",
    "\n",
    "            # Check if padding is necessary\n",
    "            output_size = outputs.size(1)\n",
    "            dense_input_size = dense_layer.fc.in_features\n",
    "            \n",
    "            if output_size < dense_input_size:\n",
    "                # Pad the outputs if they are smaller than the expected size for the dense layer\n",
    "                padding_size = dense_input_size - output_size\n",
    "                # Pad on the second dimension (feature dimension)\n",
    "                outputs = torch.nn.functional.pad(outputs, (0, padding_size))\n",
    "            elif output_size > dense_input_size:\n",
    "                # In case outputs are larger (though unlikely, we trim)\n",
    "                outputs = outputs[:, :dense_input_size]\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1) \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Evaluation Loss: {average_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return average_loss, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Pass inputs through SMCA model\n",
    "            outputs = model(modalityAlpha=audio_features.to(device), modalityBeta=text_features.to(device), modalityGamma=video_features.to(device))\n",
    "\n",
    "            # Check if padding is necessary\n",
    "            output_size = outputs.size(1)\n",
    "            dense_input_size = dense_layer.fc.in_features\n",
    "            \n",
    "            if output_size < dense_input_size:\n",
    "                # Pad the outputs if they are smaller than the expected size for the dense layer\n",
    "                padding_size = dense_input_size - output_size\n",
    "                # Pad on the second dimension (feature dimension)\n",
    "                outputs = torch.nn.functional.pad(outputs, (0, padding_size))\n",
    "            elif output_size > dense_input_size:\n",
    "                # In case outputs are larger (though unlikely, we trim)\n",
    "                outputs = outputs[:, :dense_input_size]\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1)  \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    return average_loss, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest output size from the SMCA model\n",
    "def find_largest_output_size(model, dataloader, device):\n",
    "    max_output_size = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            # Move features to device\n",
    "            text_features = text_features.to(device)\n",
    "            audio_features = audio_features.to(device)\n",
    "            video_features = video_features.to(device)\n",
    "\n",
    "            # Pass inputs through the SMCA model\n",
    "            outputs = model(\n",
    "                modalityAlpha=audio_features, \n",
    "                modalityBeta=text_features, \n",
    "                modalityGamma=video_features\n",
    "            )\n",
    "\n",
    "            # Compare and store the maximum output size\n",
    "            if outputs.size(1) > max_output_size:\n",
    "                max_output_size = outputs.size(1)\n",
    "\n",
    "    return max_output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1576x1339 and 768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Ensure you have a dataloader that yields inputs and targets\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Validate model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m val_loss, precision, recall, f1_score \u001b[38;5;241m=\u001b[39m evaluate_model(model\u001b[38;5;241m=\u001b[39mmodel, dense_layer\u001b[38;5;241m=\u001b[39mdense_layer, dataloader\u001b[38;5;241m=\u001b[39mval_dataloader, criterion\u001b[38;5;241m=\u001b[39mcriterion, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dense_layer, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :dense_input_size]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Pass the fused features through the dense layer\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdense_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edjin\\OneDrive\\Documents\\Programming Files\\Thesis\\SMCA\\SMCA\\..\\modules\\classifier.py:12\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Pass the input through the dense layer\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Apply the sigmoid activation function\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1576x1339 and 768x1)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelA(output_dim, output_dim) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = find_largest_output_size(model, train_dataloader, device)\n",
    "    max_output_size_val = find_largest_output_size(model, val_dataloader, device)\n",
    "    max_output_size_test = find_largest_output_size(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=output_dim).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(dense_layer.parameters())  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelB(output_dim, output_dim) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = find_largest_output_size(model, train_dataloader, device)\n",
    "    max_output_size_val = find_largest_output_size(model, val_dataloader, device)\n",
    "    max_output_size_test = find_largest_output_size(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=output_dim).to(device)  # Initialize and move to the correct device\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(dense_layer.parameters())  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
