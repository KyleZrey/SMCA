{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from modules.cross_attention import CrossAttention\n",
    "from modules.dataloader import load_npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbracementLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(EmbracementLayer, self).__init__()\n",
    "        self.fc = nn.Linear(d_in, d_out)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, video_features, audio_features, text_features):\n",
    "        # Concatenate features along the last dimension\n",
    "        combined_features = torch.cat([video_features, audio_features, text_features], dim=-1)\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        transformed_features = self.fc(combined_features)\n",
    "        \n",
    "        # Apply normalization and activation\n",
    "        norm_features = self.norm(transformed_features)\n",
    "        output = self.activation(norm_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to match dimensions\n",
    "class LinearTransformations(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearTransformations, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HadamardProduct(tensor1, tensor2):\n",
    "    # Ensure both tensors have the same shape\n",
    "    if tensor1.shape != tensor2.shape:\n",
    "        raise ValueError(\"Tensors must have the same shape for Hadamard product.\")\n",
    "    \n",
    "    # Compute the Hadamard product\n",
    "    return tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Function\n",
    "def PairCrossAttention(modalityAlpha, modalityBeta, d_out_kq=768, d_out_v=768):\n",
    "    cross_attn = CrossAttention(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Combined Shape: torch.Size([1, 1, 197, 768])\n",
      "Audio Combined Shape: torch.Size([1, 1, 197, 768])\n",
      "Text Combined Shape: torch.Size([1, 1, 197, 768])\n",
      "Fused Representation Shape: torch.Size([1, 1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load .npy files\n",
    "    video_features = load_npy_files(r'/Users/kyleandrecastro/Documents/GitHub/features/visual')\n",
    "    audio_features = load_npy_files(r'/Users/kyleandrecastro/Documents/GitHub/features/audio')\n",
    "    text_features = load_npy_files(r'/Users/kyleandrecastro/Documents/GitHub/features/text')\n",
    "\n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    text_file_name, text_features = text_features[0]\n",
    "\n",
    "    # # Print the file names\n",
    "    # print(\"\\nSelected File Names:\")\n",
    "    # print(\"Video file:\", video_file_name)\n",
    "    # print(\"Audio file:\", audio_file_name)\n",
    "    # print(\"Text file:\", text_file_name)\n",
    "    \n",
    "    # Reshape features\n",
    "    video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "    audio_features = audio_features.unsqueeze(0)    # Add batch dimension\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    # Apply linear transformation to match dimensions\n",
    "    linear_transform_video = LinearTransformations(video_features.shape[-1], 768)\n",
    "    linear_transform_audio = LinearTransformations(audio_features.shape[-1], 768)\n",
    "    linear_transform_text = LinearTransformations(text_features.shape[-1], 768)\n",
    "\n",
    "    video_features = linear_transform_video(video_features)\n",
    "    audio_features = linear_transform_audio(audio_features)\n",
    "    text_features = linear_transform_text(text_features)\n",
    "    \n",
    "    # Cross-Attention for every possible pairs\n",
    "    video_audio = PairCrossAttention(video_features, audio_features)\n",
    "    video_text = PairCrossAttention(video_features, text_features)\n",
    "    audio_video = PairCrossAttention(audio_features, video_features)\n",
    "    audio_text = PairCrossAttention(audio_features, text_features)\n",
    "    text_video = PairCrossAttention(text_features, video_features)\n",
    "    text_audio = PairCrossAttention(text_features, audio_features)\n",
    "\n",
    "    # Combine the Cross-Attention outputs using hadamard\n",
    "    video_combined = HadamardProduct(video_audio, video_text)\n",
    "    audio_combined = HadamardProduct(audio_video, audio_text)\n",
    "    text_combined = HadamardProduct(text_video, text_audio)\n",
    "    text_combined = text_combined.expand(1, 1, 197, 768)\n",
    "\n",
    "    print(\"Video Combined Shape:\", video_combined.shape)\n",
    "    print(\"Audio Combined Shape:\", audio_combined.shape)\n",
    "    print(\"Text Combined Shape:\", text_combined.shape)\n",
    "\n",
    "    # Initialize and apply Embracement Layer\n",
    "    d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "    embracement_layer = EmbracementLayer(d_in, 768)\n",
    "    \n",
    "    fused_representation = embracement_layer(video_combined, audio_combined, text_combined)\n",
    "\n",
    "    print(\"Fused Representation Shape:\", fused_representation.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
