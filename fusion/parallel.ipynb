{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from modules.cross_attention import CrossAttention\n",
    "from modules.dataloader import load_npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbracementLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(EmbracementLayer, self).__init__()\n",
    "        self.fc = nn.Linear(d_in, d_out)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, video_features, audio_features, text_features):\n",
    "        # Concatenate features along the last dimension\n",
    "        combined_features = torch.cat([video_features, audio_features, text_features], dim=-1)\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        transformed_features = self.fc(combined_features)\n",
    "        \n",
    "        # Apply normalization and activation\n",
    "        norm_features = self.norm(transformed_features)\n",
    "        output = self.activation(norm_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to match dimensions\n",
    "class LinearTransformations(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearTransformations, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HadamardProduct(tensor1, tensor2):\n",
    "    # Ensure both tensors have the same shape\n",
    "    if tensor1.shape != tensor2.shape:\n",
    "        raise ValueError(\"Tensors must have the same shape for Hadamard product.\")\n",
    "    \n",
    "    # Compute the Hadamard product\n",
    "    return tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Function\n",
    "def PairCrossAttention(modalityAlpha, modalityBeta, d_out_kq=768, d_out_v=768):\n",
    "    cross_attn = CrossAttention(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Combined Shape: torch.Size([1, 95, 768])\n",
      "Audio Combined Shape: torch.Size([1, 197, 768])\n",
      "Text Combined Shape: torch.Size([1, 1, 197, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 95 but got size 197 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m d_in \u001b[38;5;241m=\u001b[39m video_combined\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m audio_combined\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m text_combined\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     53\u001b[0m embracement_layer \u001b[38;5;241m=\u001b[39m EmbracementLayer(d_in, \u001b[38;5;241m768\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m fused_representation \u001b[38;5;241m=\u001b[39m embracement_layer(video_combined, audio_combined, text_combined)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFused Representation Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fused_representation\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 10\u001b[0m, in \u001b[0;36mEmbracementLayer.forward\u001b[1;34m(self, video_features, audio_features, text_features)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_features, audio_features, text_features):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Concatenate features along the last dimension\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([video_features, audio_features, text_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Apply linear transformation\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     transformed_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(combined_features)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 95 but got size 197 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load .npy files\n",
    "    video_features = load_npy_files(r'D:\\Projects\\Thesis\\Video\\test')\n",
    "    audio_features = load_npy_files(r'D:\\Projects\\SMCA\\misc\\audio_fe\\logmel_spectrograms\\test')\n",
    "    text_features = load_npy_files(r'D:\\Projects\\SMCA\\misc\\textStream_BERT\\feature_vectors\\test')\n",
    "\n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    text_file_name, text_features = text_features[0]\n",
    "\n",
    "    # # Print the file names\n",
    "    # print(\"\\nSelected File Names:\")\n",
    "    # print(\"Video file:\", video_file_name)\n",
    "    # print(\"Audio file:\", audio_file_name)\n",
    "    # print(\"Text file:\", text_file_name)\n",
    "    \n",
    "    # Reshape features\n",
    "    video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "    audio_features = audio_features.unsqueeze(0)    # Add batch dimension\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    # Apply linear transformation to match dimensions\n",
    "    linear_transform_video = LinearTransformations(video_features.shape[-1], 768)\n",
    "    linear_transform_audio = LinearTransformations(audio_features.shape[-1], 768)\n",
    "    linear_transform_text = LinearTransformations(text_features.shape[-1], 768)\n",
    "\n",
    "    video_features = linear_transform_video(video_features)\n",
    "    audio_features = linear_transform_audio(audio_features)\n",
    "    text_features = linear_transform_text(text_features)\n",
    "    \n",
    "    # Cross-Attention for every possible pairs\n",
    "    video_audio = PairCrossAttention(video_features, audio_features)\n",
    "    video_text = PairCrossAttention(video_features, text_features)\n",
    "    audio_video = PairCrossAttention(audio_features, video_features)\n",
    "    audio_text = PairCrossAttention(audio_features, text_features)\n",
    "    text_video = PairCrossAttention(text_features, video_features)\n",
    "    text_audio = PairCrossAttention(text_features, audio_features)\n",
    "\n",
    "    # Combine the Cross-Attention outputs using hadamard\n",
    "    video_combined = HadamardProduct(video_audio, video_text)\n",
    "    audio_combined = HadamardProduct(audio_video, audio_text)\n",
    "    text_combined = HadamardProduct(text_video, text_audio)\n",
    "    text_combined = text_combined.expand(1, 1, 197, 768)\n",
    "\n",
    "    print(\"Video Combined Shape:\", video_combined.shape)\n",
    "    print(\"Audio Combined Shape:\", audio_combined.shape)\n",
    "    print(\"Text Combined Shape:\", text_combined.shape)\n",
    "\n",
    "    # Initialize and apply Embracement Layer\n",
    "    d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "    embracement_layer = EmbracementLayer(d_in, 768)\n",
    "    \n",
    "    fused_representation = embracement_layer(video_combined, audio_combined, text_combined)\n",
    "\n",
    "    print(\"Fused Representation Shape:\", fused_representation.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
