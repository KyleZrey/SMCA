{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for module imports\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import custom modules\n",
    "from modules.cross_attention import CrossAttention\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('/Users/kyleandrecastro/Documents/GitHub/SMCA/misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = '/Users/kyleandrecastro/Documents/GitHub/SMCA/misc/textStream_BERT/feature_vectors/test'\n",
    "audio_features_dir = '/Users/kyleandrecastro/Documents/GitHub/SMCA/misc/audio_fe/logmel_spectrograms/test'\n",
    "video_features_dir = '/Users/kyleandrecastro/Documents/GitHub/features/visual'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "# # Splitting data for training, validation, and testing\n",
    "# train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Further splitting remaining set into validation and test sets\n",
    "# val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "# val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "# test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "# # Combine all data for K-fold cross-validation\n",
    "# full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Function\n",
    "def PairCrossAttention(modalityAlpha, modalityBeta, d_out_kq=768, d_out_v=768):\n",
    "    cross_attn = CrossAttention(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HadamardProduct(tensor1, tensor2):\n",
    "    # Ensure both tensors have the same shape\n",
    "    if tensor1.shape != tensor2.shape:\n",
    "        raise ValueError(\"Tensors must have the same shape for Hadamard product.\")\n",
    "    \n",
    "    # Compute the Hadamard product\n",
    "    return tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor except the batch dimension\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbracementLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(EmbracementLayer, self).__init__()\n",
    "        self.fc = nn.Linear(d_in, d_out)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, video_features, audio_features, text_features):\n",
    "        # Concatenate features along the last dimension\n",
    "        combined_features = torch.cat([video_features, audio_features, text_features], dim=-1)\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        transformed_features = self.fc(combined_features)\n",
    "        \n",
    "        # Apply normalization and activation\n",
    "        norm_features = self.norm(transformed_features)\n",
    "        output = self.activation(norm_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file name: /Users/kyleandrecastro/Documents/GitHub/SMCA/misc/textStream_BERT/feature_vectors/test/text_tt0021814.npy\n",
      "Audio file name: /Users/kyleandrecastro/Documents/GitHub/SMCA/misc/audio_fe/logmel_spectrograms/test/audio_tt0021814.npy\n",
      "Video file name: /Users/kyleandrecastro/Documents/GitHub/features/visual/tt0021814_features.npy\n",
      "Text features shape: torch.Size([1024])\n",
      "Audio features shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([95, 768]) \n",
      "\n",
      "text_features shape: torch.Size([1, 1024])\n",
      "audio_features shape: torch.Size([197, 768])\n",
      "video_features shape: torch.Size([95, 768]) \n",
      "\n",
      "video_audio shape: torch.Size([95, 768])\n",
      "video_text shape: torch.Size([95, 768])\n",
      "audio_video shape: torch.Size([197, 768])\n",
      "audio_text shape: torch.Size([197, 768])\n",
      "text_video shape: torch.Size([1, 768])\n",
      "text_audio shape: torch.Size([1, 768]) \n",
      "\n",
      "Text Combined Shape: torch.Size([1, 768])\n",
      "Video Combined Shape: torch.Size([95, 768])\n",
      "Audio Combined Shape: torch.Size([197, 768]) \n",
      "\n",
      "Fused Features Shape: torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    text_file_name, text_features = text_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    \n",
    "    print(\"Text file name:\", text_file_name)\n",
    "    print(\"Audio file name:\", audio_file_name)\n",
    "    print(\"Video file name:\", video_file_name)\n",
    "\n",
    "    print(\"Text features shape:\", text_features.shape)\n",
    "    print(\"Audio features shape:\", audio_features.shape)\n",
    "    print(\"Video features shape:\", video_features.shape, '\\n')\n",
    "\n",
    "    # Reshape feature\n",
    "    # video_features = video_features.unsqueeze(0)  # Add batch dimension   \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    print(\"text_features shape:\", text_features.shape)\n",
    "    print(\"audio_features shape:\", audio_features.shape)\n",
    "    print(\"video_features shape:\", video_features.shape, '\\n')\n",
    "    \n",
    "    # Cross-Attention for every possible pairs\n",
    "    video_audio = PairCrossAttention(video_features, audio_features)\n",
    "    video_text = PairCrossAttention(video_features, text_features)\n",
    "    audio_video = PairCrossAttention(audio_features, video_features)\n",
    "    audio_text = PairCrossAttention(audio_features, text_features)\n",
    "    text_video = PairCrossAttention(text_features, video_features)\n",
    "    text_audio = PairCrossAttention(text_features, audio_features)\n",
    "\n",
    "    # Print shapes of cross-attention results\n",
    "    print(\"video_audio shape:\", video_audio.shape)\n",
    "    print(\"video_text shape:\", video_text.shape)\n",
    "    print(\"audio_video shape:\", audio_video.shape)\n",
    "    print(\"audio_text shape:\", audio_text.shape)\n",
    "    print(\"text_video shape:\", text_video.shape)\n",
    "    print(\"text_audio shape:\", text_audio.shape, '\\n')\n",
    "\n",
    "    # Combine the Cross-Attention outputs using hadamard\n",
    "    video_combined = HadamardProduct(video_audio, video_text)\n",
    "    audio_combined = HadamardProduct(audio_video, audio_text)\n",
    "    text_combined = HadamardProduct(text_video, text_audio)\n",
    "    \n",
    "    print(\"Text Combined Shape:\", text_combined.shape)\n",
    "    print(\"Video Combined Shape:\", video_combined.shape)\n",
    "    print(\"Audio Combined Shape:\", audio_combined.shape, '\\n')\n",
    "    \n",
    "    # Fusion using Embracement Layer\n",
    "    d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "    embracement_layer = EmbracementLayer(d_in, d_in)\n",
    "    fused_features = embracement_layer(video_combined[-1], audio_combined[-1], text_combined[-1])\n",
    "\n",
    "    print(\"Fused Features Shape:\", fused_features.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
