{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in_1, d_in_2, d_out_kq, d_out_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in_1, d_out_kq))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in_2, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in_2, d_out_v))\n",
    "    \n",
    "    def forward(self, modality1, modality2):\n",
    "        \n",
    "        queries = modality1.matmul(self.W_query)  \n",
    "        keys = modality2.matmul(self.W_key)      \n",
    "        values = modality2.matmul(self.W_value) \n",
    "\n",
    "        attn_scores = queries.matmul(keys.transpose(-2, -1))\n",
    "        attn_weights = torch.softmax(attn_scores / (self.d_out_kq ** 0.5), dim=-1)\n",
    "        context_vector = attn_weights.matmul(values) \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbracementLayer(nn.Module):\n",
    "    def __init__(self, docking_dim, output_dim, num_modalities):\n",
    "        super(EmbracementLayer, self).__init__()\n",
    "        self.num_modalities = num_modalities\n",
    "        self.docking_dim = docking_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Linear layers to transform concatenated features to the output dimension\n",
    "        self.linear = nn.Linear(docking_dim, output_dim)\n",
    "\n",
    "        # Weights for each modality\n",
    "        self.modality_weights = nn.Parameter(torch.ones(num_modalities))\n",
    "\n",
    "    def forward(self, modalities):\n",
    "        # Ensure that modalities have the same shape\n",
    "        assert all(modalities[0].shape == modality.shape for modality in modalities), \\\n",
    "            \"All modality tensors must have the same shape.\"\n",
    "\n",
    "        # Concatenate the modalities along the last dimension\n",
    "        combined = torch.cat(modalities, dim=-1)\n",
    "\n",
    "        # Print the shapes for debugging\n",
    "        print(\"Combined shape:\", combined.shape)\n",
    "        print(\"Modality weights shape:\", self.modality_weights.shape)\n",
    "\n",
    "        # Apply modality weights\n",
    "        modality_weights = F.softmax(self.modality_weights, dim=0)\n",
    "\n",
    "        # Expand modality weights to match features dimension\n",
    "        num_features_per_modality = combined.size(-1) // self.num_modalities\n",
    "        modality_weights_expanded = modality_weights.unsqueeze(-1).expand(-1, -1, num_features_per_modality)\n",
    "\n",
    "        # Split combined tensor into chunks\n",
    "        combined_splits = combined.chunk(self.num_modalities, dim=-1)\n",
    "\n",
    "        # Ensure modality_weights shape matches combined_splits\n",
    "        modality_weights = modality_weights.view(1, 1, 1, -1).expand(combined.size(0), combined.size(1), combined.size(2), -1)\n",
    "        \n",
    "        # Compute weighted sum\n",
    "        weighted_combined = sum(weight * split for weight, split in zip(modality_weights_expanded.unbind(), combined_splits))\n",
    "        # Flatten dimensions before applying the linear layer\n",
    "        combined_view = weighted_combined.view(weighted_combined.size(0), -1, weighted_combined.size(-1))\n",
    "        fused = self.linear(combined_view)\n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to match dimensions\n",
    "class LinearTransformations(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearTransformations, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_files(directory):\n",
    "    file_list = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.npy')]\n",
    "    feature_vectors = [(file, torch.tensor(np.load(file))) for file in file_list]\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Function\n",
    "def PairCrossAttention(modalityAlpha, modalityBeta, d_out_kq=768, d_out_v=768):\n",
    "    cross_attn = CrossAttention(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Combined Shape: torch.Size([1, 1, 197, 1536])\n",
      "Audio Combined Shape: torch.Size([1, 1, 197, 1536])\n",
      "Text Combined Shape: torch.Size([1, 1, 197, 1536])\n",
      "Combined shape: torch.Size([1, 1, 197, 4608])\n",
      "Modality weights shape: torch.Size([3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (-1) isn't allowed in a leading, non-existing dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m embracement_layer \u001b[38;5;241m=\u001b[39m EmbracementLayer(docking_dim\u001b[38;5;241m=\u001b[39mvideo_combined\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, num_modalities\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Fuse the combined outputs\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m fused_representation \u001b[38;5;241m=\u001b[39m \u001b[43membracement_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_combined\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFused Representation Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fused_representation\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mEmbracementLayer.forward\u001b[0;34m(self, modalities)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Expand modality weights to match features dimension\u001b[39;00m\n\u001b[1;32m     30\u001b[0m num_features_per_modality \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_modalities\n\u001b[0;32m---> 31\u001b[0m modality_weights_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mmodality_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_modality\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Split combined tensor into chunks\u001b[39;00m\n\u001b[1;32m     34\u001b[0m combined_splits \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_modalities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (-1) isn't allowed in a leading, non-existing dimension 0"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load .npy files\n",
    "    video_features = load_npy_files(r'/Users/kyleandrecastro/Documents/GitHub/features/visual')\n",
    "    audio_features = load_npy_files(r'/Users/kyleandrecastro/Documents/GitHub/features/audio')\n",
    "    text_features = load_npy_files(r'/Users/kyleandrecastro/Documents/GitHub/features/text')\n",
    "\n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    text_file_name, text_features = text_features[0]\n",
    "\n",
    "    # # Print the file names\n",
    "    # print(\"\\nSelected File Names:\")\n",
    "    # print(\"Video file:\", video_file_name)\n",
    "    # print(\"Audio file:\", audio_file_name)\n",
    "    # print(\"Text file:\", text_file_name)\n",
    "    \n",
    "    # Reshape features\n",
    "    video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "    audio_features = audio_features.unsqueeze(0)    # Add batch dimension\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    # Apply linear transformation to match dimensions\n",
    "    linear_transform_video = LinearTransformations(video_features.shape[-1], 768)\n",
    "    linear_transform_audio = LinearTransformations(audio_features.shape[-1], 768)\n",
    "    linear_transform_text = LinearTransformations(text_features.shape[-1], 768)\n",
    "\n",
    "    video_features = linear_transform_video(video_features)\n",
    "    audio_features = linear_transform_audio(audio_features)\n",
    "    text_features = linear_transform_text(text_features)\n",
    "    \n",
    "    # Cross-Attention for every possible pairs\n",
    "    video_audio = PairCrossAttention(video_features, audio_features)\n",
    "    video_text = PairCrossAttention(video_features, text_features)\n",
    "    audio_video = PairCrossAttention(audio_features, video_features)\n",
    "    audio_text = PairCrossAttention(audio_features, text_features)\n",
    "    text_video = PairCrossAttention(text_features, video_features)\n",
    "    text_audio = PairCrossAttention(text_features, audio_features)\n",
    "\n",
    "    # Combine the Cross-Attention outputs\n",
    "    video_combined = torch.cat((video_audio, video_text), dim=-1)\n",
    "    audio_combined = torch.cat((audio_video, audio_text), dim=-1)\n",
    "    text_combined = torch.cat((text_video, text_audio), dim=-1)\n",
    "\n",
    "    # Adjust text_combined to match dimensions\n",
    "    text_combined = text_combined.expand(-1, -1, video_combined.size(2), -1)\n",
    "\n",
    "    print(\"Video Combined Shape:\", video_combined.shape)\n",
    "    print(\"Audio Combined Shape:\", audio_combined.shape)\n",
    "    print(\"Text Combined Shape:\", text_combined.shape)\n",
    "\n",
    "    # Instantiate Embracement Layer\n",
    "    embracement_layer = EmbracementLayer(docking_dim=video_combined.shape[-1] * 3, output_dim=768, num_modalities=3)\n",
    "\n",
    "    # Fuse the combined outputs\n",
    "    fused_representation = embracement_layer([video_combined, audio_combined, text_combined])\n",
    "\n",
    "    print(\"Fused Representation Shape:\", fused_representation.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
