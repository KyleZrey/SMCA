{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "\n",
    "from modules.cross_attentionb import CrossAttentionB\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.linear_transformation import LinearTransformations\n",
    "from modules.output_max import output_max\n",
    "from evaluation_validation.train_model import train_model\n",
    "from evaluation_validation.evaluate_model import evaluate_model\n",
    "from evaluation_validation.test_model import test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 0, 'green': 1} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "        \n",
    "        # Debugging output\n",
    "        if label_data.shape[0] == 0:\n",
    "            print(f\"Empty target for IMDBid {imdbid} at index {idx}\")\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Text'\n",
    "audio_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Audio'\n",
    "video_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Video'\n",
    "\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMCA Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 1 of SMCA\n",
    "def SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(\n",
    "        d_in_query=modalityGamma.shape[-1],\n",
    "        d_in_kv=modalityAlphaBeta.shape[-1],\n",
    "        d_out_kq=d_out_kq,\n",
    "        d_out_v=d_out_v\n",
    "    ).to(device)\n",
    "    multimodal_representation = cross_attn(modalityGamma, modalityAlphaBeta)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelB(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v, device):\n",
    "        super(SMCAModelB, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma, device):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityGamma (as query) and modalityAlphaBeta (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Flatten the output\n",
    "        return torch.flatten(multimodal_representation, start_dim=1)  # Flatten all dimensions except batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-4):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/10\n",
      "Evaluation Loss: 0.6920\n",
      "Precision: 0.7455\n",
      "Recall: 0.5655\n",
      "F1 Score: 0.6431\n",
      "Training Loss: 0.6976, Validation Loss: 0.6920\n",
      "------------------------------\n",
      "Epoch 2/10\n",
      "Evaluation Loss: 0.6914\n",
      "Precision: 0.6818\n",
      "Recall: 0.5172\n",
      "F1 Score: 0.5882\n",
      "Training Loss: 0.6925, Validation Loss: 0.6914\n",
      "------------------------------\n",
      "Epoch 3/10\n",
      "Evaluation Loss: 0.6897\n",
      "Precision: 0.7376\n",
      "Recall: 0.7172\n",
      "F1 Score: 0.7273\n",
      "Training Loss: 0.6922, Validation Loss: 0.6897\n",
      "------------------------------\n",
      "Epoch 4/10\n",
      "Evaluation Loss: 0.6884\n",
      "Precision: 0.7424\n",
      "Recall: 0.6759\n",
      "F1 Score: 0.7076\n",
      "Training Loss: 0.6891, Validation Loss: 0.6884\n",
      "------------------------------\n",
      "Epoch 5/10\n",
      "Evaluation Loss: 0.6845\n",
      "Precision: 0.7452\n",
      "Recall: 0.8069\n",
      "F1 Score: 0.7748\n",
      "Training Loss: 0.6866, Validation Loss: 0.6845\n",
      "------------------------------\n",
      "Epoch 6/10\n",
      "Evaluation Loss: 0.6842\n",
      "Precision: 0.7322\n",
      "Recall: 0.9241\n",
      "F1 Score: 0.8171\n",
      "Training Loss: 0.6841, Validation Loss: 0.6842\n",
      "------------------------------\n",
      "Epoch 7/10\n",
      "Evaluation Loss: 0.6812\n",
      "Precision: 0.7302\n",
      "Recall: 0.9517\n",
      "F1 Score: 0.8263\n",
      "Training Loss: 0.6825, Validation Loss: 0.6812\n",
      "------------------------------\n",
      "Epoch 8/10\n",
      "Evaluation Loss: 0.6762\n",
      "Precision: 0.7407\n",
      "Recall: 0.9655\n",
      "F1 Score: 0.8383\n",
      "Training Loss: 0.6782, Validation Loss: 0.6762\n",
      "------------------------------\n",
      "Epoch 9/10\n",
      "Evaluation Loss: 0.6736\n",
      "Precision: 0.7409\n",
      "Recall: 0.9862\n",
      "F1 Score: 0.8462\n",
      "Training Loss: 0.6776, Validation Loss: 0.6736\n",
      "------------------------------\n",
      "Epoch 10/10\n",
      "Evaluation Loss: 0.6743\n",
      "Precision: 0.7380\n",
      "Recall: 0.9517\n",
      "F1 Score: 0.8313\n",
      "Training Loss: 0.6750, Validation Loss: 0.6743\n",
      "------------------------------\n",
      "Testing the model on the test set...\n",
      "Test Loss: 0.6016\n",
      "Test Precision: 0.7313\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.8448\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelB(512, 256, device) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = output_max(model, train_dataloader, device)\n",
    "    max_output_size_val = output_max(model, val_dataloader, device)\n",
    "    max_output_size_test = output_max(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=512).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(dense_layer.parameters())  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
