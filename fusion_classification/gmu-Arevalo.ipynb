{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Print missing files after checking all\n",
    "        if self.missing_files:\n",
    "            print(\"Missing files:\")\n",
    "            for item in self.missing_files:\n",
    "                print(f\"IMDBid: {item['IMDBid']}\")\n",
    "            print(f\"Total IMDB IDs with missing files: {len(self.missing_files)}\")\n",
    "        else:\n",
    "            print(\"No missing files.\")\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files:\n",
      "IMDBid: tt2494280\n",
      "IMDBid: tt1724962\n",
      "IMDBid: tt1152836\n",
      "IMDBid: tt0389790\n",
      "IMDBid: tt3053228\n",
      "IMDBid: tt1045778\n",
      "IMDBid: tt1758795\n",
      "IMDBid: tt0099385\n",
      "IMDBid: tt2917484\n",
      "IMDBid: tt4769836\n",
      "IMDBid: tt0089652\n",
      "IMDBid: tt0465494\n",
      "IMDBid: tt3675748\n",
      "IMDBid: tt2126362\n",
      "IMDBid: tt0988083\n",
      "IMDBid: tt2101341\n",
      "IMDBid: tt0401997\n",
      "IMDBid: tt1661461\n",
      "IMDBid: tt1313139\n",
      "IMDBid: tt1094661\n",
      "IMDBid: tt5162658\n",
      "IMDBid: tt0104839\n",
      "IMDBid: tt1288558\n",
      "IMDBid: tt5962210\n",
      "IMDBid: tt2937696\n",
      "IMDBid: tt0284363\n",
      "IMDBid: tt5580390\n",
      "IMDBid: tt2293750\n",
      "IMDBid: tt2980472\n",
      "IMDBid: tt0082186\n",
      "IMDBid: tt0924129\n",
      "IMDBid: tt0988595\n",
      "IMDBid: tt1349482\n",
      "IMDBid: tt4158096\n",
      "IMDBid: tt1403241\n",
      "IMDBid: tt2713642\n",
      "IMDBid: tt1682940\n",
      "IMDBid: tt10327354\n",
      "IMDBid: tt1087842\n",
      "IMDBid: tt1800302\n",
      "IMDBid: tt0113855\n",
      "IMDBid: tt2504022\n",
      "IMDBid: tt7248248\n",
      "IMDBid: tt1720164\n",
      "IMDBid: tt1336621\n",
      "IMDBid: tt0266987\n",
      "IMDBid: tt0859635\n",
      "Total IMDB IDs with missing files: 47\n",
      "Missing files:\n",
      "IMDBid: tt2437712\n",
      "IMDBid: tt0099371\n",
      "IMDBid: tt2935564\n",
      "IMDBid: tt0140336\n",
      "IMDBid: tt4687276\n",
      "IMDBid: tt0367085\n",
      "IMDBid: tt0220827\n",
      "IMDBid: tt0465602\n",
      "IMDBid: tt2350496\n",
      "IMDBid: tt1084950\n",
      "IMDBid: tt0110093\n",
      "IMDBid: tt1273235\n",
      "IMDBid: tt4503510\n",
      "IMDBid: tt1772925\n",
      "IMDBid: tt2180411\n",
      "IMDBid: tt0181865\n",
      "IMDBid: tt0200469\n",
      "IMDBid: tt0259288\n",
      "Total IMDB IDs with missing files: 18\n",
      "Missing files:\n",
      "IMDBid: tt0190590\n",
      "IMDBid: tt2383068\n",
      "IMDBid: tt1488555\n",
      "IMDBid: tt0758730\n",
      "IMDBid: tt0389557\n",
      "IMDBid: tt0498353\n",
      "IMDBid: tt5084170\n",
      "IMDBid: tt0405052\n",
      "IMDBid: tt1104733\n",
      "IMDBid: tt1924429\n",
      "IMDBid: tt5027774\n",
      "IMDBid: tt0990413\n",
      "IMDBid: tt5580036\n",
      "IMDBid: tt2524674\n",
      "Total IMDB IDs with missing files: 14\n",
      "Missing files:\n",
      "IMDBid: tt0988595\n",
      "IMDBid: tt1724962\n",
      "IMDBid: tt0758730\n",
      "IMDBid: tt0200469\n",
      "IMDBid: tt0389790\n",
      "IMDBid: tt4503510\n",
      "IMDBid: tt0401997\n",
      "IMDBid: tt1349482\n",
      "IMDBid: tt0082186\n",
      "IMDBid: tt1094661\n",
      "IMDBid: tt0924129\n",
      "IMDBid: tt2437712\n",
      "IMDBid: tt2917484\n",
      "IMDBid: tt3053228\n",
      "IMDBid: tt0099371\n",
      "IMDBid: tt2101341\n",
      "IMDBid: tt0099385\n",
      "IMDBid: tt0259288\n",
      "IMDBid: tt1336621\n",
      "IMDBid: tt1087842\n",
      "IMDBid: tt2937696\n",
      "IMDBid: tt1288558\n",
      "IMDBid: tt2524674\n",
      "IMDBid: tt4158096\n",
      "IMDBid: tt3675748\n",
      "IMDBid: tt1800302\n",
      "IMDBid: tt1104733\n",
      "IMDBid: tt0465494\n",
      "IMDBid: tt0498353\n",
      "IMDBid: tt0110093\n",
      "IMDBid: tt5580036\n",
      "IMDBid: tt5962210\n",
      "IMDBid: tt2180411\n",
      "IMDBid: tt0405052\n",
      "IMDBid: tt2713642\n",
      "IMDBid: tt1772925\n",
      "IMDBid: tt2980472\n",
      "IMDBid: tt0988083\n",
      "IMDBid: tt2935564\n",
      "IMDBid: tt0140336\n",
      "IMDBid: tt7248248\n",
      "IMDBid: tt0104839\n",
      "IMDBid: tt1720164\n",
      "IMDBid: tt0113855\n",
      "IMDBid: tt5084170\n",
      "IMDBid: tt0089652\n",
      "IMDBid: tt2504022\n",
      "IMDBid: tt0220827\n",
      "IMDBid: tt0190590\n",
      "IMDBid: tt0284363\n",
      "IMDBid: tt5162658\n",
      "IMDBid: tt1682940\n",
      "IMDBid: tt10327354\n",
      "IMDBid: tt1152836\n",
      "IMDBid: tt1084950\n",
      "IMDBid: tt4687276\n",
      "IMDBid: tt2293750\n",
      "IMDBid: tt0465602\n",
      "IMDBid: tt0367085\n",
      "IMDBid: tt0266987\n",
      "IMDBid: tt1273235\n",
      "IMDBid: tt2126362\n",
      "IMDBid: tt2494280\n",
      "IMDBid: tt0990413\n",
      "IMDBid: tt0859635\n",
      "IMDBid: tt1488555\n",
      "IMDBid: tt4769836\n",
      "IMDBid: tt2350496\n",
      "IMDBid: tt1313139\n",
      "IMDBid: tt2383068\n",
      "IMDBid: tt5580390\n",
      "IMDBid: tt1758795\n",
      "IMDBid: tt5027774\n",
      "IMDBid: tt0181865\n",
      "IMDBid: tt1924429\n",
      "IMDBid: tt1661461\n",
      "IMDBid: tt1403241\n",
      "IMDBid: tt1045778\n",
      "IMDBid: tt0389557\n",
      "Total IMDB IDs with missing files: 79\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataloader (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for text, audio, video, labels in train_dataloader:\n",
    "    # print(f\"Text Shape: {text.shape}\")\n",
    "    # print(f\"Audio Shape: {audio.shape}\")\n",
    "    # print(f\"Video Shape: {video.shape}\")\n",
    "    # print(f\"Labels Shape: {labels.shape}\")\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Features Shape: torch.Size([8, 1024])\n",
      "Audio Features Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Features Shape: torch.Size([8, 141, 768])\n",
      "Labels shape: torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "for text_features, audio_features, video_features, targets in train_dataloader:\n",
    "    print(\"Text Features Shape:\", text_features.shape)\n",
    "    print(\"Audio Features Shape:\", audio_features.shape)\n",
    "    print(\"Video Features Shape:\", video_features.shape)\n",
    "    print(\"Labels shape:\", targets.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Sample:\n",
      "Sample 1:\n",
      "------------------------------\n",
      "Text Data Shape: torch.Size([1024])\n",
      "Audio Data Shape: torch.Size([1, 197, 768])\n",
      "Video Data Shape: torch.Size([89, 768])\n",
      "Label: tensor([1.])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to print a sample from the dataset\n",
    "def print_sample(dataset, index):\n",
    "    text_data, audio_data, video_data, label_data = dataset[index]\n",
    "    print(f\"Sample {index}:\")\n",
    "    # print(\"Text Data:\", text_data)\n",
    "    # print(\"Audio Data:\", audio_data)\n",
    "    # print(\"Video Data:\", video_data)\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Text Data Shape:\", text_data.shape)\n",
    "    print(\"Audio Data Shape:\", audio_data.shape)\n",
    "    print(\"Video Data Shape:\", video_data.shape)\n",
    "    print(\"Label:\", label_data)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Print a sample from each dataset\n",
    "print(\"Training Dataset Sample:\")\n",
    "print_sample(train_dataset, 1)  # Change 5 to any index to view different samples\n",
    "\n",
    "# print(\"Validation Dataset Sample:\")\n",
    "# print_sample(val_dataset, 0)  # Change 5 to any index to view different samples\n",
    "\n",
    "# print(\"Test Dataset Sample:\")\n",
    "# print_sample(test_dataset, 0)  # Change 5 to any index to view different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataLoader Samples:\n",
      "Batch 0:\n",
      "Text Data Shape: torch.Size([8, 1024])\n",
      "Audio Data Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Data Shape: torch.Size([8, 141, 768])\n",
      "Labels: torch.Size([8, 1])\n",
      "------------------------------\n",
      "Batch 1:\n",
      "Text Data Shape: torch.Size([8, 1024])\n",
      "Audio Data Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Data Shape: torch.Size([8, 162, 768])\n",
      "Labels: torch.Size([8, 1])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_dataloader_samples(dataloader, num_batches=1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        text_data, audio_data, video_data, labels_data = batch\n",
    "\n",
    "        # # Convert labels to a list of integers if they are tensors\n",
    "        # if isinstance(labels, torch.Tensor):\n",
    "        #     labels = labels.tolist()\n",
    "\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(\"Text Data Shape:\", text_data.shape)\n",
    "        print(\"Audio Data Shape:\", audio_data.shape)\n",
    "        print(\"Video Data Shape:\", video_data.shape)\n",
    "        print(\"Labels:\", labels_data.shape)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Print a few batches from the training DataLoader\n",
    "print(\"Training DataLoader Samples:\")\n",
    "print_dataloader_samples(train_dataloader, num_batches=2)\n",
    "\n",
    "# # Print a few batches from the validation DataLoader\n",
    "# print(\"Validation DataLoader Samples:\")\n",
    "# print_dataloader_samples(val_dataloader, num_batches=5)\n",
    "\n",
    "# # Print a few batches from the validation DataLoader\n",
    "# print(\"Validation DataLoader Samples:\")\n",
    "# print_dataloader_samples(test_dataloader, num_batches=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMU Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Gated Multimodal Unit of Arevalo et al. (2017)\n",
    "class GatedMultimodalUnit(torch.nn.Module):\n",
    "    def __init__(self, text_dim, audio_dim, video_dim, output_dim):\n",
    "        super(GatedMultimodalUnit, self).__init__()\n",
    "        \n",
    "        # Linear transformation for text\n",
    "        self.text_linear = LinearTransformations(text_dim, output_dim)\n",
    "        \n",
    "        # Convolutional layers for audio and video features\n",
    "        self.audio_conv = nn.Conv1d(audio_dim, output_dim, kernel_size=1)\n",
    "        self.video_conv = nn.Conv1d(video_dim, output_dim, kernel_size=1)\n",
    "        \n",
    "        self.output_dim = output_dim \n",
    "        \n",
    "        # Activation functions\n",
    "        self.activation = nn.Tanh()\n",
    "        self.gate_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Weight matrices for each modality\n",
    "        self.W1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Gating matrices\n",
    "        self.Y1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        init.xavier_uniform_(self.W1)\n",
    "        init.xavier_uniform_(self.W2)\n",
    "        init.xavier_uniform_(self.W3)\n",
    "        \n",
    "        # Initialize gating matrices\n",
    "        init.xavier_uniform_(self.Y1)\n",
    "        init.xavier_uniform_(self.Y2)\n",
    "        init.xavier_uniform_(self.Y3)\n",
    "        \n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "\n",
    "        # Process text features to match shape\n",
    "        x_t = self.text_linear(text_features)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process audio features to match shape\n",
    "        audio_features = audio_features.squeeze(1).permute(0, 2, 1)               # Shape: [batch_size, audio_dim, sequence_length] \n",
    "        x_a = self.audio_conv(audio_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process video features to match shape\n",
    "        video_features = video_features.permute(0, 2, 1)   # Shape: [batch_size, video_dim, sequence_length]\n",
    "        x_v = self.video_conv(video_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    " \n",
    "        h1 = self.activation(torch.matmul(x_t, self.W1))        # Shape: [batch_size, output_dim]\n",
    "        h2 = self.activation(torch.matmul(x_a, self.W2))        # Shape: [batch_size, output_dim]\n",
    "        h3 = self.activation(torch.matmul(x_v, self.W3))        # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Compute modality-specific gating weights\n",
    "        z1 = self.gate_activation(torch.matmul(x_t, self.Y1))  # Shape: [batch_size, output_dim]\n",
    "        z2 = self.gate_activation(torch.matmul(x_a, self.Y2))  # Shape: [batch_size, output_dim]\n",
    "        z3 = self.gate_activation(torch.matmul(x_v, self.Y3))  # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Calculate final output\n",
    "        h = z1 * h1 + z2 * h2 + z3 * h3         \n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "GMU Output Shape: torch.Size([8, 512])\n",
      "GMU Output:  tensor([[ 0.2273,  0.0607,  0.0284,  ..., -0.0692, -0.0312,  0.0102],\n",
      "        [ 0.0216,  0.1029, -0.1472,  ..., -0.0246, -0.0594,  0.1048],\n",
      "        [ 0.0635,  0.0570, -0.1804,  ..., -0.1075, -0.0520, -0.0629],\n",
      "        ...,\n",
      "        [ 0.3142,  0.0715, -0.0241,  ..., -0.1501,  0.0664,  0.0266],\n",
      "        [-0.0191,  0.0056, -0.2571,  ..., -0.1559,  0.0445, -0.1376],\n",
      "        [ 0.1070,  0.0735, -0.1657,  ..., -0.0360, -0.0804, -0.0536]])\n"
     ]
    }
   ],
   "source": [
    "# Test the GMU model using the items from dataloader as input\n",
    "\n",
    "# Define dimensions\n",
    "text_dim = 1024\n",
    "audio_dim = 768  # Number of channels in audio data\n",
    "video_dim = 768  # Number of channels in video data\n",
    "output_dim = 512  # You can set this to any value, depending on your requirements\n",
    "\n",
    "# Instantiate the GMU model\n",
    "gmu = GatedMultimodalUnit(text_dim, audio_dim, video_dim, output_dim)\n",
    "\n",
    "# Use DataLoader to get a batch of data\n",
    "for batch in train_dataloader:  # You can use any DataLoader (train_dataloader, val_dataloader, etc.)\n",
    "    text_data, audio_data, video_data, labels = batch\n",
    "    \n",
    "   \n",
    "    # Feed the entire batch to the GMU model\n",
    "    with torch.no_grad():\n",
    "        output = gmu(text_data, audio_data, video_data)\n",
    "    \n",
    "    # Print the output shape\n",
    "    print('-'*50)\n",
    "    print(\"GMU Output Shape:\", output.shape)\n",
    "    print(\"GMU Output: \", output)\n",
    "    \n",
    "    # Break after the first batch for testing purposes\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:\n",
      "Text file: tt0021814.npy\n",
      "Audio file: feature_tt0021814.npy\n",
      "Video file: tt0021814_features.npy\n",
      "--------------------------------------------------\n",
      "Text Feature Shape: torch.Size([1024])\n",
      "Audio Feature Shape: torch.Size([1, 197, 768])\n",
      "Video Feature Shape torch.Size([95, 768])\n",
      "--------------------------------------------------\n",
      "Model output shape: torch.Size([1, 768]) ###[batch_size, output_dim]\n",
      "--------------------------------------------------\n",
      "Model output: tensor([[-2.9581e-01, -1.1746e-01,  1.3323e-01,  9.9278e-02, -3.0289e-01,\n",
      "         -8.6217e-02, -9.7507e-02,  2.2854e-03, -4.7345e-02,  8.5223e-02,\n",
      "          6.1074e-02,  1.9196e-01, -4.8759e-02,  1.2723e-01, -6.1550e-02,\n",
      "         -2.8465e-01, -1.0836e-01, -1.4581e-01,  2.8069e-01, -5.6658e-02,\n",
      "         -2.1290e-01, -2.9289e-01,  1.4289e-01,  2.8544e-01, -9.2031e-02,\n",
      "         -1.4967e-02,  1.5897e-01, -2.8699e-02, -4.9340e-03, -2.4834e-03,\n",
      "          3.2645e-02, -7.1377e-03,  6.1778e-02,  2.0561e-02,  1.7114e-01,\n",
      "          2.3410e-01,  2.3388e-01,  2.1193e-01,  2.6539e-01,  6.2045e-03,\n",
      "          7.6644e-02,  5.5756e-02,  7.2336e-02,  2.2697e-01, -1.0155e-01,\n",
      "          9.2825e-02,  3.3952e-02,  4.9817e-02,  7.8139e-02,  1.2017e-01,\n",
      "         -4.2016e-02,  7.0347e-02, -7.1178e-03, -3.2208e-01, -6.8237e-02,\n",
      "          3.9256e-02,  2.0295e-01,  6.0062e-02,  2.0730e-02, -1.6045e-01,\n",
      "          2.0097e-02, -2.4452e-02, -3.0980e-01, -1.9181e-01, -1.0499e-01,\n",
      "         -8.2395e-02, -6.5489e-02, -5.3638e-02,  2.4086e-01,  2.3263e-01,\n",
      "         -3.8212e-02,  2.3832e-01, -2.8690e-01,  4.6799e-02, -1.3612e-01,\n",
      "         -1.0962e-01, -3.0739e-02,  8.6286e-02,  2.9573e-01, -5.5855e-03,\n",
      "          1.4274e-01, -8.0993e-02, -1.7976e-01,  1.0485e-01, -4.1203e-02,\n",
      "          4.6980e-02, -2.3011e-02,  6.0436e-02,  1.5247e-01, -2.1075e-01,\n",
      "          2.1661e-01,  1.6209e-01,  2.5466e-01, -8.3727e-02,  2.5437e-01,\n",
      "          9.8098e-02,  6.9118e-02, -2.8574e-01, -1.6080e-01,  1.8624e-02,\n",
      "         -5.4210e-02, -1.2474e-01,  5.1795e-02,  2.0208e-01, -1.2517e-01,\n",
      "          2.8350e-01, -1.5094e-01,  1.4111e-01, -1.3172e-01, -1.1157e-02,\n",
      "         -1.6895e-01,  1.5599e-01, -1.0439e-01,  1.2963e-01, -4.1358e-02,\n",
      "         -1.1265e-01, -1.8729e-01,  5.1468e-02, -1.1175e-01,  7.2625e-02,\n",
      "          6.5578e-02,  1.7368e-02,  1.1912e-01, -1.1371e-02,  1.9924e-01,\n",
      "          2.0977e-02, -1.7028e-01,  1.3919e-01,  4.8453e-02,  2.8129e-02,\n",
      "         -2.9110e-02, -1.2723e-01, -4.6929e-02,  1.0087e-01, -3.4888e-02,\n",
      "          1.6815e-01, -1.6579e-01, -8.1151e-02,  2.6996e-01, -1.4768e-01,\n",
      "          8.2470e-03,  2.3729e-01, -6.7895e-03, -1.2829e-01, -3.3520e-01,\n",
      "         -1.3078e-01,  3.2636e-01, -2.4665e-01,  4.5063e-02, -3.7412e-01,\n",
      "         -4.3391e-02, -2.9403e-02, -3.0125e-01,  4.3874e-02,  2.9157e-01,\n",
      "         -3.5261e-03,  6.4849e-02,  7.3767e-02, -1.4965e-01,  4.2554e-02,\n",
      "         -4.3417e-02,  6.2822e-02, -6.7544e-02, -3.6036e-02, -3.2027e-01,\n",
      "          1.7022e-01,  1.7253e-02,  4.8157e-03, -1.7459e-01, -1.1038e-01,\n",
      "         -4.0690e-01,  2.1750e-01, -8.5489e-02,  1.5113e-01, -1.1716e-01,\n",
      "          5.3193e-02, -5.1264e-02,  4.0872e-01, -1.0729e-01, -2.3108e-01,\n",
      "          4.4484e-02,  9.2503e-03, -2.3396e-01,  2.2620e-02,  3.8490e-02,\n",
      "         -4.9036e-01, -7.0564e-02,  7.5558e-02, -5.9399e-02,  8.3780e-02,\n",
      "         -1.1905e-01, -3.3421e-02,  2.9547e-01, -1.9110e-02, -1.0337e-01,\n",
      "          2.1138e-02,  6.7663e-02,  2.6128e-01, -3.8726e-02, -3.9852e-01,\n",
      "          3.7175e-01, -6.0112e-03,  3.8546e-02, -4.4026e-02, -4.2941e-02,\n",
      "         -1.2725e-01, -8.7994e-03,  4.5352e-02,  1.6567e-01, -7.6213e-02,\n",
      "         -6.0566e-02, -1.8858e-01, -3.6889e-02,  8.8832e-02, -1.9073e-01,\n",
      "         -1.5108e-01, -1.1600e-02,  1.2207e-01,  4.5990e-02,  1.5561e-01,\n",
      "          2.3526e-02,  7.6800e-02,  5.3596e-02,  2.3058e-01,  1.6647e-01,\n",
      "          2.0973e-01, -5.6497e-02, -5.8534e-02, -1.7662e-01, -8.8062e-02,\n",
      "          8.8237e-02, -2.3481e-01, -4.0224e-02,  1.2102e-01,  6.9246e-02,\n",
      "          2.3272e-01, -1.1696e-01,  4.2893e-02, -2.5363e-02, -4.1866e-02,\n",
      "         -8.8172e-02,  3.4192e-01, -1.3394e-01, -1.1755e-01, -1.1593e-01,\n",
      "          4.0537e-02,  2.2016e-02, -5.1061e-02, -2.5008e-01,  4.9737e-02,\n",
      "         -1.6758e-02,  3.8184e-01,  4.6748e-02, -1.4578e-01,  1.4474e-01,\n",
      "         -9.6297e-02,  1.1611e-01, -2.3049e-02,  3.3596e-01,  1.4667e-01,\n",
      "         -4.1716e-01,  5.8972e-02, -1.4903e-01, -1.8685e-01,  1.8194e-01,\n",
      "          1.5008e-01, -1.4531e-03,  9.3370e-02, -3.3429e-02,  2.4177e-02,\n",
      "         -1.9271e-01,  7.0249e-02,  3.8934e-02,  1.2502e-03, -2.7570e-01,\n",
      "         -1.9605e-01,  6.6491e-02,  1.1109e-01, -2.0564e-02, -7.2450e-02,\n",
      "         -7.6894e-02,  2.3994e-02, -1.9816e-01,  3.8673e-01, -3.7721e-04,\n",
      "          7.9068e-02, -5.3780e-02,  2.1499e-01,  5.4868e-02, -1.4110e-01,\n",
      "          6.8889e-02, -6.0705e-02,  1.0891e-01, -1.6805e-01, -1.2819e-01,\n",
      "          1.8674e-02, -8.9276e-02,  9.8289e-02, -5.4245e-02, -1.7879e-01,\n",
      "          4.9389e-02,  4.5237e-02,  1.6028e-01, -1.3538e-01, -1.6179e-01,\n",
      "         -5.9020e-03,  2.7208e-01, -1.8698e-02, -1.0963e-01, -2.7143e-02,\n",
      "         -1.6998e-01,  2.6052e-01,  1.5233e-01, -2.8456e-01,  1.4213e-02,\n",
      "          2.1751e-01, -6.8310e-02, -4.6369e-02,  1.2351e-02, -9.4060e-03,\n",
      "         -6.2259e-03,  1.7826e-01, -1.0736e-01, -7.1303e-02, -2.3280e-02,\n",
      "         -3.2895e-01,  1.8902e-01,  2.5952e-01, -1.1725e-01,  2.0959e-02,\n",
      "          1.3635e-01, -2.7133e-02, -1.0366e-02, -1.9240e-01, -1.0440e-02,\n",
      "          2.6779e-01,  1.8392e-01, -1.1703e-01, -1.6979e-01, -1.3372e-01,\n",
      "          2.9453e-02, -7.3986e-02, -5.5249e-02,  1.4196e-01, -8.0577e-03,\n",
      "          1.3802e-02, -1.0451e-01,  1.6681e-01,  5.7758e-02, -1.1626e-01,\n",
      "         -1.2465e-01,  3.5728e-02,  9.6292e-02,  1.3194e-01, -1.9178e-01,\n",
      "          1.1501e-01, -1.3443e-02,  1.1339e-03,  1.3857e-01,  6.2009e-02,\n",
      "         -5.5220e-02,  8.7752e-03,  1.2053e-01, -1.1735e-02, -5.3130e-02,\n",
      "          5.6587e-02,  1.4208e-01, -7.1923e-02, -1.2988e-01, -1.4877e-01,\n",
      "          1.5747e-01,  2.3200e-03, -4.8977e-02,  3.4384e-02,  2.6582e-03,\n",
      "          1.9639e-02,  2.1288e-01, -8.9579e-02, -1.9537e-01,  4.4299e-02,\n",
      "          1.8337e-01, -5.2774e-02,  2.2496e-01,  1.0349e-01,  1.6612e-01,\n",
      "          1.8009e-01, -2.2532e-02,  7.1700e-02, -1.8968e-01,  3.5732e-02,\n",
      "         -9.6845e-02,  1.3558e-01,  1.8689e-01, -3.2427e-01,  2.2196e-01,\n",
      "          1.8717e-01, -2.4668e-02,  6.2916e-02, -2.0846e-01, -1.2240e-01,\n",
      "         -1.2078e-01, -7.3810e-04, -8.5376e-02, -1.3897e-01,  6.7178e-03,\n",
      "         -2.0895e-01,  4.2068e-02, -1.1203e-02,  3.8396e-02,  9.5064e-02,\n",
      "          1.8881e-01,  7.7334e-02,  1.7000e-01, -1.4529e-01, -1.5688e-02,\n",
      "         -1.4187e-01,  8.3993e-02,  8.5620e-02,  4.3164e-02, -4.0749e-02,\n",
      "         -1.5431e-01, -6.6050e-02, -1.4724e-01, -2.5539e-01, -1.2501e-01,\n",
      "          2.5447e-01,  2.4066e-01, -2.0431e-01,  1.0906e-01, -7.6233e-02,\n",
      "          9.8061e-02, -2.0653e-01,  7.8636e-02, -2.2246e-02,  1.7929e-01,\n",
      "         -2.2414e-01,  5.9535e-02,  8.8214e-02,  3.7130e-01, -9.1975e-02,\n",
      "         -2.2205e-02, -1.5936e-01,  2.7376e-01, -2.1904e-01, -1.4422e-01,\n",
      "          1.4670e-02,  7.5915e-02, -8.6693e-02,  1.4714e-01, -6.3138e-02,\n",
      "          1.3784e-01, -1.4534e-01, -2.6383e-01, -4.5591e-02, -3.1750e-01,\n",
      "         -4.8571e-02,  1.3003e-01,  2.0052e-02,  2.3107e-01,  7.7313e-03,\n",
      "         -3.6806e-02, -1.6058e-01, -7.4240e-02, -3.2916e-02, -4.9078e-02,\n",
      "          1.5115e-01, -1.2177e-01, -7.3075e-02,  4.5591e-02, -1.7170e-01,\n",
      "          1.5426e-01, -4.4344e-02, -6.0494e-02,  1.7395e-01, -9.2108e-02,\n",
      "         -1.6276e-02, -3.1580e-01, -7.2664e-03, -3.8923e-02,  2.9461e-01,\n",
      "          6.2806e-02, -1.5628e-01,  2.1433e-01, -2.6007e-01,  3.2428e-01,\n",
      "         -3.6531e-02,  2.7196e-01,  7.9726e-04, -5.3965e-02,  9.0113e-02,\n",
      "         -6.4494e-02,  1.5984e-01,  8.6884e-02,  1.3186e-01,  1.4173e-01,\n",
      "         -1.8899e-02, -1.6234e-01, -3.2378e-01,  9.2632e-02,  1.3627e-01,\n",
      "         -1.3805e-01, -9.2302e-02, -1.4383e-02,  2.8330e-01,  8.6763e-02,\n",
      "          9.5729e-03,  1.1606e-02,  1.9664e-01, -5.4569e-02,  1.6001e-01,\n",
      "         -7.6049e-02, -2.0618e-01, -2.3584e-02, -2.1188e-01, -5.3769e-02,\n",
      "         -1.2518e-01,  2.2959e-01, -5.7641e-02, -3.8876e-02, -1.0576e-01,\n",
      "          1.5787e-01, -2.4381e-01, -1.5583e-01,  1.9801e-01,  1.0562e-01,\n",
      "         -2.0769e-01,  6.8591e-03, -4.2950e-02,  4.4259e-02,  7.2313e-02,\n",
      "         -1.5206e-03, -2.0954e-02,  2.7551e-03,  2.2456e-01,  8.0470e-02,\n",
      "          3.6525e-02, -2.2077e-01,  3.3052e-01, -3.4111e-02,  3.2660e-01,\n",
      "          4.8722e-02, -2.4300e-01,  1.0558e-01, -1.1161e-01, -1.7037e-01,\n",
      "          1.4392e-01,  1.1667e-01, -2.8548e-01,  1.6361e-03, -8.3213e-02,\n",
      "         -1.9247e-01, -5.5355e-02, -1.8527e-01,  2.2455e-01, -2.2027e-01,\n",
      "          1.8279e-01,  1.8542e-01, -6.1921e-02, -7.6359e-02, -5.3458e-02,\n",
      "          9.6761e-02,  1.7997e-01,  1.0024e-01, -7.5249e-03,  2.6026e-02,\n",
      "         -4.4442e-02,  2.1992e-02, -1.7116e-01,  8.5419e-02,  5.8779e-02,\n",
      "         -5.9809e-02,  3.5885e-02, -2.8949e-01, -6.2942e-02,  1.1221e-01,\n",
      "         -1.3927e-03, -2.1764e-01, -8.3567e-02,  1.9434e-01, -7.7932e-02,\n",
      "          1.4378e-01, -3.1433e-01, -2.0698e-02,  1.1263e-01,  1.6400e-01,\n",
      "          1.8130e-01,  3.6864e-02,  3.8373e-02,  8.5991e-03,  2.1513e-01,\n",
      "          8.7435e-02,  1.1281e-01,  1.5987e-01, -1.2760e-01, -4.0777e-02,\n",
      "         -2.8114e-02, -7.1056e-02, -4.2487e-02,  2.0395e-01,  1.1444e-01,\n",
      "         -5.3218e-02,  1.8099e-01,  4.3849e-02, -1.1426e-01,  2.3788e-01,\n",
      "         -3.6477e-01,  9.2200e-02, -1.1832e-01,  4.0452e-02, -3.3185e-02,\n",
      "          1.4805e-01,  1.4414e-02, -2.3227e-02, -2.1366e-01, -1.4459e-02,\n",
      "          1.0319e-01,  1.4538e-01,  7.7349e-02, -2.9518e-01,  5.2882e-02,\n",
      "         -2.5100e-01,  8.3183e-02,  4.0502e-02, -1.9210e-01,  2.6402e-01,\n",
      "         -1.9832e-01,  2.4014e-01,  4.8386e-02,  1.6630e-01,  6.4522e-02,\n",
      "          9.1535e-02, -4.8962e-02,  9.5153e-02, -2.5899e-02,  1.6129e-01,\n",
      "          1.8911e-01,  8.3283e-02, -1.2592e-01,  2.3871e-01, -7.3687e-02,\n",
      "         -6.0403e-02, -1.9056e-01, -9.9206e-02,  1.1244e-02, -1.4154e-01,\n",
      "         -3.9308e-02, -1.8692e-01,  7.2477e-02,  6.4352e-02,  1.3910e-01,\n",
      "          3.6536e-02,  2.0001e-01,  2.5878e-01, -1.3294e-01, -1.4892e-01,\n",
      "         -2.7118e-02, -8.0602e-02, -2.3148e-01, -2.4555e-01, -3.6678e-02,\n",
      "          2.5237e-01,  1.5402e-01,  1.3312e-01, -1.9625e-01, -2.3001e-01,\n",
      "          1.0597e-01, -2.8286e-02,  2.2290e-01, -1.6894e-02, -1.6065e-01,\n",
      "         -1.0057e-01,  1.3638e-03, -9.4132e-02,  1.5067e-02, -5.2848e-02,\n",
      "         -5.7002e-02, -2.5622e-01, -1.6657e-01,  2.5700e-01,  3.6663e-01,\n",
      "         -2.4658e-01,  2.4778e-01, -1.0654e-01, -2.3896e-01,  3.2776e-02,\n",
      "         -9.4343e-02, -1.3233e-01, -5.1509e-02, -2.1413e-02,  3.7344e-02,\n",
      "          5.6302e-02, -7.3751e-02, -1.2621e-01, -1.4591e-01,  2.1444e-04,\n",
      "          1.7227e-01,  1.2508e-02, -4.5007e-02,  1.3141e-02,  1.0774e-02,\n",
      "          1.9389e-01,  1.7673e-01,  1.9228e-01,  5.3953e-02, -2.7669e-02,\n",
      "         -9.6639e-02, -1.8116e-01, -5.2519e-02,  1.3161e-01,  1.0669e-01,\n",
      "         -1.0225e-01,  9.0381e-02, -2.3341e-01, -1.7661e-02,  1.3390e-01,\n",
      "          5.1266e-02,  8.8800e-02,  9.9724e-02, -1.2597e-01, -9.9451e-02,\n",
      "          8.9541e-02, -1.1265e-01, -1.6283e-02,  2.8239e-01,  1.2236e-01,\n",
      "          1.4777e-01, -1.1048e-01, -2.8593e-02, -9.2902e-02,  8.2526e-02,\n",
      "          1.7849e-01, -8.7242e-02,  1.8630e-01,  4.0385e-02, -2.6440e-01,\n",
      "          1.2333e-01,  4.2654e-02,  9.5707e-03,  4.5522e-02, -1.6502e-01,\n",
      "         -1.3479e-02, -1.0048e-02, -2.4385e-01,  2.1068e-01, -1.5963e-01,\n",
      "          2.2606e-01, -2.2862e-01, -8.1863e-02,  5.2204e-02, -1.2914e-01,\n",
      "         -1.0057e-01,  1.2199e-01, -1.0864e-01,  4.0931e-02,  1.7874e-01,\n",
      "          3.6253e-01, -1.9734e-01, -1.0479e-02, -1.4184e-01,  2.2663e-02,\n",
      "         -7.9859e-02,  8.4602e-02, -4.4222e-03,  4.5048e-02, -1.1186e-01,\n",
      "          3.0872e-01, -4.9409e-02, -8.2368e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from modules.dataloader import load_npy_files\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "# Select the first file from each modality directories (for testing) [insert index]\n",
    "text_file_name, text_features = text_features[0]\n",
    "audio_file_name, audio_features = audio_features[0]\n",
    "video_file_name, video_features = video_features[0]\n",
    "\n",
    "print(\"Selected File:\")\n",
    "print(\"Text file:\", os.path.basename(text_file_name))\n",
    "print(\"Audio file:\", os.path.basename(audio_file_name))\n",
    "print(\"Video file:\", os.path.basename(video_file_name))\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "# Define dimensions (make sure these match your model's expected input sizes)\n",
    "text_dim = 1024\n",
    "audio_dim = 768  # Number of channels in audio data\n",
    "video_dim = 768  # Number of channels in video data\n",
    "output_dim = 768  # You can set this to any value, depending on your requirements\n",
    "\n",
    "# Initialize the GMU model\n",
    "model = GatedMultimodalUnit(text_dim, audio_dim, video_dim, output_dim)\n",
    "\n",
    "# Move model to the same device as your data (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare the selected data samples\n",
    "text_features = text_features.to(device)  # Convert to tensor and move to device\n",
    "audio_features = audio_features.to(device)  # Convert to tensor and move to device\n",
    "video_features = video_features.to(device)  # Convert to tensor and move to device\n",
    "\n",
    "print(\"Text Feature Shape:\", text_features.shape)\n",
    "print(\"Audio Feature Shape:\", audio_features.shape)\n",
    "print(\"Video Feature Shape\", video_features.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Pass the data through the GMU model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    output = model(text_features.unsqueeze(0), audio_features, video_features.unsqueeze(0))\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Model output shape:\", output.shape, \"###[batch_size, output_dim]\")\n",
    "print(\"-\"*50)\n",
    "print(\"Model output:\", output) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-3):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dense_layer, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    dense_layer.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass inputs through GMU model\n",
    "        outputs = model(text_features, audio_features, video_features)\n",
    "        \n",
    "        # Pass the GMU outputs through the dense layer to get final predictions\n",
    "        predictions = dense_layer(outputs)    # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Training Loss: {average_loss:.4f}\")\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).squeeze()\n",
    "            )\n",
    "\n",
    "            # Pass inputs through GMU model\n",
    "            outputs = model(text_features, audio_features, video_features)\n",
    "            \n",
    "            # Pass the GMU outputs through the dense layer to get final predictions\n",
    "            predictions = dense_layer(outputs).squeeze()  \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Evaluation Loss: {average_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return average_loss, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(\n",
    "    text_dim, \n",
    "    audio_dim, \n",
    "    video_dim, \n",
    "    output_dim, \n",
    "    dataset, \n",
    "    num_folds, \n",
    "    collate_fn, \n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    output_file,\n",
    "    device=None\n",
    "):\n",
    "    # Set device configuration\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  # Creates the directory if it does not exist\n",
    "    \n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # lists to store metrics for each fold\n",
    "    fold_losses = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1_scores = []\n",
    "    f1_scores_per_fold = []  # List to store F1 scores for each fold\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"------ Fold {fold + 1 }/{num_folds} ------\")\n",
    "\n",
    "        # Create data loaders for the train and validation sets\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "        val_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=collate_fn)\n",
    "        \n",
    "        # Initialize the model, dense_layer, criterion, and optimizer\n",
    "        model = GatedMultimodalUnit(text_dim=text_dim, audio_dim=audio_dim, video_dim=video_dim, output_dim=output_dim).to(device)\n",
    "        \n",
    "        dense_layer = DenseLayer(input_size=output_dim).to(device)\n",
    "        criterion = BCELoss()\n",
    "        optimizer = get_optimizer(list(model.parameters()) + list(dense_layer.parameters()), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            train_loss = train_model(model, dense_layer, train_dataloader, criterion, optimizer, device)\n",
    "            val_loss, precision, recall, f1_score = evaluate_model(model, dense_layer, val_dataloader, criterion, device)\n",
    "            \n",
    "            print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "        \n",
    "        # Accumulate metrics after the final epoch of the fold\n",
    "        print(f\"Results for Fold {fold}: Validation Loss = {val_loss:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1_score:.4f}\")\n",
    "        fold_losses.append(val_loss)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_recalls.append(recall)\n",
    "        fold_f1_scores.append(f1_score)\n",
    "        f1_scores_per_fold.append(f1_score)  # Save F1 score for the current fold\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "    avg_precision = np.mean(fold_precisions)\n",
    "    avg_recall = np.mean(fold_recalls)\n",
    "    avg_f1_score = np.mean(fold_f1_scores)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n",
    "    \n",
    "    # Save F1 scores per fold to a .npy file\n",
    "    np.save(output_file, np.array(f1_scores_per_fold))\n",
    "    print(f\"F1 scores per fold saved to {output_file}\")\n",
    "    \n",
    "    return avg_loss, avg_precision, avg_recall, avg_f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "--------------------------------------------------\n",
      "------ Fold 1/5 ------\n",
      "Epoch 1/10\n",
      "Training Loss: 0.5041\n",
      "Evaluation Loss: 0.3499\n",
      "Precision: 0.6613\n",
      "Recall: 0.6613\n",
      "F1 Score: 0.6613\n",
      "Training Loss: 0.5041, Validation Loss: 0.3499\n",
      "Precision: 0.6613, Recall: 0.6613, F1 Score: 0.6613\n",
      "Epoch 2/10\n",
      "Training Loss: 0.3459\n",
      "Evaluation Loss: 0.2907\n",
      "Precision: 0.7843\n",
      "Recall: 0.6452\n",
      "F1 Score: 0.7080\n",
      "Training Loss: 0.3459, Validation Loss: 0.2907\n",
      "Precision: 0.7843, Recall: 0.6452, F1 Score: 0.7080\n",
      "Epoch 3/10\n",
      "Training Loss: 0.2679\n",
      "Evaluation Loss: 0.2306\n",
      "Precision: 0.7833\n",
      "Recall: 0.7581\n",
      "F1 Score: 0.7705\n",
      "Training Loss: 0.2679, Validation Loss: 0.2306\n",
      "Precision: 0.7833, Recall: 0.7581, F1 Score: 0.7705\n",
      "Epoch 4/10\n",
      "Training Loss: 0.2289\n",
      "Evaluation Loss: 0.2507\n",
      "Precision: 0.7059\n",
      "Recall: 0.7742\n",
      "F1 Score: 0.7385\n",
      "Training Loss: 0.2289, Validation Loss: 0.2507\n",
      "Precision: 0.7059, Recall: 0.7742, F1 Score: 0.7385\n",
      "Epoch 5/10\n",
      "Training Loss: 0.2230\n",
      "Evaluation Loss: 0.3444\n",
      "Precision: 0.6585\n",
      "Recall: 0.8710\n",
      "F1 Score: 0.7500\n",
      "Training Loss: 0.2230, Validation Loss: 0.3444\n",
      "Precision: 0.6585, Recall: 0.8710, F1 Score: 0.7500\n",
      "Epoch 6/10\n",
      "Training Loss: 0.1609\n",
      "Evaluation Loss: 0.2960\n",
      "Precision: 0.8478\n",
      "Recall: 0.6290\n",
      "F1 Score: 0.7222\n",
      "Training Loss: 0.1609, Validation Loss: 0.2960\n",
      "Precision: 0.8478, Recall: 0.6290, F1 Score: 0.7222\n",
      "Epoch 7/10\n",
      "Training Loss: 0.1253\n",
      "Evaluation Loss: 0.2552\n",
      "Precision: 0.8364\n",
      "Recall: 0.7419\n",
      "F1 Score: 0.7863\n",
      "Training Loss: 0.1253, Validation Loss: 0.2552\n",
      "Precision: 0.8364, Recall: 0.7419, F1 Score: 0.7863\n",
      "Epoch 8/10\n",
      "Training Loss: 0.0881\n",
      "Evaluation Loss: 0.3591\n",
      "Precision: 0.7742\n",
      "Recall: 0.7742\n",
      "F1 Score: 0.7742\n",
      "Training Loss: 0.0881, Validation Loss: 0.3591\n",
      "Precision: 0.7742, Recall: 0.7742, F1 Score: 0.7742\n",
      "Epoch 9/10\n",
      "Training Loss: 0.0754\n",
      "Evaluation Loss: 0.3735\n",
      "Precision: 0.8627\n",
      "Recall: 0.7097\n",
      "F1 Score: 0.7788\n",
      "Training Loss: 0.0754, Validation Loss: 0.3735\n",
      "Precision: 0.8627, Recall: 0.7097, F1 Score: 0.7788\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0639\n",
      "Evaluation Loss: 0.3538\n",
      "Precision: 0.7273\n",
      "Recall: 0.7742\n",
      "F1 Score: 0.7500\n",
      "Training Loss: 0.0639, Validation Loss: 0.3538\n",
      "Precision: 0.7273, Recall: 0.7742, F1 Score: 0.7500\n",
      "Results for Fold 0: Validation Loss = 0.3538, Precision = 0.7273, Recall = 0.7742, F1 Score = 0.7500\n",
      "--------------------------------------------------\n",
      "------ Fold 2/5 ------\n",
      "Epoch 1/10\n",
      "Training Loss: 0.5294\n",
      "Evaluation Loss: 0.3944\n",
      "Precision: 0.8043\n",
      "Recall: 0.5286\n",
      "F1 Score: 0.6379\n",
      "Training Loss: 0.5294, Validation Loss: 0.3944\n",
      "Precision: 0.8043, Recall: 0.5286, F1 Score: 0.6379\n",
      "Epoch 2/10\n",
      "Training Loss: 0.3398\n",
      "Evaluation Loss: 0.3773\n",
      "Precision: 0.6753\n",
      "Recall: 0.7429\n",
      "F1 Score: 0.7075\n",
      "Training Loss: 0.3398, Validation Loss: 0.3773\n",
      "Precision: 0.6753, Recall: 0.7429, F1 Score: 0.7075\n",
      "Epoch 3/10\n",
      "Training Loss: 0.3036\n",
      "Evaluation Loss: 0.4424\n",
      "Precision: 0.5714\n",
      "Recall: 0.8571\n",
      "F1 Score: 0.6857\n",
      "Training Loss: 0.3036, Validation Loss: 0.4424\n",
      "Precision: 0.5714, Recall: 0.8571, F1 Score: 0.6857\n",
      "Epoch 4/10\n",
      "Training Loss: 0.2221\n",
      "Evaluation Loss: 0.3297\n",
      "Precision: 0.7778\n",
      "Recall: 0.7000\n",
      "F1 Score: 0.7368\n",
      "Training Loss: 0.2221, Validation Loss: 0.3297\n",
      "Precision: 0.7778, Recall: 0.7000, F1 Score: 0.7368\n",
      "Epoch 5/10\n",
      "Training Loss: 0.1997\n",
      "Evaluation Loss: 0.4930\n",
      "Precision: 0.5676\n",
      "Recall: 0.9000\n",
      "F1 Score: 0.6961\n",
      "Training Loss: 0.1997, Validation Loss: 0.4930\n",
      "Precision: 0.5676, Recall: 0.9000, F1 Score: 0.6961\n",
      "Epoch 6/10\n",
      "Training Loss: 0.1748\n",
      "Evaluation Loss: 0.4205\n",
      "Precision: 0.7736\n",
      "Recall: 0.5857\n",
      "F1 Score: 0.6667\n",
      "Training Loss: 0.1748, Validation Loss: 0.4205\n",
      "Precision: 0.7736, Recall: 0.5857, F1 Score: 0.6667\n",
      "Epoch 7/10\n",
      "Training Loss: 0.1203\n",
      "Evaluation Loss: 0.4522\n",
      "Precision: 0.7377\n",
      "Recall: 0.6429\n",
      "F1 Score: 0.6870\n",
      "Training Loss: 0.1203, Validation Loss: 0.4522\n",
      "Precision: 0.7377, Recall: 0.6429, F1 Score: 0.6870\n",
      "Epoch 8/10\n",
      "Training Loss: 0.1091\n",
      "Evaluation Loss: 0.4237\n",
      "Precision: 0.7361\n",
      "Recall: 0.7571\n",
      "F1 Score: 0.7465\n",
      "Training Loss: 0.1091, Validation Loss: 0.4237\n",
      "Precision: 0.7361, Recall: 0.7571, F1 Score: 0.7465\n",
      "Epoch 9/10\n",
      "Training Loss: 0.0823\n",
      "Evaluation Loss: 0.6764\n",
      "Precision: 0.6304\n",
      "Recall: 0.8286\n",
      "F1 Score: 0.7160\n",
      "Training Loss: 0.0823, Validation Loss: 0.6764\n",
      "Precision: 0.6304, Recall: 0.8286, F1 Score: 0.7160\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0821\n",
      "Evaluation Loss: 0.5015\n",
      "Precision: 0.7206\n",
      "Recall: 0.7000\n",
      "F1 Score: 0.7101\n",
      "Training Loss: 0.0821, Validation Loss: 0.5015\n",
      "Precision: 0.7206, Recall: 0.7000, F1 Score: 0.7101\n",
      "Results for Fold 1: Validation Loss = 0.5015, Precision = 0.7206, Recall = 0.7000, F1 Score = 0.7101\n",
      "--------------------------------------------------\n",
      "------ Fold 3/5 ------\n",
      "Epoch 1/10\n",
      "Training Loss: 0.5704\n",
      "Evaluation Loss: 0.4815\n",
      "Precision: 0.5521\n",
      "Recall: 0.8030\n",
      "F1 Score: 0.6543\n",
      "Training Loss: 0.5704, Validation Loss: 0.4815\n",
      "Precision: 0.5521, Recall: 0.8030, F1 Score: 0.6543\n",
      "Epoch 2/10\n",
      "Training Loss: 0.3687\n",
      "Evaluation Loss: 0.3013\n",
      "Precision: 0.9429\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.6535\n",
      "Training Loss: 0.3687, Validation Loss: 0.3013\n",
      "Precision: 0.9429, Recall: 0.5000, F1 Score: 0.6535\n",
      "Epoch 3/10\n",
      "Training Loss: 0.3226\n",
      "Evaluation Loss: 0.2969\n",
      "Precision: 0.9268\n",
      "Recall: 0.5758\n",
      "F1 Score: 0.7103\n",
      "Training Loss: 0.3226, Validation Loss: 0.2969\n",
      "Precision: 0.9268, Recall: 0.5758, F1 Score: 0.7103\n",
      "Epoch 4/10\n",
      "Training Loss: 0.2311\n",
      "Evaluation Loss: 0.2566\n",
      "Precision: 0.8167\n",
      "Recall: 0.7424\n",
      "F1 Score: 0.7778\n",
      "Training Loss: 0.2311, Validation Loss: 0.2566\n",
      "Precision: 0.8167, Recall: 0.7424, F1 Score: 0.7778\n",
      "Epoch 5/10\n",
      "Training Loss: 0.2120\n",
      "Evaluation Loss: 0.2705\n",
      "Precision: 0.8167\n",
      "Recall: 0.7424\n",
      "F1 Score: 0.7778\n",
      "Training Loss: 0.2120, Validation Loss: 0.2705\n",
      "Precision: 0.8167, Recall: 0.7424, F1 Score: 0.7778\n",
      "Epoch 6/10\n",
      "Training Loss: 0.1901\n",
      "Evaluation Loss: 0.2466\n",
      "Precision: 0.8846\n",
      "Recall: 0.6970\n",
      "F1 Score: 0.7797\n",
      "Training Loss: 0.1901, Validation Loss: 0.2466\n",
      "Precision: 0.8846, Recall: 0.6970, F1 Score: 0.7797\n",
      "Epoch 7/10\n",
      "Training Loss: 0.1478\n",
      "Evaluation Loss: 0.3041\n",
      "Precision: 0.8627\n",
      "Recall: 0.6667\n",
      "F1 Score: 0.7521\n",
      "Training Loss: 0.1478, Validation Loss: 0.3041\n",
      "Precision: 0.8627, Recall: 0.6667, F1 Score: 0.7521\n",
      "Epoch 8/10\n",
      "Training Loss: 0.1389\n",
      "Evaluation Loss: 0.2708\n",
      "Precision: 0.8333\n",
      "Recall: 0.7576\n",
      "F1 Score: 0.7937\n",
      "Training Loss: 0.1389, Validation Loss: 0.2708\n",
      "Precision: 0.8333, Recall: 0.7576, F1 Score: 0.7937\n",
      "Epoch 9/10\n",
      "Training Loss: 0.0887\n",
      "Evaluation Loss: 0.4082\n",
      "Precision: 0.8269\n",
      "Recall: 0.6515\n",
      "F1 Score: 0.7288\n",
      "Training Loss: 0.0887, Validation Loss: 0.4082\n",
      "Precision: 0.8269, Recall: 0.6515, F1 Score: 0.7288\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0820\n",
      "Evaluation Loss: 0.4116\n",
      "Precision: 0.8431\n",
      "Recall: 0.6515\n",
      "F1 Score: 0.7350\n",
      "Training Loss: 0.0820, Validation Loss: 0.4116\n",
      "Precision: 0.8431, Recall: 0.6515, F1 Score: 0.7350\n",
      "Results for Fold 2: Validation Loss = 0.4116, Precision = 0.8431, Recall = 0.6515, F1 Score = 0.7350\n",
      "--------------------------------------------------\n",
      "------ Fold 4/5 ------\n",
      "Epoch 1/10\n",
      "Training Loss: 0.5358\n",
      "Evaluation Loss: 0.4663\n",
      "Precision: 0.7273\n",
      "Recall: 0.6575\n",
      "F1 Score: 0.6906\n",
      "Training Loss: 0.5358, Validation Loss: 0.4663\n",
      "Precision: 0.7273, Recall: 0.6575, F1 Score: 0.6906\n",
      "Epoch 2/10\n",
      "Training Loss: 0.3382\n",
      "Evaluation Loss: 0.3909\n",
      "Precision: 0.9143\n",
      "Recall: 0.4384\n",
      "F1 Score: 0.5926\n",
      "Training Loss: 0.3382, Validation Loss: 0.3909\n",
      "Precision: 0.9143, Recall: 0.4384, F1 Score: 0.5926\n",
      "Epoch 3/10\n",
      "Training Loss: 0.2600\n",
      "Evaluation Loss: 0.3250\n",
      "Precision: 0.9149\n",
      "Recall: 0.5890\n",
      "F1 Score: 0.7167\n",
      "Training Loss: 0.2600, Validation Loss: 0.3250\n",
      "Precision: 0.9149, Recall: 0.5890, F1 Score: 0.7167\n",
      "Epoch 4/10\n",
      "Training Loss: 0.2177\n",
      "Evaluation Loss: 0.3489\n",
      "Precision: 0.9348\n",
      "Recall: 0.5890\n",
      "F1 Score: 0.7227\n",
      "Training Loss: 0.2177, Validation Loss: 0.3489\n",
      "Precision: 0.9348, Recall: 0.5890, F1 Score: 0.7227\n",
      "Epoch 5/10\n",
      "Training Loss: 0.1796\n",
      "Evaluation Loss: 0.3822\n",
      "Precision: 0.6824\n",
      "Recall: 0.7945\n",
      "F1 Score: 0.7342\n",
      "Training Loss: 0.1796, Validation Loss: 0.3822\n",
      "Precision: 0.6824, Recall: 0.7945, F1 Score: 0.7342\n",
      "Epoch 6/10\n",
      "Training Loss: 0.1776\n",
      "Evaluation Loss: 0.2895\n",
      "Precision: 0.7703\n",
      "Recall: 0.7808\n",
      "F1 Score: 0.7755\n",
      "Training Loss: 0.1776, Validation Loss: 0.2895\n",
      "Precision: 0.7703, Recall: 0.7808, F1 Score: 0.7755\n",
      "Epoch 7/10\n",
      "Training Loss: 0.1307\n",
      "Evaluation Loss: 0.3679\n",
      "Precision: 0.6705\n",
      "Recall: 0.8082\n",
      "F1 Score: 0.7329\n",
      "Training Loss: 0.1307, Validation Loss: 0.3679\n",
      "Precision: 0.6705, Recall: 0.8082, F1 Score: 0.7329\n",
      "Epoch 8/10\n",
      "Training Loss: 0.1318\n",
      "Evaluation Loss: 0.3049\n",
      "Precision: 0.8125\n",
      "Recall: 0.7123\n",
      "F1 Score: 0.7591\n",
      "Training Loss: 0.1318, Validation Loss: 0.3049\n",
      "Precision: 0.8125, Recall: 0.7123, F1 Score: 0.7591\n",
      "Epoch 9/10\n",
      "Training Loss: 0.0674\n",
      "Evaluation Loss: 0.4039\n",
      "Precision: 0.7945\n",
      "Recall: 0.7945\n",
      "F1 Score: 0.7945\n",
      "Training Loss: 0.0674, Validation Loss: 0.4039\n",
      "Precision: 0.7945, Recall: 0.7945, F1 Score: 0.7945\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0557\n",
      "Evaluation Loss: 0.4625\n",
      "Precision: 0.8793\n",
      "Recall: 0.6986\n",
      "F1 Score: 0.7786\n",
      "Training Loss: 0.0557, Validation Loss: 0.4625\n",
      "Precision: 0.8793, Recall: 0.6986, F1 Score: 0.7786\n",
      "Results for Fold 3: Validation Loss = 0.4625, Precision = 0.8793, Recall = 0.6986, F1 Score = 0.7786\n",
      "--------------------------------------------------\n",
      "------ Fold 5/5 ------\n",
      "Epoch 1/10\n",
      "Training Loss: 0.4959\n",
      "Evaluation Loss: 0.3801\n",
      "Precision: 0.8529\n",
      "Recall: 0.4028\n",
      "F1 Score: 0.5472\n",
      "Training Loss: 0.4959, Validation Loss: 0.3801\n",
      "Precision: 0.8529, Recall: 0.4028, F1 Score: 0.5472\n",
      "Epoch 2/10\n",
      "Training Loss: 0.3085\n",
      "Evaluation Loss: 0.3777\n",
      "Precision: 0.9286\n",
      "Recall: 0.5417\n",
      "F1 Score: 0.6842\n",
      "Training Loss: 0.3085, Validation Loss: 0.3777\n",
      "Precision: 0.9286, Recall: 0.5417, F1 Score: 0.6842\n",
      "Epoch 3/10\n",
      "Training Loss: 0.2231\n",
      "Evaluation Loss: 0.3284\n",
      "Precision: 0.8727\n",
      "Recall: 0.6667\n",
      "F1 Score: 0.7559\n",
      "Training Loss: 0.2231, Validation Loss: 0.3284\n",
      "Precision: 0.8727, Recall: 0.6667, F1 Score: 0.7559\n",
      "Epoch 4/10\n",
      "Training Loss: 0.1871\n",
      "Evaluation Loss: 0.8029\n",
      "Precision: 1.0000\n",
      "Recall: 0.2083\n",
      "F1 Score: 0.3448\n",
      "Training Loss: 0.1871, Validation Loss: 0.8029\n",
      "Precision: 1.0000, Recall: 0.2083, F1 Score: 0.3448\n",
      "Epoch 5/10\n",
      "Training Loss: 0.2189\n",
      "Evaluation Loss: 0.3843\n",
      "Precision: 0.7237\n",
      "Recall: 0.7639\n",
      "F1 Score: 0.7432\n",
      "Training Loss: 0.2189, Validation Loss: 0.3843\n",
      "Precision: 0.7237, Recall: 0.7639, F1 Score: 0.7432\n",
      "Epoch 6/10\n",
      "Training Loss: 0.1764\n",
      "Evaluation Loss: 0.3007\n",
      "Precision: 0.8095\n",
      "Recall: 0.7083\n",
      "F1 Score: 0.7556\n",
      "Training Loss: 0.1764, Validation Loss: 0.3007\n",
      "Precision: 0.8095, Recall: 0.7083, F1 Score: 0.7556\n",
      "Epoch 7/10\n",
      "Training Loss: 0.1840\n",
      "Evaluation Loss: 0.3397\n",
      "Precision: 0.7742\n",
      "Recall: 0.6667\n",
      "F1 Score: 0.7164\n",
      "Training Loss: 0.1840, Validation Loss: 0.3397\n",
      "Precision: 0.7742, Recall: 0.6667, F1 Score: 0.7164\n",
      "Epoch 8/10\n",
      "Training Loss: 0.1100\n",
      "Evaluation Loss: 0.4153\n",
      "Precision: 0.7941\n",
      "Recall: 0.7500\n",
      "F1 Score: 0.7714\n",
      "Training Loss: 0.1100, Validation Loss: 0.4153\n",
      "Precision: 0.7941, Recall: 0.7500, F1 Score: 0.7714\n",
      "Epoch 9/10\n",
      "Training Loss: 0.0748\n",
      "Evaluation Loss: 0.5211\n",
      "Precision: 0.6628\n",
      "Recall: 0.7917\n",
      "F1 Score: 0.7215\n",
      "Training Loss: 0.0748, Validation Loss: 0.5211\n",
      "Precision: 0.6628, Recall: 0.7917, F1 Score: 0.7215\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0799\n",
      "Evaluation Loss: 0.4040\n",
      "Precision: 0.8033\n",
      "Recall: 0.6806\n",
      "F1 Score: 0.7368\n",
      "Training Loss: 0.0799, Validation Loss: 0.4040\n",
      "Precision: 0.8033, Recall: 0.6806, F1 Score: 0.7368\n",
      "Results for Fold 4: Validation Loss = 0.4040, Precision = 0.8033, Recall = 0.6806, F1 Score = 0.7368\n",
      "--------------------------------------------------\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Loss: 0.4267\n",
      "Average Precision: 0.7947\n",
      "Average Recall: 0.7010\n",
      "Average F1 Score: 0.7421\n",
      "F1 scores per fold saved to results/gmu-Arevalo-F1_scores.npy\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Define the dimensions\n",
    "text_dim = 1024  \n",
    "audio_dim = 768 \n",
    "video_dim = 768  \n",
    "output_dim = 512 \n",
    "\n",
    "# Cross-validation\n",
    "average_cv_loss = cross_validate_model(\n",
    "    text_dim=text_dim,\n",
    "    audio_dim=audio_dim,\n",
    "    video_dim=video_dim,\n",
    "    output_dim=output_dim,\n",
    "    dataset=full_dataset,  \n",
    "    num_folds=5,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn,\n",
    "    learning_rate=1e-3,\n",
    "    output_file=\"results/gmu-Arevalo-F1_scores.npy\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
