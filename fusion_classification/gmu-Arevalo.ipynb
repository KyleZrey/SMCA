{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible configurations for audio feature directories\n",
    "audio_feature_paths = {\n",
    "    'logmel': '../misc/audio_features/logmel',\n",
    "    'mfcc': '../misc/audio_features/mfcc'\n",
    "}\n",
    "\n",
    "# Function to get the audio feature path based on the selected configuration\n",
    "def get_audio_feature_path(config_name):\n",
    "    if config_name in audio_feature_paths:\n",
    "        return audio_feature_paths[config_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Configuration '{config_name}' not found. Available options: logmel, mfcc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "### Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "### Audio Feature selection: 'logmel' or 'mfcc'\n",
    "selected_config = 'logmel'\n",
    "audio_features_dir = get_audio_feature_path(selected_config)\n",
    "\n",
    "#### Data Configuration: 8 16 32 64 128\n",
    "train_batch_size = 32   # Set the batch size for training data\n",
    "val_batch_size = 16     # Set the batch size for validation data\n",
    "test_batch_size= 16     # Set the batch size for testing data\n",
    "\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "threshold = 0.5              # for predictions\n",
    "learning_rate = 1e-5         # For optimizer\n",
    "cl_dropout_rate = 0.4        # for FinalClassifier\n",
    "att_dropout_rate = 0.3       # for MutualCrossAttention\n",
    "num_epochs = 5              # for model training\n",
    "\n",
    "### Classifier Configuration\n",
    "isBCELoss = True                          # !!! SET ACCORDINGLY !!!\n",
    "criterion = BCELoss()\n",
    "# criterion = BCEWithLogits()\n",
    "# criterion = CustomLoss(pos_weight=2.94)\n",
    "# criterion = FocalLoss(alpha=0.25, gamma=2, pos_weight=0.34)\n",
    "\n",
    "# !!! Choose Classifier !!! False = Dense Layer, True = Final Classifier\n",
    "isFinalClassifier = False\n",
    "\n",
    "# ### For cross validation\n",
    "# num_folds = 5           # Set the number of folds for cross-validation\n",
    "# batch_size = 32         # Set the batch size for cross-validation\n",
    "\n",
    "# Define the dimensions for GMU\n",
    "text_dim = 1024  \n",
    "audio_dim = 768 \n",
    "video_dim = 768  \n",
    "output_dim = 512 \n",
    "\n",
    "## For cross validation\n",
    "num_folds = 5          # Set the number of folds for cross-validation\n",
    "num_epochs_cv = 10     # Set the number of epochs for cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_indices = []\n",
    "        missing_files = []\n",
    "\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Filter id_label_df to only include valid rows\n",
    "        self.id_label_df = self.id_label_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.missing_files = missing_files\n",
    "        \n",
    "        # Update valid_indices to reflect the new indices after resetting\n",
    "        valid_indices = list(range(len(self.id_label_df)))\n",
    "\n",
    "        # Return valid indices\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return imdbid, text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unpack batch elements\n",
    "    imdbids, text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return imdbids, text_data, audio_data, video_data_padded, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_video_features(video_features, lower_bound=35, upper_bound=197):\n",
    "    # Assuming video_features is a list of tuples where the second element is the numpy array\n",
    "    filtered_video_features = [v for v in video_features if lower_bound <= v[1].shape[0] <= upper_bound]\n",
    "    return filtered_video_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1325\n",
      "train_df shape: (927, 2)\n",
      "val_df shape: (199, 2)\n",
      "test_df shape: (199, 2)\n",
      "Train label distribution: Label\n",
      "green    693\n",
      "red      234\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "Test label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Train DataLoader: Total Samples = 927, Number of Batches = 29\n",
      "Validation DataLoader: Total Samples = 199, Number of Batches = 13\n",
      "Test DataLoader: Total Samples = 199, Number of Batches = 13\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = '../misc/text_features'\n",
    "audio_features_dir = audio_features_dir\n",
    "video_features_dir = '../misc/video_features'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "video_features = filter_video_features(video_features)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "id_label_df = id_label_df.drop(columns=['Movie Title', 'URL'])\n",
    "\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n",
    "# perform train-test split on the filtered DataFrame\n",
    "train_df, val_test_df = train_test_split(\n",
    "    full_dataset.id_label_df, test_size=0.3, random_state=42, stratify=full_dataset.id_label_df['Label'])\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(\n",
    "    val_test_df, test_size=0.5, random_state=42, stratify=val_test_df['Label'])\n",
    "\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:\", val_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)\n",
    "\n",
    "print(\"Train label distribution:\", train_df['Label'].value_counts())\n",
    "print(\"Validation label distribution:\", val_df['Label'].value_counts())\n",
    "print(\"Test label distribution:\", test_df['Label'].value_counts())\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# create datasets based on these splits\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Function to calculate and print the size of each DataLoader\n",
    "def print_dataloader_sizes(dataloader, name):\n",
    "    total_samples = len(dataloader.dataset)  # Get the size of the dataset\n",
    "    num_batches = len(dataloader)  # Get the number of batches\n",
    "    print(f\"{name} DataLoader: Total Samples = {total_samples}, Number of Batches = {num_batches}\")\n",
    "\n",
    "# Print sizes of each DataLoader\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMU Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Gated Multimodal Unit of Arevalo et al. (2017)\n",
    "class GatedMultimodalUnit(torch.nn.Module):\n",
    "    def __init__(self, text_dim, audio_dim, video_dim, output_dim):\n",
    "        super(GatedMultimodalUnit, self).__init__()\n",
    "        \n",
    "        # Linear transformation for text\n",
    "        self.text_linear = LinearTransformations(text_dim, output_dim)\n",
    "        \n",
    "        # Convolutional layers for audio and video features\n",
    "        self.audio_conv = nn.Conv1d(audio_dim, output_dim, kernel_size=1)\n",
    "        self.video_conv = nn.Conv1d(video_dim, output_dim, kernel_size=1)\n",
    "        \n",
    "        self.output_dim = output_dim \n",
    "        \n",
    "        # Activation functions\n",
    "        self.activation = nn.Tanh()\n",
    "        self.gate_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Weight matrices for each modality\n",
    "        self.W1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Gating matrices\n",
    "        self.Y1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        init.xavier_uniform_(self.W1)\n",
    "        init.xavier_uniform_(self.W2)\n",
    "        init.xavier_uniform_(self.W3)\n",
    "        \n",
    "        # Initialize gating matrices\n",
    "        init.xavier_uniform_(self.Y1)\n",
    "        init.xavier_uniform_(self.Y2)\n",
    "        init.xavier_uniform_(self.Y3)\n",
    "        \n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "\n",
    "        # Process text features to match shape\n",
    "        x_t = self.text_linear(text_features)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process audio features to match shape\n",
    "        audio_features = audio_features.squeeze(1).permute(0, 2, 1)               # Shape: [batch_size, audio_dim, sequence_length] \n",
    "        x_a = self.audio_conv(audio_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process video features to match shape\n",
    "        video_features = video_features.permute(0, 2, 1)   # Shape: [batch_size, video_dim, sequence_length]\n",
    "        x_v = self.video_conv(video_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    " \n",
    "        h1 = self.activation(torch.matmul(x_t, self.W1))        # Shape: [batch_size, output_dim]\n",
    "        h2 = self.activation(torch.matmul(x_a, self.W2))        # Shape: [batch_size, output_dim]\n",
    "        h3 = self.activation(torch.matmul(x_v, self.W3))        # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Compute modality-specific gating weights\n",
    "        z1 = self.gate_activation(torch.matmul(x_t, self.Y1))  # Shape: [batch_size, output_dim]\n",
    "        z2 = self.gate_activation(torch.matmul(x_a, self.Y2))  # Shape: [batch_size, output_dim]\n",
    "        z3 = self.gate_activation(torch.matmul(x_v, self.Y3))  # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Calculate final output\n",
    "        h = z1 * h1 + z2 * h2 + z3 * h3         \n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=0.5):\n",
    "        super(FinalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)  # From 2304 to 2048\n",
    "        self.fc2 = nn.Linear(1024, 512)        # From 2048 to 1024\n",
    "        # self.fc3 = nn.Linear(1024, 512)         # Optional 512 or 768\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(512, 1)          # Final output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.fc3(x)\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        if isBCELoss:\n",
    "            x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=learning_rate):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    dense_layer, \n",
    "    dataloader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device,\n",
    "    output_dir='results/gmu/', \n",
    "    output_filename='train_predictions.csv',\n",
    "    output_dim=768\n",
    "):\n",
    "    model.train()\n",
    "    dense_layer.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "    \n",
    "    # Reset metrics at the start of training\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "\n",
    "    for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device)\n",
    "        )\n",
    "        \n",
    "        # Pass inputs through GMU model\n",
    "        outputs = model(text_features, audio_features, video_features)\n",
    "            \n",
    "        # Pass the GMU outputs through the dense layer to get final predictions\n",
    "        predictions = dense_layer(outputs)  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not isBCELoss:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        preds = (predictions >= threshold).float()\n",
    "        \n",
    "        print(\"Target Shape\", targets.shape)\n",
    "        print(\"Preds Shape\", preds.shape)\n",
    "\n",
    "        # targets = targets.squeeze()\n",
    "        # preds = preds.squeeze()\n",
    "\n",
    "        # Collect results for each sample\n",
    "        for i in range(len(imdbids)):\n",
    "            results.append({\n",
    "                'IMDBid': imdbids[i],\n",
    "                'Raw Prediction': predictions[i].item(),\n",
    "                'Binary Prediction': preds[i].item(),\n",
    "                'Target': targets[i].item()\n",
    "            })\n",
    "        \n",
    "        # Update metrics for binary classification\n",
    "        precision_metric.update(preds.long(), targets.long())\n",
    "        recall_metric.update(preds.long(), targets.long())\n",
    "        f1_metric.update(preds.long(), targets.long())\n",
    "        accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "    # Compute average precision, recall, F1 score, and accuracy\n",
    "    train_precision = precision_metric.compute().item()\n",
    "    train_recall = recall_metric.compute().item()\n",
    "    train_f1_score = f1_metric.compute().item()\n",
    "    train_accuracy = accuracy_metric.compute().item()  # Compute accuracy\n",
    "    \n",
    "    train_average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
    "    print(f\"Train Loss: {train_average_loss:.4f}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "        \n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Training Loss: {average_loss:.4f}\")\n",
    "    return train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device, output_dir='results/', \n",
    "    output_filename='val_predictions.csv',):\n",
    "\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "    \n",
    "    # Reset metrics at the start of training\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "    \n",
    "    print(\"-\" * 20, \"Eval\", \"-\" * 20)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).squeeze()\n",
    "            )\n",
    "\n",
    "            # Pass inputs through GMU model\n",
    "            outputs = model(text_features, audio_features, video_features)\n",
    "            \n",
    "            # Pass the GMU outputs through the dense layer to get final predictions\n",
    "            predictions = dense_layer(outputs) # Shape: [batch_size, 1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets.view(-1,1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if not isBCELoss:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions >= threshold).float()\n",
    "\n",
    "            preds = preds.squeeze()\n",
    "            targets = targets.squeeze()\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Collect results for each sample\n",
    "            for i in range(len(imdbids)):\n",
    "                results.append({\n",
    "                    'IMDBid': imdbids[i],\n",
    "                    'Raw Prediction': predictions[i].item(),\n",
    "                    'Binary Prediction': preds[i].item(),\n",
    "                    'Target': targets[i].item()\n",
    "                })\n",
    "            \n",
    "            # Update metrics for binary classification\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "            accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "    # Compute average precision, recall, F1 score, and accuracy\n",
    "    val_precision = precision_metric.compute().item()\n",
    "    val_recall = recall_metric.compute().item()\n",
    "    val_f1_score = f1_metric.compute().item()\n",
    "    val_accuracy = accuracy_metric.compute().item() \n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    val_conf_matrix = confusion_matrix(all_targets, np.round(all_predictions))\n",
    "\n",
    "    val_average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Eval Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Eval Precision: {val_precision:.4f}\")\n",
    "    print(f\"Eval Recall: {val_recall:.4f}\")\n",
    "    print(f\"Eval F1 Score: {val_f1_score:.4f}\")\n",
    "    print(f\"Eval Loss: {val_average_loss:.4f}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "    \n",
    "    return val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names=['Negative', 'Positive']):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and validation loss\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange', marker='x')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "----------------------------------------\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x512 and 768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Ensure you have a dataloader that yields inputs and targets\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_average_loss)  \u001b[38;5;66;03m# Store training loss\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dense_layer, dataloader, criterion, optimizer, device, output_dir, output_filename, output_dim)\u001b[0m\n\u001b[0;32m     42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(text_features, audio_features, video_features)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Pass the GMU outputs through the dense layer to get final predictions\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdense_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, 1]\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edjin\\OneDrive\\Documents\\Programming Files\\Thesis\\SMCA\\fusion_classification\\..\\modules\\classifier.py:13\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Pass the input through the dense layer\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Apply the sigmoid activation function !!!DONT USE WITH BCEWithLogits!!!\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x512 and 768x1)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Set device configuration\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = GatedMultimodalUnit(text_dim=text_dim, audio_dim=audio_dim, video_dim=video_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Own DenseLayer or FinalClassifier\n",
    "    if isFinalClassifier:\n",
    "        dense_layer = FinalClassifier(output_dim).to(device) \n",
    "    else:\n",
    "        dense_layer = DenseLayer(output_dim).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(list(dense_layer.parameters()), learning_rate)\n",
    "    \n",
    "    # Lists to store the training and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device, output_dim=output_dim)\n",
    "        train_losses.append(train_average_loss)  # Store training loss\n",
    "\n",
    "        print(\"-\" * 20, \"Train\", \"-\" * 20)\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Train Precision: {train_precision:.4f}\")\n",
    "        print(f\"Train Recall: {train_recall:.4f}\")\n",
    "        print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
    "        print(f\"Train Loss: {train_average_loss:.4f}\")\n",
    "    \n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is None:\n",
    "                print(\"After train: model:\", \"No gradient for:\", name)\n",
    "        \n",
    "        for name, param in dense_layer.named_parameters():\n",
    "            if param.grad is None:\n",
    "                print(\"After train: classifier:\", \"No gradient for:\", name)\n",
    "\n",
    "        # Validate step\n",
    "        val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device, output_dim=output_dim)\n",
    "        val_losses.append(val_average_loss)  # Store validation loss\n",
    "        \n",
    "        print(\"-\" * 20, \"Eval\", \"-\" * 20)\n",
    "        print(f\"Eval Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Eval Precision: {val_precision:.4f}\")\n",
    "        print(f\"Eval Recall: {val_recall:.4f}\")\n",
    "        print(f\"Eval F1 Score: {val_f1_score:.4f}\")\n",
    "        print(f\"Eval Loss: {val_average_loss:.4f}\")\n",
    "        \n",
    "        # print(f\"Training Loss: {train_average_loss:.4f}, Validation Loss: {eval_average_loss:.4f}\")\n",
    "\n",
    "    # Testing the model\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_average_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_conf_matrix = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device, output_dim=output_dim)\n",
    "    \n",
    "    print(\"-\" * 20, \"Test\", \"-\" * 20)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1_score:.4f}\")\n",
    "    print(f\"Test Loss: {test_average_loss:.4f}\")\n",
    "    \n",
    "    # Summary of metrics\n",
    "    metrics_summary = {\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Validation Accuracy\": [val_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Train Precision\": [train_precision],\n",
    "        \"Validation Precision\": [val_precision],\n",
    "        \"Test Precision\": [test_precision],\n",
    "        \"Train Recall\": [train_recall],\n",
    "        \"Validation Recall\": [val_recall],\n",
    "        \"Test Recall\": [test_recall],\n",
    "        \"Train F1 Score\": [train_f1_score],\n",
    "        \"Validation F1 Score\": [val_f1_score],\n",
    "        \"Test F1 Score\": [test_f1_score],\n",
    "        \"Train Loss\": [train_average_loss],\n",
    "        \"Validation Loss\": [val_average_loss],\n",
    "        \"Test Loss\": [test_average_loss]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "#     # Initialize lists to collect metrics across all folds\n",
    "#     total_train_losses = []\n",
    "#     total_val_losses = []\n",
    "#     total_train_accuracies = []\n",
    "#     total_val_accuracies = []\n",
    "#     total_train_precisions = []\n",
    "#     total_val_precisions = []\n",
    "#     total_train_recalls = []\n",
    "#     total_val_recalls = []\n",
    "#     total_train_f1_scores = []\n",
    "#     total_val_f1_scores = []\n",
    "\n",
    "#     for fold, (train_index, val_index) in enumerate(kf.split(full_dataset), 1):\n",
    "#         print(\"-\" * 50)\n",
    "#         print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "#         # Create subsets for training and validation\n",
    "#         train_subset = Subset(full_dataset, train_index)\n",
    "#         val_subset = Subset(full_dataset, val_index)\n",
    "        \n",
    "#         # DataLoaders with batch size 8 and collate function\n",
    "#         train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "#         val_loader = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "        \n",
    "#         # Initialize the model and dense layer for the current fold\n",
    "#         model = GatedMultimodalUnit(text_dim=text_dim, audio_dim=audio_dim, video_dim=video_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "#         if isFinalClassifier:\n",
    "#             dense_layer = FinalClassifier(input_size=output_dim).to(device) \n",
    "#         else:\n",
    "#             dense_layer = DenseLayer(output_dim).to(device)\n",
    "        \n",
    "#         # Combine parameters of GMU model and DenseLayer for the optimizer\n",
    "#         optimizer = get_optimizer(list(model.parameters()) + list(dense_layer.parameters()))\n",
    "        \n",
    "#         print(f\"Training model for fold {fold}\")\n",
    "#         train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score = train_model(\n",
    "#             model=model,\n",
    "#             dense_layer=dense_layer,\n",
    "#             dataloader=train_loader,\n",
    "#             criterion=criterion,\n",
    "#             optimizer=optimizer,\n",
    "#             device=device\n",
    "#         )\n",
    "        \n",
    "#         # Store training metrics\n",
    "#         total_train_losses.append(train_average_loss)\n",
    "\n",
    "\n",
    "#         print(f\"Evaluating model for fold {fold}\")\n",
    "#         eval_average_loss, eval_accuracy, eval_precision, eval_recall, eval_f1_score, eval_conf_matrix = evaluate_model(\n",
    "#             model=model,\n",
    "#             dense_layer=dense_layer,\n",
    "#             dataloader=val_loader,\n",
    "#             criterion=criterion,\n",
    "#             device=device\n",
    "#         )\n",
    "        \n",
    "#         # Store validation metrics\n",
    "#         total_val_losses.append(eval_average_loss)\n",
    "\n",
    "\n",
    "#     # Compute averages for cross-validation results\n",
    "#     average_cv_loss = sum(total_val_losses) / n_splits\n",
    "#     average_cv_precision = sum(total_val_precisions) / n_splits\n",
    "#     average_cv_recall = sum(total_val_recalls) / n_splits\n",
    "#     average_cv_f1 = sum(total_val_f1_scores) / n_splits\n",
    "#     average_cv_accuracy = sum(total_val_accuracies) / n_splits\n",
    "\n",
    "#     print(f\"Average Cross-Validation Loss: {average_cv_loss:.4f}\")\n",
    "#     print(f\"Average Cross-Validation Precision: {average_cv_precision:.4f}\")\n",
    "#     print(f\"Average Cross-Validation Recall: {average_cv_recall:.4f}\")\n",
    "#     print(f\"Average Cross-Validation F1 Score: {average_cv_f1:.4f}\")\n",
    "#     print(f\"Average Cross-Validation Accuracy: {average_cv_accuracy:.4f}\")\n",
    "\n",
    "#     # Summary of metrics\n",
    "#     metrics_summary = {\n",
    "#         \"Train Accuracy\": total_train_accuracies,\n",
    "#         \"Validation Accuracy\": total_val_accuracies,\n",
    "#         \"Train Precision\": total_train_precisions,\n",
    "#         \"Validation Precision\": total_val_precisions,\n",
    "#         \"Train Recall\": total_train_recalls,\n",
    "#         \"Validation Recall\": total_val_recalls,\n",
    "#         \"Train F1 Score\": total_train_f1_scores,\n",
    "#         \"Validation F1 Score\": total_val_f1_scores,\n",
    "#         \"Train Loss\": total_train_losses,\n",
    "#         \"Validation Loss\": total_val_losses,\n",
    "#     }\n",
    "\n",
    "#     # Create DataFrame and save to CSV\n",
    "#     metrics_df = pd.DataFrame(metrics_summary)\n",
    "#     metrics_df.to_csv(\"cross_validation_metrics_summary.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
