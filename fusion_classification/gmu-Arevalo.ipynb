{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryAccuracy\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible configurations for audio feature directories\n",
    "audio_feature_paths = {\n",
    "    'logmel': 'D:\\Projects\\Thesis\\Audio',\n",
    "    'mfcc': 'D:\\Projects\\Thesis\\Audio 2\\mfcc_extracted'\n",
    "}\n",
    "\n",
    "# Function to get the audio feature path based on the selected configuration\n",
    "def get_audio_feature_path(config_name):\n",
    "    if config_name in audio_feature_paths:\n",
    "        return audio_feature_paths[config_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Configuration '{config_name}' not found. Available options: logmel, mfcc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "### Device configuration\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "### Audio Feature selection: 'logmel' or 'mfcc'\n",
    "selected_config = 'logmel'\n",
    "audio_features_dir = get_audio_feature_path(selected_config)\n",
    "\n",
    "#### Data Configuration: 8 16 32 64 128\n",
    "train_batch_size = 32   # Set the batch size for training data\n",
    "val_batch_size = 16     # Set the batch size for validation data\n",
    "test_batch_size= 16     # Set the batch size for testing data\n",
    "\n",
    "#FIXED CONSTANT\n",
    "max_pad = 197\n",
    "\n",
    "### Hyperparameters\n",
    "threshold = 0.5              # for predictions\n",
    "learning_rate = 1e-5         # For optimizer\n",
    "cl_dropout_rate = 0.4        # for FinalClassifier\n",
    "att_dropout_rate = 0.3       # for MutualCrossAttention\n",
    "num_epochs = 5              # for model training\n",
    "\n",
    "### Classifier Configuration\n",
    "isBCELoss = True                          # !!! SET ACCORDINGLY !!!\n",
    "criterion = BCELoss()\n",
    "# criterion = BCEWithLogits()\n",
    "# criterion = CustomLoss(pos_weight=2.94)\n",
    "# criterion = FocalLoss(alpha=0.25, gamma=2, pos_weight=0.34)\n",
    "\n",
    "# !!! Choose Classifier !!! False = Dense Layer, True = Final Classifier\n",
    "isFinalClassifier = False\n",
    "\n",
    "# ### For cross validation\n",
    "# num_folds = 5           # Set the number of folds for cross-validation\n",
    "# batch_size = 32         # Set the batch size for cross-validation\n",
    "\n",
    "# Define the dimensions for GMU\n",
    "text_dim = 1024  \n",
    "audio_dim = 768 \n",
    "video_dim = 768  \n",
    "output_dim = 512 \n",
    "\n",
    "# Other GMU Config\n",
    "n_splits = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=0.5):\n",
    "        super(FinalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)  # From 2304 to 2048\n",
    "        self.fc2 = nn.Linear(1024, 512)        # From 2048 to 1024\n",
    "        # self.fc3 = nn.Linear(1024, 512)         # Optional 512 or 768\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(512, 1)          # Final output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.fc3(x)\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        if isBCELoss:\n",
    "            x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_indices = []\n",
    "        missing_files = []\n",
    "\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Filter id_label_df to only include valid rows\n",
    "        self.id_label_df = self.id_label_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.missing_files = missing_files\n",
    "        \n",
    "        # Update valid_indices to reflect the new indices after resetting\n",
    "        valid_indices = list(range(len(self.id_label_df)))\n",
    "\n",
    "        # Return valid indices\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return imdbid, text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imdbids, text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Pad video data\n",
    "    max_length = max(v.size(0) for v in video_data)\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    label_data = torch.stack(label_data)\n",
    "\n",
    "    return imdbids, text_data, audio_data, video_data_padded, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_video_features(video_features, lower_bound=35, upper_bound=197):\n",
    "    # Assuming video_features is a list of tuples where the second element is the numpy array\n",
    "    filtered_video_features = [v for v in video_features if lower_bound <= v[1].shape[0] <= upper_bound]\n",
    "    return filtered_video_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1325\n",
      "train_df shape: (927, 2)\n",
      "val_df shape: (199, 2)\n",
      "test_df shape: (199, 2)\n",
      "Train label distribution: Label\n",
      "green    693\n",
      "red      234\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "Test label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Train DataLoader: Total Samples = 927, Number of Batches = 29\n",
      "Validation DataLoader: Total Samples = 199, Number of Batches = 13\n",
      "Test DataLoader: Total Samples = 199, Number of Batches = 13\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\Projects\\Thesis\\Text'\n",
    "audio_features_dir = audio_features_dir\n",
    "video_features_dir = 'D:\\Projects\\Thesis\\Video'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "video_features = filter_video_features(video_features)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "id_label_df = id_label_df.drop(columns=['Movie Title', 'URL'])\n",
    "\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n",
    "# perform train-test split on the filtered DataFrame\n",
    "train_df, val_test_df = train_test_split(\n",
    "    full_dataset.id_label_df, test_size=0.3, random_state=42, stratify=full_dataset.id_label_df['Label'])\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(\n",
    "    val_test_df, test_size=0.5, random_state=42, stratify=val_test_df['Label'])\n",
    "\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:\", val_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)\n",
    "\n",
    "print(\"Train label distribution:\", train_df['Label'].value_counts())\n",
    "print(\"Validation label distribution:\", val_df['Label'].value_counts())\n",
    "print(\"Test label distribution:\", test_df['Label'].value_counts())\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# create datasets based on these splits\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Function to calculate and print the size of each DataLoader\n",
    "def print_dataloader_sizes(dataloader, name):\n",
    "    total_samples = len(dataloader.dataset)  # Get the size of the dataset\n",
    "    num_batches = len(dataloader)  # Get the number of batches\n",
    "    print(f\"{name} DataLoader: Total Samples = {total_samples}, Number of Batches = {num_batches}\")\n",
    "\n",
    "# Print sizes of each DataLoader\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMU Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Gated Multimodal Unit of Arevalo et al. (2017)\n",
    "class GatedMultimodalUnit(torch.nn.Module):\n",
    "    def __init__(self, text_dim, audio_dim, video_dim, output_dim):\n",
    "        super(GatedMultimodalUnit, self).__init__()\n",
    "        \n",
    "        # Linear transformation for text\n",
    "        self.text_linear = LinearTransformations(text_dim, output_dim)\n",
    "        \n",
    "        # Convolutional layers for audio and video features\n",
    "        self.audio_conv = nn.Conv1d(audio_dim, output_dim, kernel_size=1)\n",
    "        self.video_conv = nn.Conv1d(video_dim, output_dim, kernel_size=1)\n",
    "        \n",
    "        self.output_dim = output_dim \n",
    "        \n",
    "        # Activation functions\n",
    "        self.activation = nn.Tanh()\n",
    "        self.gate_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Weight matrices for each modality\n",
    "        self.W1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Gating matrices\n",
    "        self.Y1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        init.xavier_uniform_(self.W1)\n",
    "        init.xavier_uniform_(self.W2)\n",
    "        init.xavier_uniform_(self.W3)\n",
    "        \n",
    "        # Initialize gating matrices\n",
    "        init.xavier_uniform_(self.Y1)\n",
    "        init.xavier_uniform_(self.Y2)\n",
    "        init.xavier_uniform_(self.Y3)\n",
    "        \n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "\n",
    "        # Process text features to match shape\n",
    "        x_t = self.text_linear(text_features)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process audio features to match shape\n",
    "        audio_features = audio_features.squeeze(1).permute(0, 2, 1)               # Shape: [batch_size, audio_dim, sequence_length] \n",
    "        x_a = self.audio_conv(audio_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process video features to match shape\n",
    "        video_features = video_features.permute(0, 2, 1)   # Shape: [batch_size, video_dim, sequence_length]\n",
    "        x_v = self.video_conv(video_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    " \n",
    "        h1 = self.activation(torch.matmul(x_t, self.W1))        # Shape: [batch_size, output_dim]\n",
    "        h2 = self.activation(torch.matmul(x_a, self.W2))        # Shape: [batch_size, output_dim]\n",
    "        h3 = self.activation(torch.matmul(x_v, self.W3))        # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Compute modality-specific gating weights\n",
    "        z1 = self.gate_activation(torch.matmul(x_t, self.Y1))  # Shape: [batch_size, output_dim]\n",
    "        z2 = self.gate_activation(torch.matmul(x_a, self.Y2))  # Shape: [batch_size, output_dim]\n",
    "        z3 = self.gate_activation(torch.matmul(x_v, self.Y3))  # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Calculate final output\n",
    "        h = z1 * h1 + z2 * h2 + z3 * h3         \n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=learning_rate):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dense_layer, dataloader, criterion, optimizer, device, output_dir='results/', output_filename='train_predictions.csv',\n",
    " ):\n",
    "    model.train()\n",
    "    dense_layer.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "    \n",
    "    # Reset metrics at the start of training\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "\n",
    "    print(\"-\" * 20, \"Train\", \"-\" * 20)\n",
    "    \n",
    "    for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass inputs through GMU model\n",
    "        outputs = model(text_features, audio_features, video_features)\n",
    "        \n",
    "        # Pass the GMU outputs through the dense layer to get final predictions\n",
    "        predictions = dense_layer(outputs)  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not isBCELoss:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        preds = (predictions >= threshold).float()\n",
    "\n",
    "        # Collect results for each sample\n",
    "        for i in range(len(imdbids)):\n",
    "            results.append({\n",
    "                'IMDBid': imdbids[i],\n",
    "                'Raw Prediction': predictions[i].item(),\n",
    "                'Binary Prediction': preds[i].item(),\n",
    "                'Target': targets[i].item()\n",
    "            })\n",
    "        \n",
    "        # Update metrics for binary classification\n",
    "        precision_metric.update(preds.long(), targets.long())\n",
    "        recall_metric.update(preds.long(), targets.long())\n",
    "        f1_metric.update(preds.long(), targets.long())\n",
    "        accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "    # Compute average precision, recall, F1 score, and accuracy\n",
    "    train_precision = precision_metric.compute().item()\n",
    "    train_recall = recall_metric.compute().item()\n",
    "    train_f1_score = f1_metric.compute().item()\n",
    "    train_accuracy = accuracy_metric.compute().item()  # Compute accuracy\n",
    "    \n",
    "    train_average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
    "    print(f\"Train Loss: {train_average_loss:.4f}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "        \n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Training Loss: {average_loss:.4f}\")\n",
    "    return train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device, output_dir='results/', \n",
    "    output_filename='val_predictions.csv',):\n",
    "\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "    \n",
    "    # Reset metrics at the start of training\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "    \n",
    "    print(\"-\" * 20, \"Eval\", \"-\" * 20)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).squeeze()\n",
    "            )\n",
    "\n",
    "            # Pass inputs through GMU model\n",
    "            outputs = model(text_features, audio_features, video_features)\n",
    "            \n",
    "            # Pass the GMU outputs through the dense layer to get final predictions\n",
    "            predictions = dense_layer(outputs) # Shape: [batch_size, 1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets.view(-1,1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if not isBCELoss:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions >= threshold).float()\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Collect results for each sample\n",
    "            for i in range(len(imdbids)):\n",
    "                results.append({\n",
    "                    'IMDBid': imdbids[i],\n",
    "                    'Raw Prediction': predictions[i].item(),\n",
    "                    'Binary Prediction': preds[i].item(),\n",
    "                    'Target': targets[i].item()\n",
    "                })\n",
    "            \n",
    "            # Update metrics for binary classification\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "            accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "    # Compute average precision, recall, F1 score, and accuracy\n",
    "    val_precision = precision_metric.compute().item()\n",
    "    val_recall = recall_metric.compute().item()\n",
    "    val_f1_score = f1_metric.compute().item()\n",
    "    val_accuracy = accuracy_metric.compute().item() \n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    val_conf_matrix = confusion_matrix(all_targets, np.round(all_predictions))\n",
    "\n",
    "    val_average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Eval Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Eval Precision: {val_precision:.4f}\")\n",
    "    print(f\"Eval Recall: {val_recall:.4f}\")\n",
    "    print(f\"Eval F1 Score: {val_f1_score:.4f}\")\n",
    "    print(f\"Eval Loss: {val_average_loss:.4f}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "    \n",
    "    return val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold 1/50\n",
      "Training model for fold 1\n",
      "-------------------- Train --------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `input` and `target` should have the same dimensions, got shapes torch.Size([8, 1]) and torch.Size([8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m get_optimizer(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(dense_layer\u001b[38;5;241m.\u001b[39mparameters()))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     42\u001b[0m     dense_layer\u001b[38;5;241m=\u001b[39mdense_layer,\n\u001b[0;32m     43\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     44\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     45\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     46\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Store training metrics\u001b[39;00m\n\u001b[0;32m     50\u001b[0m total_train_losses\u001b[38;5;241m.\u001b[39mappend(train_average_loss)\n",
      "Cell \u001b[1;32mIn[133], line 67\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dense_layer, dataloader, criterion, optimizer, device, output_dir, output_filename)\u001b[0m\n\u001b[0;32m     59\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMDBid\u001b[39m\u001b[38;5;124m'\u001b[39m: imdbids[i],\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRaw Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions[i]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBinary Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m: preds[i]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m: targets[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     64\u001b[0m     })\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Update metrics for binary classification\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m precision_metric\u001b[38;5;241m.\u001b[39mupdate(preds\u001b[38;5;241m.\u001b[39mlong(), targets\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     68\u001b[0m recall_metric\u001b[38;5;241m.\u001b[39mupdate(preds\u001b[38;5;241m.\u001b[39mlong(), targets\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     69\u001b[0m f1_metric\u001b[38;5;241m.\u001b[39mupdate(preds\u001b[38;5;241m.\u001b[39mlong(), targets\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torcheval\\metrics\\classification\\precision.py:216\u001b[0m, in \u001b[0;36mBinaryPrecision.update\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    214\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 216\u001b[0m num_tp, num_fp, num_label \u001b[38;5;241m=\u001b[39m _binary_precision_update(\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_tp\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_fp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_fp\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torcheval\\metrics\\functional\\classification\\precision.py:227\u001b[0m, in \u001b[0;36m_binary_precision_update\u001b[1;34m(input, target, threshold)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_binary_precision_update\u001b[39m(\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    223\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    224\u001b[0m     threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    225\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 227\u001b[0m     _binary_precision_update_input_check(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m<\u001b[39m threshold, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    230\u001b[0m     num_tp \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m target)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torcheval\\metrics\\functional\\classification\\precision.py:243\u001b[0m, in \u001b[0;36m_binary_precision_update_input_check\u001b[1;34m(input, target)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_binary_precision_update_input_check\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    240\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m--> 243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `input` and `target` should have the same dimensions, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget should be a one-dimensional tensor, got shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The `input` and `target` should have the same dimensions, got shapes torch.Size([8, 1]) and torch.Size([8])."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    # Initialize lists to collect metrics across all folds\n",
    "    total_train_losses = []\n",
    "    total_val_losses = []\n",
    "    total_train_accuracies = []\n",
    "    total_val_accuracies = []\n",
    "    total_train_precisions = []\n",
    "    total_val_precisions = []\n",
    "    total_train_recalls = []\n",
    "    total_val_recalls = []\n",
    "    total_train_f1_scores = []\n",
    "    total_val_f1_scores = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(full_dataset), 1):\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "        # Create subsets for training and validation\n",
    "        train_subset = Subset(full_dataset, train_index)\n",
    "        val_subset = Subset(full_dataset, val_index)\n",
    "        \n",
    "        # DataLoaders with batch size 8 and collate function\n",
    "        train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "        \n",
    "        # Initialize the model and dense layer for the current fold\n",
    "        model = GatedMultimodalUnit(text_dim=text_dim, audio_dim=audio_dim, video_dim=video_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "        if isFinalClassifier:\n",
    "            dense_layer = FinalClassifier(input_size=output_dim).to(device) \n",
    "        else:\n",
    "            dense_layer = DenseLayer(output_dim).to(device)\n",
    "        \n",
    "        # Combine parameters of GMU model and DenseLayer for the optimizer\n",
    "        optimizer = get_optimizer(list(model.parameters()) + list(dense_layer.parameters()))\n",
    "        \n",
    "        print(f\"Training model for fold {fold}\")\n",
    "        train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score = train_model(\n",
    "            model=model,\n",
    "            dense_layer=dense_layer,\n",
    "            dataloader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Store training metrics\n",
    "        total_train_losses.append(train_average_loss)\n",
    "        total_train_accuracies.append(train_accuracy)\n",
    "        total_train_precisions.append(train_precision)\n",
    "        total_train_recalls.append(train_recall)\n",
    "        total_train_f1_scores.append(train_f1_score)\n",
    "\n",
    "        print(f\"Evaluating model for fold {fold}\")\n",
    "        eval_average_loss, eval_accuracy, eval_precision, eval_recall, eval_f1_score, eval_conf_matrix = evaluate_model(\n",
    "            model=model,\n",
    "            dense_layer=dense_layer,\n",
    "            dataloader=val_loader,\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Store validation metrics\n",
    "        total_val_losses.append(eval_average_loss)\n",
    "        total_val_accuracies.append(eval_accuracy)\n",
    "        total_val_precisions.append(eval_precision)\n",
    "        total_val_recalls.append(eval_recall)\n",
    "        total_val_f1_scores.append(eval_f1_score)\n",
    "\n",
    "    # Compute averages for cross-validation results\n",
    "    average_cv_loss = sum(total_val_losses) / n_splits\n",
    "    average_cv_precision = sum(total_val_precisions) / n_splits\n",
    "    average_cv_recall = sum(total_val_recalls) / n_splits\n",
    "    average_cv_f1 = sum(total_val_f1_scores) / n_splits\n",
    "    average_cv_accuracy = sum(total_val_accuracies) / n_splits\n",
    "\n",
    "    print(f\"Average Cross-Validation Loss: {average_cv_loss:.4f}\")\n",
    "    print(f\"Average Cross-Validation Precision: {average_cv_precision:.4f}\")\n",
    "    print(f\"Average Cross-Validation Recall: {average_cv_recall:.4f}\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {average_cv_f1:.4f}\")\n",
    "    print(f\"Average Cross-Validation Accuracy: {average_cv_accuracy:.4f}\")\n",
    "\n",
    "    # Summary of metrics\n",
    "    metrics_summary = {\n",
    "        \"Train Accuracy\": total_train_accuracies,\n",
    "        \"Validation Accuracy\": total_val_accuracies,\n",
    "        \"Train Precision\": total_train_precisions,\n",
    "        \"Validation Precision\": total_val_precisions,\n",
    "        \"Train Recall\": total_train_recalls,\n",
    "        \"Validation Recall\": total_val_recalls,\n",
    "        \"Train F1 Score\": total_train_f1_scores,\n",
    "        \"Validation F1 Score\": total_val_f1_scores,\n",
    "        \"Train Loss\": total_train_losses,\n",
    "        \"Validation Loss\": total_val_losses,\n",
    "    }\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    metrics_df = pd.DataFrame(metrics_summary)\n",
    "    metrics_df.to_csv(\"cross_validation_metrics_summary.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
