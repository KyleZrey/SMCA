{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Print missing files after checking all\n",
    "        if self.missing_files:\n",
    "            print(\"Missing files:\")\n",
    "            for item in self.missing_files:\n",
    "                print(f\"IMDBid: {item['IMDBid']}\")\n",
    "            print(f\"Total IMDB IDs with missing files: {len(self.missing_files)}\")\n",
    "        else:\n",
    "            print(\"No missing files.\")\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files:\n",
      "IMDBid: tt2494280\n",
      "IMDBid: tt1724962\n",
      "IMDBid: tt1152836\n",
      "IMDBid: tt0389790\n",
      "IMDBid: tt3053228\n",
      "IMDBid: tt1045778\n",
      "IMDBid: tt1758795\n",
      "IMDBid: tt0099385\n",
      "IMDBid: tt2917484\n",
      "IMDBid: tt4769836\n",
      "IMDBid: tt0089652\n",
      "IMDBid: tt0465494\n",
      "IMDBid: tt3675748\n",
      "IMDBid: tt2126362\n",
      "IMDBid: tt0988083\n",
      "IMDBid: tt2101341\n",
      "IMDBid: tt0401997\n",
      "IMDBid: tt1661461\n",
      "IMDBid: tt1313139\n",
      "IMDBid: tt1094661\n",
      "IMDBid: tt5162658\n",
      "IMDBid: tt0104839\n",
      "IMDBid: tt1288558\n",
      "IMDBid: tt5962210\n",
      "IMDBid: tt2937696\n",
      "IMDBid: tt0284363\n",
      "IMDBid: tt5580390\n",
      "IMDBid: tt2293750\n",
      "IMDBid: tt2980472\n",
      "IMDBid: tt0082186\n",
      "IMDBid: tt0924129\n",
      "IMDBid: tt0988595\n",
      "IMDBid: tt1349482\n",
      "IMDBid: tt4158096\n",
      "IMDBid: tt1403241\n",
      "IMDBid: tt2713642\n",
      "IMDBid: tt1682940\n",
      "IMDBid: tt10327354\n",
      "IMDBid: tt1087842\n",
      "IMDBid: tt1800302\n",
      "IMDBid: tt0113855\n",
      "IMDBid: tt2504022\n",
      "IMDBid: tt7248248\n",
      "IMDBid: tt1720164\n",
      "IMDBid: tt1336621\n",
      "IMDBid: tt0266987\n",
      "IMDBid: tt0859635\n",
      "Total IMDB IDs with missing files: 47\n",
      "Missing files:\n",
      "IMDBid: tt2437712\n",
      "IMDBid: tt0099371\n",
      "IMDBid: tt2935564\n",
      "IMDBid: tt0140336\n",
      "IMDBid: tt4687276\n",
      "IMDBid: tt0367085\n",
      "IMDBid: tt0220827\n",
      "IMDBid: tt0465602\n",
      "IMDBid: tt2350496\n",
      "IMDBid: tt1084950\n",
      "IMDBid: tt0110093\n",
      "IMDBid: tt1273235\n",
      "IMDBid: tt4503510\n",
      "IMDBid: tt1772925\n",
      "IMDBid: tt2180411\n",
      "IMDBid: tt0181865\n",
      "IMDBid: tt0200469\n",
      "IMDBid: tt0259288\n",
      "Total IMDB IDs with missing files: 18\n",
      "Missing files:\n",
      "IMDBid: tt0190590\n",
      "IMDBid: tt2383068\n",
      "IMDBid: tt1488555\n",
      "IMDBid: tt0758730\n",
      "IMDBid: tt0389557\n",
      "IMDBid: tt0498353\n",
      "IMDBid: tt5084170\n",
      "IMDBid: tt0405052\n",
      "IMDBid: tt1104733\n",
      "IMDBid: tt1924429\n",
      "IMDBid: tt5027774\n",
      "IMDBid: tt0990413\n",
      "IMDBid: tt5580036\n",
      "IMDBid: tt2524674\n",
      "Total IMDB IDs with missing files: 14\n",
      "Missing files:\n",
      "IMDBid: tt0988595\n",
      "IMDBid: tt1724962\n",
      "IMDBid: tt0758730\n",
      "IMDBid: tt0200469\n",
      "IMDBid: tt0389790\n",
      "IMDBid: tt4503510\n",
      "IMDBid: tt0401997\n",
      "IMDBid: tt1349482\n",
      "IMDBid: tt0082186\n",
      "IMDBid: tt1094661\n",
      "IMDBid: tt0924129\n",
      "IMDBid: tt2437712\n",
      "IMDBid: tt2917484\n",
      "IMDBid: tt3053228\n",
      "IMDBid: tt0099371\n",
      "IMDBid: tt2101341\n",
      "IMDBid: tt0099385\n",
      "IMDBid: tt0259288\n",
      "IMDBid: tt1336621\n",
      "IMDBid: tt1087842\n",
      "IMDBid: tt2937696\n",
      "IMDBid: tt1288558\n",
      "IMDBid: tt2524674\n",
      "IMDBid: tt4158096\n",
      "IMDBid: tt3675748\n",
      "IMDBid: tt1800302\n",
      "IMDBid: tt1104733\n",
      "IMDBid: tt0465494\n",
      "IMDBid: tt0498353\n",
      "IMDBid: tt0110093\n",
      "IMDBid: tt5580036\n",
      "IMDBid: tt5962210\n",
      "IMDBid: tt2180411\n",
      "IMDBid: tt0405052\n",
      "IMDBid: tt2713642\n",
      "IMDBid: tt1772925\n",
      "IMDBid: tt2980472\n",
      "IMDBid: tt0988083\n",
      "IMDBid: tt2935564\n",
      "IMDBid: tt0140336\n",
      "IMDBid: tt7248248\n",
      "IMDBid: tt0104839\n",
      "IMDBid: tt1720164\n",
      "IMDBid: tt0113855\n",
      "IMDBid: tt5084170\n",
      "IMDBid: tt0089652\n",
      "IMDBid: tt2504022\n",
      "IMDBid: tt0220827\n",
      "IMDBid: tt0190590\n",
      "IMDBid: tt0284363\n",
      "IMDBid: tt5162658\n",
      "IMDBid: tt1682940\n",
      "IMDBid: tt10327354\n",
      "IMDBid: tt1152836\n",
      "IMDBid: tt1084950\n",
      "IMDBid: tt4687276\n",
      "IMDBid: tt2293750\n",
      "IMDBid: tt0465602\n",
      "IMDBid: tt0367085\n",
      "IMDBid: tt0266987\n",
      "IMDBid: tt1273235\n",
      "IMDBid: tt2126362\n",
      "IMDBid: tt2494280\n",
      "IMDBid: tt0990413\n",
      "IMDBid: tt0859635\n",
      "IMDBid: tt1488555\n",
      "IMDBid: tt4769836\n",
      "IMDBid: tt2350496\n",
      "IMDBid: tt1313139\n",
      "IMDBid: tt2383068\n",
      "IMDBid: tt5580390\n",
      "IMDBid: tt1758795\n",
      "IMDBid: tt5027774\n",
      "IMDBid: tt0181865\n",
      "IMDBid: tt1924429\n",
      "IMDBid: tt1661461\n",
      "IMDBid: tt1403241\n",
      "IMDBid: tt1045778\n",
      "IMDBid: tt0389557\n",
      "Total IMDB IDs with missing files: 79\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataloader (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for text, audio, video, labels in train_dataloader:\n",
    "    # print(f\"Text Shape: {text.shape}\")\n",
    "    # print(f\"Audio Shape: {audio.shape}\")\n",
    "    # print(f\"Video Shape: {video.shape}\")\n",
    "    # print(f\"Labels Shape: {labels.shape}\")\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Features Shape: torch.Size([8, 1024])\n",
      "Audio Features Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Features Shape: torch.Size([8, 173, 768])\n",
      "Labels shape: torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "for text_features, audio_features, video_features, targets in train_dataloader:\n",
    "    print(\"Text Features Shape:\", text_features.shape)\n",
    "    print(\"Audio Features Shape:\", audio_features.shape)\n",
    "    print(\"Video Features Shape:\", video_features.shape)\n",
    "    print(\"Labels shape:\", targets.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Sample:\n",
      "Sample 1:\n",
      "------------------------------\n",
      "Text Data Shape: torch.Size([1024])\n",
      "Audio Data Shape: torch.Size([1, 197, 768])\n",
      "Video Data Shape: torch.Size([89, 768])\n",
      "Label: tensor([1.])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to print a sample from the dataset\n",
    "def print_sample(dataset, index):\n",
    "    text_data, audio_data, video_data, label_data = dataset[index]\n",
    "    print(f\"Sample {index}:\")\n",
    "    # print(\"Text Data:\", text_data)\n",
    "    # print(\"Audio Data:\", audio_data)\n",
    "    # print(\"Video Data:\", video_data)\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Text Data Shape:\", text_data.shape)\n",
    "    print(\"Audio Data Shape:\", audio_data.shape)\n",
    "    print(\"Video Data Shape:\", video_data.shape)\n",
    "    print(\"Label:\", label_data)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Print a sample from each dataset\n",
    "print(\"Training Dataset Sample:\")\n",
    "print_sample(train_dataset, 1)  # Change 5 to any index to view different samples\n",
    "\n",
    "# print(\"Validation Dataset Sample:\")\n",
    "# print_sample(val_dataset, 0)  # Change 5 to any index to view different samples\n",
    "\n",
    "# print(\"Test Dataset Sample:\")\n",
    "# print_sample(test_dataset, 0)  # Change 5 to any index to view different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataLoader Samples:\n",
      "Batch 0:\n",
      "Text Data Shape: torch.Size([8, 1024])\n",
      "Audio Data Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Data Shape: torch.Size([8, 145, 768])\n",
      "Labels: torch.Size([8, 1])\n",
      "------------------------------\n",
      "Batch 1:\n",
      "Text Data Shape: torch.Size([8, 1024])\n",
      "Audio Data Shape: torch.Size([8, 1, 197, 768])\n",
      "Video Data Shape: torch.Size([8, 162, 768])\n",
      "Labels: torch.Size([8, 1])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_dataloader_samples(dataloader, num_batches=1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        text_data, audio_data, video_data, labels_data = batch\n",
    "\n",
    "        # # Convert labels to a list of integers if they are tensors\n",
    "        # if isinstance(labels, torch.Tensor):\n",
    "        #     labels = labels.tolist()\n",
    "\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(\"Text Data Shape:\", text_data.shape)\n",
    "        print(\"Audio Data Shape:\", audio_data.shape)\n",
    "        print(\"Video Data Shape:\", video_data.shape)\n",
    "        print(\"Labels:\", labels_data.shape)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Print a few batches from the training DataLoader\n",
    "print(\"Training DataLoader Samples:\")\n",
    "print_dataloader_samples(train_dataloader, num_batches=2)\n",
    "\n",
    "# # Print a few batches from the validation DataLoader\n",
    "# print(\"Validation DataLoader Samples:\")\n",
    "# print_dataloader_samples(val_dataloader, num_batches=5)\n",
    "\n",
    "# # Print a few batches from the validation DataLoader\n",
    "# print(\"Validation DataLoader Samples:\")\n",
    "# print_dataloader_samples(test_dataloader, num_batches=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMU Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Gated Multimodal Unit of Arevalo et al. (2017)\n",
    "class GatedMultimodalUnit(torch.nn.Module):\n",
    "    def __init__(self, text_dim, audio_dim, video_dim, output_dim):\n",
    "        super(GatedMultimodalUnit, self).__init__()\n",
    "        \n",
    "        # Linear transformation for text\n",
    "        self.text_linear = LinearTransformations(text_dim, output_dim)\n",
    "        \n",
    "        # Convolutional layers for audio and video features\n",
    "        self.audio_conv = nn.Conv1d(audio_dim, output_dim, kernel_size=1)\n",
    "        self.video_conv = nn.Conv1d(video_dim, output_dim, kernel_size=1)\n",
    "        \n",
    "        self.output_dim = output_dim \n",
    "        \n",
    "        # Activation functions\n",
    "        self.activation = nn.Tanh()\n",
    "        self.gate_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Weight matrices for each modality\n",
    "        self.W1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.W3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Gating matrices\n",
    "        self.Y1 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y2 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.Y3 = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        init.xavier_uniform_(self.W1)\n",
    "        init.xavier_uniform_(self.W2)\n",
    "        init.xavier_uniform_(self.W3)\n",
    "        \n",
    "        # Initialize gating matrices\n",
    "        init.xavier_uniform_(self.Y1)\n",
    "        init.xavier_uniform_(self.Y2)\n",
    "        init.xavier_uniform_(self.Y3)\n",
    "        \n",
    "        \n",
    "    def forward(self, text_features, audio_features, video_features):\n",
    "\n",
    "        # Process text features to match shape\n",
    "        x_t = self.text_linear(text_features)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process audio features to match shape\n",
    "        audio_features = audio_features.squeeze(1).permute(0, 2, 1)               # Shape: [batch_size, audio_dim, sequence_length] \n",
    "        x_a = self.audio_conv(audio_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    "\n",
    "        # Process video features to match shape\n",
    "        video_features = video_features.permute(0, 2, 1)   # Shape: [batch_size, video_dim, sequence_length]\n",
    "        x_v = self.video_conv(video_features).mean(dim=-1)              # Shape: [batch_size, output_dim]\n",
    " \n",
    "        h1 = self.activation(torch.matmul(x_t, self.W1))        # Shape: [batch_size, output_dim]\n",
    "        h2 = self.activation(torch.matmul(x_a, self.W2))        # Shape: [batch_size, output_dim]\n",
    "        h3 = self.activation(torch.matmul(x_v, self.W3))        # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Compute modality-specific gating weights\n",
    "        z1 = self.gate_activation(torch.matmul(x_t, self.Y1))  # Shape: [batch_size, output_dim]\n",
    "        z2 = self.gate_activation(torch.matmul(x_a, self.Y2))  # Shape: [batch_size, output_dim]\n",
    "        z3 = self.gate_activation(torch.matmul(x_v, self.Y3))  # Shape: [batch_size, output_dim]\n",
    "        \n",
    "        # Calculate final output\n",
    "        h = z1 * h1 + z2 * h2 + z3 * h3         \n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "GMU Output Shape: torch.Size([8, 512])\n",
      "GMU Output:  tensor([[ 0.1382,  0.0638,  0.1628,  ...,  0.1201,  0.1369,  0.0208],\n",
      "        [ 0.0326,  0.0710,  0.2093,  ...,  0.0479,  0.1834,  0.0766],\n",
      "        [-0.0876,  0.0845,  0.0854,  ..., -0.1002,  0.0340,  0.1118],\n",
      "        ...,\n",
      "        [ 0.1899,  0.0347,  0.0661,  ..., -0.0532,  0.2278, -0.0294],\n",
      "        [-0.0246,  0.1453,  0.1431,  ...,  0.1154,  0.2444, -0.0301],\n",
      "        [-0.0336,  0.0048,  0.2045,  ...,  0.1810,  0.2638, -0.0491]])\n"
     ]
    }
   ],
   "source": [
    "# Test the GMU model using the items from dataloader as input\n",
    "\n",
    "# Define dimensions\n",
    "text_dim = 1024\n",
    "audio_dim = 768  # Number of channels in audio data\n",
    "video_dim = 768  # Number of channels in video data\n",
    "output_dim = 512  # You can set this to any value, depending on your requirements\n",
    "\n",
    "# Instantiate the GMU model\n",
    "gmu = GatedMultimodalUnit(text_dim, audio_dim, video_dim, output_dim)\n",
    "\n",
    "# Use DataLoader to get a batch of data\n",
    "for batch in train_dataloader:  # You can use any DataLoader (train_dataloader, val_dataloader, etc.)\n",
    "    text_data, audio_data, video_data, labels = batch\n",
    "    \n",
    "   \n",
    "    # Feed the entire batch to the GMU model\n",
    "    with torch.no_grad():\n",
    "        output = gmu(text_data, audio_data, video_data)\n",
    "    \n",
    "    # Print the output shape\n",
    "    print('-'*50)\n",
    "    print(\"GMU Output Shape:\", output.shape)\n",
    "    print(\"GMU Output: \", output)\n",
    "    \n",
    "    # Break after the first batch for testing purposes\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:\n",
      "Text file: tt0021814.npy\n",
      "Audio file: feature_tt0021814.npy\n",
      "Video file: tt0021814_features.npy\n",
      "--------------------------------------------------\n",
      "Text Feature Shape: torch.Size([1024])\n",
      "Audio Feature Shape: torch.Size([1, 197, 768])\n",
      "Video Feature Shape torch.Size([95, 768])\n",
      "--------------------------------------------------\n",
      "Model output shape: torch.Size([1, 768]) ###[batch_size, output_dim]\n",
      "--------------------------------------------------\n",
      "Model output: tensor([[-7.8372e-02,  9.9081e-03, -2.8619e-02, -1.0420e-01, -1.0034e-01,\n",
      "         -1.6091e-01,  3.9978e-02,  1.6649e-01, -1.9757e-01,  2.3029e-01,\n",
      "          1.2140e-01, -2.0359e-01,  1.8710e-01,  5.9678e-02,  7.7132e-02,\n",
      "         -8.8488e-02, -3.1042e-01, -1.2895e-01, -1.2193e-01, -1.6796e-01,\n",
      "          5.7486e-03, -1.7989e-02,  3.6665e-02, -2.8600e-01,  1.1463e-01,\n",
      "         -1.1215e-01,  7.5445e-02, -2.9206e-01,  2.4519e-01,  2.3103e-01,\n",
      "         -2.4058e-02, -2.5407e-01, -5.0657e-02, -7.7305e-02,  1.2652e-01,\n",
      "         -4.0762e-02,  3.9658e-02, -2.7458e-02,  1.2103e-01,  2.2002e-01,\n",
      "         -1.0260e-01,  2.2438e-01,  1.3911e-01, -1.1156e-01,  3.2057e-02,\n",
      "          5.3641e-02, -9.9498e-02,  2.7552e-01, -5.3724e-02, -1.7384e-01,\n",
      "          2.6668e-02,  2.4414e-02, -1.1149e-02, -1.4308e-02, -1.3806e-01,\n",
      "         -4.3808e-02,  1.3074e-01,  5.2335e-02,  1.7602e-02, -2.9195e-02,\n",
      "         -5.3113e-02, -1.0536e-01,  7.9111e-02,  9.3959e-03, -1.2284e-01,\n",
      "         -4.9762e-02, -6.6180e-02, -1.1128e-01, -4.9787e-02,  1.7254e-01,\n",
      "          1.8704e-01,  2.2036e-02,  5.0105e-02,  8.4117e-02, -1.4741e-01,\n",
      "         -9.8517e-02, -9.3725e-02, -2.2126e-01,  2.3803e-03, -1.2234e-01,\n",
      "         -2.1176e-01,  1.7223e-04,  1.0690e-01, -1.5240e-01,  1.2818e-01,\n",
      "          2.0026e-02,  1.9822e-01, -3.2971e-02, -5.9625e-02, -2.4556e-01,\n",
      "         -2.1108e-01, -1.7273e-01, -9.6521e-03,  1.9526e-01, -7.6923e-02,\n",
      "          3.3751e-01,  3.1784e-02, -7.5173e-02, -5.9687e-02, -1.8425e-01,\n",
      "          1.0409e-01,  2.3899e-01, -1.9727e-01,  4.6853e-02,  9.9696e-02,\n",
      "          1.8835e-01,  2.2333e-01,  1.3457e-01, -1.3226e-01,  1.6104e-01,\n",
      "         -7.2055e-02, -3.0941e-01, -2.0468e-01, -1.2495e-01,  7.6585e-02,\n",
      "          1.4528e-01,  1.6633e-01,  7.8560e-02, -1.9784e-01,  1.9113e-01,\n",
      "         -7.4228e-02,  1.0514e-01, -1.7131e-01,  3.2399e-01, -2.0357e-01,\n",
      "          3.1635e-01,  1.4743e-01,  1.1248e-01,  1.5653e-01,  2.9618e-02,\n",
      "         -3.6052e-01,  9.8769e-02,  2.8971e-01,  6.5306e-02, -2.2519e-01,\n",
      "          2.1859e-01,  1.4109e-01, -2.2152e-02,  7.9953e-02,  1.5339e-01,\n",
      "          2.1288e-01, -5.6771e-02, -2.3553e-01,  2.4379e-02, -2.4008e-01,\n",
      "         -9.6479e-02, -1.1905e-01,  1.7043e-01,  8.4937e-02,  2.5117e-02,\n",
      "         -8.5492e-02, -1.9626e-02, -8.6035e-02,  1.6294e-01,  2.2128e-01,\n",
      "          6.1728e-02,  1.6636e-02,  8.4929e-02, -1.1098e-01, -1.5021e-01,\n",
      "          3.4203e-02, -1.9270e-01, -1.1007e-01, -9.9114e-04, -4.0923e-02,\n",
      "         -2.4488e-02,  1.7806e-01, -5.9853e-02,  8.2603e-02, -7.5959e-02,\n",
      "          5.5392e-02, -9.0377e-02, -2.6767e-01,  7.7606e-02,  8.7312e-02,\n",
      "         -1.0563e-01,  5.0157e-02, -4.0964e-02,  2.2886e-01, -1.9932e-01,\n",
      "         -1.2205e-02,  5.4188e-03, -4.2489e-02, -3.4254e-01,  2.1013e-02,\n",
      "          1.8504e-02, -1.7265e-01,  6.6733e-02,  5.0996e-02,  8.6289e-02,\n",
      "         -7.7650e-02,  1.1407e-01,  1.6301e-02,  6.9524e-02,  3.8857e-02,\n",
      "         -3.4032e-01,  4.0353e-01,  6.9519e-02,  1.0736e-02, -6.9228e-02,\n",
      "         -4.3585e-02,  1.9817e-01,  1.2066e-01, -3.0675e-01, -4.1562e-02,\n",
      "          4.5728e-02,  1.1816e-01, -1.2466e-01,  4.4488e-02,  2.3227e-01,\n",
      "         -1.3339e-01,  4.6784e-02, -4.2193e-02,  9.5923e-02, -1.1174e-01,\n",
      "         -3.0753e-01, -1.5710e-01, -2.4224e-01, -1.6954e-02, -1.6490e-02,\n",
      "         -4.4275e-02,  5.2059e-02,  1.7978e-01, -2.9086e-02, -2.6108e-01,\n",
      "         -7.8472e-02,  3.1501e-01,  4.3517e-02, -7.1591e-02, -2.0019e-02,\n",
      "          4.9150e-02, -2.9602e-01, -1.1823e-01, -2.8382e-01, -5.8192e-03,\n",
      "         -9.2829e-02, -2.1341e-01,  1.3998e-01, -3.9519e-02, -5.8061e-05,\n",
      "         -3.1647e-01,  1.1919e-01, -1.2488e-01,  7.1353e-02, -9.4551e-02,\n",
      "          1.2448e-02, -2.2653e-01,  1.0797e-01, -1.4966e-01,  1.0701e-01,\n",
      "         -1.5245e-01,  3.8969e-02, -1.1301e-01, -1.6286e-01, -1.5365e-01,\n",
      "          1.3000e-01, -1.1985e-01, -1.6983e-01, -3.5536e-03,  1.6615e-01,\n",
      "          1.4475e-01,  1.3815e-01,  1.4881e-01,  2.4610e-01,  2.4917e-01,\n",
      "          5.0849e-02, -4.5597e-02, -1.7116e-01,  9.9942e-02,  1.9277e-01,\n",
      "         -5.4304e-02, -6.7742e-02,  1.5060e-01,  1.9162e-01, -1.8697e-01,\n",
      "          5.6361e-02,  2.5845e-01,  1.2328e-01, -2.6792e-01,  1.1937e-01,\n",
      "          1.6370e-01,  1.9792e-01,  1.5074e-01, -6.6829e-02,  2.4450e-01,\n",
      "          1.9104e-01,  1.3414e-01, -1.9488e-01,  1.3358e-01,  1.2523e-01,\n",
      "          2.7010e-02, -1.8789e-02,  4.0581e-01,  2.8377e-01, -1.2797e-01,\n",
      "          4.0198e-01,  5.5622e-02, -9.9155e-02,  3.6730e-02,  1.6142e-01,\n",
      "         -1.9708e-01, -6.7694e-02,  1.1491e-01,  6.5556e-02,  1.5543e-01,\n",
      "          1.9207e-01, -3.3854e-01, -1.5672e-01, -1.9620e-01, -5.8889e-02,\n",
      "         -9.9314e-02, -8.3072e-02,  2.3608e-02,  3.5502e-01,  1.0708e-01,\n",
      "          3.1639e-02,  5.0575e-02, -9.5465e-02, -2.4570e-01,  9.4791e-02,\n",
      "          6.3820e-02,  1.3245e-01, -1.9949e-01,  5.2810e-02,  1.6900e-01,\n",
      "         -6.5707e-02, -1.5903e-01, -4.4811e-02,  1.6242e-01, -9.4638e-02,\n",
      "          1.3222e-01, -9.2734e-02, -2.8692e-01,  7.9688e-02,  1.8592e-01,\n",
      "          1.0463e-01, -1.2063e-02, -2.5518e-02, -1.4232e-01,  6.3512e-02,\n",
      "         -1.3570e-01,  3.3001e-02, -2.4397e-01,  3.7676e-01,  1.8545e-01,\n",
      "         -1.8914e-01, -1.1744e-01,  1.2946e-01, -2.3409e-01, -7.0558e-02,\n",
      "          3.3837e-01, -8.6510e-02,  7.3294e-02, -1.3742e-01, -1.8213e-01,\n",
      "         -1.0807e-01,  3.0632e-01,  1.8006e-01,  2.2572e-01, -2.4925e-01,\n",
      "          3.3809e-02,  2.2226e-01,  2.0590e-01, -1.8296e-01, -1.1122e-01,\n",
      "          9.5319e-02,  2.1654e-01, -5.5550e-03, -1.0960e-02, -2.6894e-02,\n",
      "         -1.7066e-01, -2.8951e-01, -9.2095e-02, -1.0799e-01,  9.8049e-03,\n",
      "         -1.2772e-01, -1.3433e-01, -1.9799e-02, -1.7425e-01, -1.1159e-01,\n",
      "         -4.7608e-02, -2.4877e-02, -1.4828e-01, -1.0331e-02, -6.2980e-02,\n",
      "          1.7000e-01, -1.2086e-02, -4.1863e-01,  7.4720e-02, -2.3556e-01,\n",
      "         -6.8404e-02, -2.6956e-03,  2.3845e-01, -1.6896e-02, -1.1098e-01,\n",
      "         -1.6921e-01,  8.0510e-03, -5.3649e-02,  7.2913e-02, -7.7201e-02,\n",
      "         -2.6486e-03, -1.9224e-02, -2.6693e-01,  4.0288e-01,  5.2437e-02,\n",
      "          9.6218e-02,  2.4663e-01, -3.4795e-02,  9.3591e-02, -2.4542e-01,\n",
      "          2.5254e-01, -6.9731e-02, -5.9439e-02, -1.1635e-01,  2.6823e-01,\n",
      "          1.2819e-01,  5.1200e-02, -2.4602e-01,  1.9198e-01,  7.5426e-02,\n",
      "          4.4323e-02,  2.1990e-01, -2.9786e-01,  8.5018e-02,  1.5268e-02,\n",
      "          2.7304e-01, -1.9024e-02,  6.1349e-02,  2.7104e-01, -5.2493e-02,\n",
      "         -2.7961e-02,  9.4132e-02,  1.1322e-01, -1.0089e-01,  1.3763e-01,\n",
      "         -2.9745e-01,  1.4567e-01,  2.0458e-02,  1.0745e-01,  1.0614e-01,\n",
      "         -8.9115e-02,  1.7657e-01, -1.7570e-02,  1.8427e-03, -2.0228e-01,\n",
      "          1.9817e-01,  9.5997e-02,  6.6067e-02, -3.9591e-02, -1.0291e-01,\n",
      "          2.7974e-02,  4.1296e-02, -2.3313e-01, -8.4039e-02, -5.1217e-02,\n",
      "         -8.1844e-02, -1.1949e-01, -1.7974e-01, -1.2345e-01, -7.3069e-02,\n",
      "         -1.6205e-01, -1.1835e-01, -1.7960e-02,  1.2386e-01,  1.5660e-01,\n",
      "         -2.1632e-01,  2.0436e-01, -6.5768e-02,  1.7376e-01, -2.0186e-01,\n",
      "         -1.0332e-01,  5.3723e-02,  2.3573e-01, -1.7849e-01, -2.1331e-01,\n",
      "         -1.8661e-01,  3.0896e-01,  2.2985e-01, -1.4024e-01, -4.5880e-02,\n",
      "          2.6571e-01,  1.2822e-01, -4.1716e-02, -1.5466e-01, -1.8384e-01,\n",
      "          3.6881e-01, -2.4924e-02, -1.7802e-01, -2.0246e-01,  5.6118e-02,\n",
      "          7.8205e-02, -1.3979e-01,  1.1371e-01,  2.7062e-01,  1.2094e-01,\n",
      "         -5.4115e-03, -1.0507e-01,  4.7389e-02, -3.3626e-02, -1.6604e-01,\n",
      "          7.2340e-02, -1.5796e-01, -4.8356e-02, -2.1710e-02, -1.9284e-01,\n",
      "         -1.4077e-01,  1.9114e-01,  1.1007e-01,  1.5151e-01, -1.8838e-01,\n",
      "          1.4757e-01,  5.4224e-03, -3.1626e-02,  1.4870e-01, -4.4987e-02,\n",
      "         -2.4087e-01, -1.3955e-01, -1.4734e-02,  2.2750e-02, -2.5560e-01,\n",
      "         -1.8715e-01,  1.9968e-01, -4.9893e-03,  3.1239e-02, -1.8863e-01,\n",
      "         -3.7357e-02,  2.1236e-01,  2.0844e-02,  1.5136e-01, -1.4872e-01,\n",
      "         -2.2979e-01, -1.2301e-01, -8.5364e-02,  5.5802e-02,  2.0022e-01,\n",
      "         -7.6053e-02, -2.4898e-02, -7.9109e-02, -1.9354e-01,  5.4344e-02,\n",
      "         -7.5881e-02,  1.4043e-01, -3.0727e-01,  5.0904e-02,  3.6227e-02,\n",
      "         -2.5426e-01, -2.9167e-01, -9.7940e-02, -2.5419e-01,  3.1544e-02,\n",
      "         -1.7306e-01,  1.6337e-01,  1.2079e-01,  8.3731e-02,  2.3550e-01,\n",
      "         -4.3096e-02, -5.8164e-02, -2.9534e-02,  9.2874e-02, -3.3247e-02,\n",
      "          2.0139e-03,  1.0044e-01, -2.2494e-01,  8.4402e-02,  1.5736e-01,\n",
      "         -2.4685e-01, -2.4170e-01,  5.2857e-02, -2.1952e-01,  2.4277e-01,\n",
      "          7.5259e-02, -4.5475e-02, -1.9846e-01, -6.4909e-02,  6.7026e-02,\n",
      "          5.3573e-02,  1.3552e-01, -7.7158e-02, -5.2958e-03,  4.7687e-02,\n",
      "          8.4845e-02,  2.1051e-02,  1.0345e-01,  2.2015e-01, -1.5558e-01,\n",
      "         -1.7219e-01,  2.2189e-02,  3.7143e-02, -1.2251e-01, -2.7087e-01,\n",
      "          7.2999e-02, -1.6837e-01,  7.4842e-02,  9.9997e-02,  6.4565e-02,\n",
      "          1.0982e-01, -1.2853e-01, -3.5144e-01, -8.5612e-02,  2.3526e-01,\n",
      "         -2.9214e-01, -8.4556e-02,  1.2418e-01, -1.0889e-01,  1.5121e-01,\n",
      "          2.2041e-01,  8.4856e-02,  4.2915e-02,  7.0335e-02,  1.4737e-01,\n",
      "         -1.7105e-01, -6.1271e-03, -2.0997e-02, -1.0797e-01, -8.6564e-02,\n",
      "          1.2486e-01,  1.0514e-01,  1.6160e-01, -1.7709e-01,  2.5618e-01,\n",
      "         -8.6961e-02,  8.5897e-02, -1.5657e-01,  1.0370e-01, -1.8603e-01,\n",
      "          1.2762e-01,  1.1332e-01, -2.0421e-01,  3.1358e-01, -9.3986e-02,\n",
      "          3.0108e-01,  1.5254e-01, -1.1319e-01, -2.1295e-01, -7.9821e-02,\n",
      "          4.0217e-02,  2.2825e-01,  3.8510e-02,  3.6727e-01, -2.2346e-01,\n",
      "          4.8585e-02, -6.0612e-02, -3.5631e-01,  6.9769e-02,  3.7037e-02,\n",
      "         -2.2111e-01,  9.6289e-02,  2.8536e-01,  9.7207e-02,  1.0628e-02,\n",
      "          1.2215e-01, -2.6612e-01,  3.0525e-01, -7.0181e-02, -2.0600e-01,\n",
      "         -9.0730e-02, -1.8955e-01, -3.3429e-02, -2.9534e-01,  2.8701e-01,\n",
      "          1.6964e-01,  2.5527e-01,  2.6514e-01,  6.7909e-02, -1.8057e-02,\n",
      "         -2.2606e-01, -1.4980e-01, -1.3057e-01, -2.0005e-01,  1.5832e-01,\n",
      "          1.0361e-01, -5.2125e-02,  2.2715e-01, -9.4010e-02,  2.6506e-03,\n",
      "          8.2728e-02,  4.0376e-01,  2.9596e-02, -2.6323e-02,  5.9073e-02,\n",
      "         -1.1941e-01,  1.0900e-01,  7.9508e-02, -1.3571e-01,  1.1254e-01,\n",
      "          1.1940e-01, -4.1427e-01, -2.5798e-01, -2.4407e-01, -3.5527e-02,\n",
      "          1.7819e-02,  3.6836e-01,  3.7098e-03, -6.5720e-02, -1.5494e-01,\n",
      "          1.2194e-01,  8.1004e-02, -1.7230e-01,  1.6781e-01, -3.8847e-02,\n",
      "         -3.4371e-02, -1.3203e-01,  5.0223e-02,  2.5002e-03,  4.6128e-02,\n",
      "         -2.0968e-01, -1.3074e-01,  3.0792e-02,  1.9787e-01, -1.0605e-01,\n",
      "         -9.4370e-03, -3.4123e-01, -6.1970e-02,  6.7780e-02,  8.2931e-02,\n",
      "          1.4690e-01,  1.5324e-01,  1.7800e-02, -7.6458e-02,  1.8900e-03,\n",
      "          1.5071e-01, -7.7422e-02,  7.4885e-02,  4.0826e-02,  3.2747e-01,\n",
      "          5.5237e-02, -1.6284e-01,  1.9961e-02,  1.5135e-01, -5.2500e-02,\n",
      "         -2.8775e-01,  1.7239e-01,  1.4452e-01,  2.5212e-01,  3.4233e-02,\n",
      "         -5.1195e-02,  9.3541e-03, -1.2678e-01, -5.0765e-02,  7.8350e-02,\n",
      "          1.2287e-01,  5.2261e-02, -9.6318e-02, -2.2234e-01, -1.0623e-01,\n",
      "          1.3457e-01, -2.0963e-02, -1.7052e-01, -9.9307e-02,  4.7175e-02,\n",
      "         -3.4211e-02,  8.8808e-02, -3.7059e-01,  1.4490e-01,  3.8191e-01,\n",
      "          1.5100e-01,  5.5581e-02,  2.2564e-01, -1.5291e-01,  3.3674e-02,\n",
      "         -3.7749e-02,  1.1433e-01, -5.3985e-02,  1.4491e-01, -2.7516e-01,\n",
      "         -2.0930e-02, -1.8642e-02, -6.7489e-02]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from modules.dataloader import load_npy_files\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "# Select the first file from each modality directories (for testing) [insert index]\n",
    "text_file_name, text_features = text_features[0]\n",
    "audio_file_name, audio_features = audio_features[0]\n",
    "video_file_name, video_features = video_features[0]\n",
    "\n",
    "print(\"Selected File:\")\n",
    "print(\"Text file:\", os.path.basename(text_file_name))\n",
    "print(\"Audio file:\", os.path.basename(audio_file_name))\n",
    "print(\"Video file:\", os.path.basename(video_file_name))\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "# Define dimensions (make sure these match your model's expected input sizes)\n",
    "text_dim = 1024\n",
    "audio_dim = 768  # Number of channels in audio data\n",
    "video_dim = 768  # Number of channels in video data\n",
    "output_dim = 768  # You can set this to any value, depending on your requirements\n",
    "\n",
    "# Initialize the GMU model\n",
    "model = GatedMultimodalUnit(text_dim, audio_dim, video_dim, output_dim)\n",
    "\n",
    "# Move model to the same device as your data (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare the selected data samples\n",
    "text_features = text_features.to(device)  # Convert to tensor and move to device\n",
    "audio_features = audio_features.to(device)  # Convert to tensor and move to device\n",
    "video_features = video_features.to(device)  # Convert to tensor and move to device\n",
    "\n",
    "print(\"Text Feature Shape:\", text_features.shape)\n",
    "print(\"Audio Feature Shape:\", audio_features.shape)\n",
    "print(\"Video Feature Shape\", video_features.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Pass the data through the GMU model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    output = model(text_features.unsqueeze(0), audio_features, video_features.unsqueeze(0))\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Model output shape:\", output.shape, \"###[batch_size, output_dim]\")\n",
    "print(\"-\"*50)\n",
    "print(\"Model output:\", output) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-3):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dense_layer, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    dense_layer.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass inputs through GMU model\n",
    "        outputs = model(text_features, audio_features, video_features)\n",
    "        \n",
    "        # Pass the GMU outputs through the dense layer to get final predictions\n",
    "        predictions = dense_layer(outputs)  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Training Loss: {average_loss:.4f}\")\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).squeeze()\n",
    "            )\n",
    "\n",
    "            # Pass inputs through GMU model\n",
    "            outputs = model(text_features, audio_features, video_features)\n",
    "            \n",
    "            # Pass the GMU outputs through the dense layer to get final predictions\n",
    "            predictions = dense_layer(outputs).squeeze()  \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Evaluation Loss: {average_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return average_loss, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(text_dim, audio_dim, video_dim, output_dim, model_class,  dense_layer_class, dataset, criterion, optimizer_class, device, n_splits, collate_fn):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(dataset), 1):\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "        # Create subsets for training and validation\n",
    "        train_subset = Subset(dataset, train_index)\n",
    "        val_subset = Subset(dataset, val_index)\n",
    "        \n",
    "        # DataLoaders with batch size 8 and collate function\n",
    "        train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "        \n",
    "        # Initialize the model and dense layer for the current fold\n",
    "        model = model_class(text_dim=text_dim, audio_dim=audio_dim, video_dim=video_dim, output_dim=output_dim).to(device)\n",
    "        dense_layer = dense_layer_class(input_size=output_dim).to(device)\n",
    "        \n",
    "        # Combine parameters of GMU model and DenseLayer for the optimizer\n",
    "        optimizer = optimizer_class(list(model.parameters()) + list(dense_layer.parameters()))\n",
    "        \n",
    "        print(f\"Training model for fold {fold}\")\n",
    "        train_loss = train_model(model, dense_layer, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        print(f\"Evaluating model for fold {fold}\")\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model, dense_layer, val_loader, criterion, device)\n",
    "        \n",
    "        total_loss += val_loss\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1_score\n",
    "    \n",
    "    average_cv_loss = total_loss / n_splits\n",
    "    average_cv_precision = total_precision / n_splits\n",
    "    average_cv_recall = total_recall / n_splits\n",
    "    average_cv_f1 = total_f1 / n_splits\n",
    "    \n",
    "    print(f\"Average Cross-Validation Loss: {average_cv_loss:.4f}\")\n",
    "    print(f\"Average Cross-Validation Precision: {average_cv_precision:.4f}\")\n",
    "    print(f\"Average Cross-Validation Recall: {average_cv_recall:.4f}\")\n",
    "    print(f\"Average Cross-Validation F1 Score: {average_cv_f1:.4f}\")\n",
    "    \n",
    "    return average_cv_loss, average_cv_precision, average_cv_recall, average_cv_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold 1/50\n",
      "Training model for fold 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Cross-validation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m average_cv_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGatedMultimodalUnit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_layer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDenseLayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use your complete dataset for cross-validation\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBCELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass optimizer class, not the instantiated optimizer\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m, in \u001b[0;36mcross_validate_model\u001b[1;34m(text_dim, audio_dim, video_dim, output_dim, model_class, dense_layer_class, dataset, criterion, optimizer_class, device, n_splits, collate_fn)\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizer_class(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(dense_layer\u001b[38;5;241m.\u001b[39mparameters()))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m val_loss, precision, recall, f1_score \u001b[38;5;241m=\u001b[39m evaluate_model(model, dense_layer, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dense_layer, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 29\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the dimensions\n",
    "text_dim = 1024  \n",
    "audio_dim = 768 \n",
    "video_dim = 768  \n",
    "output_dim = 512 \n",
    "\n",
    "# Cross-validation\n",
    "average_cv_loss = cross_validate_model(\n",
    "    text_dim=text_dim,\n",
    "    audio_dim=audio_dim,\n",
    "    video_dim=video_dim,\n",
    "    output_dim=output_dim,\n",
    "    model_class=GatedMultimodalUnit,\n",
    "    dense_layer_class=DenseLayer,\n",
    "    dataset=full_dataset,  # Use your complete dataset for cross-validation\n",
    "    criterion=BCELoss(),\n",
    "    optimizer_class=get_optimizer,  # Pass optimizer class, not the instantiated optimizer\n",
    "    device=device,\n",
    "    n_splits=50,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
