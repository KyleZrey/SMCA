{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, ExponentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "from modules.cross_attentionb import CrossAttentionB\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss, CustomLoss, BCEWithLogits\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible configurations for audio feature directories\n",
    "audio_feature_paths = {\n",
    "    'logmel': '../misc/audio_features/logmel',\n",
    "    'mfcc': '../misc/audio_features/mfcc'\n",
    "}\n",
    "\n",
    "# Function to get the audio feature path based on the selected configuration\n",
    "def get_audio_feature_path(config_name):\n",
    "    if config_name in audio_feature_paths:\n",
    "        return audio_feature_paths[config_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Configuration '{config_name}' not found. Available options: logmel, mfcc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Worker seed function for DataLoader\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "\n",
    "        return seed_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "### Modality Assignment: No Diff for assignments, but the sequence is TAV\n",
    "\n",
    "set_seed(42)  # Set global seed at the start\n",
    "\n",
    "### Audio Feature selection: 'logmel' or 'mfcc'\n",
    "selected_audio_feature= 'logmel'\n",
    "audio_features_dir = get_audio_feature_path(selected_audio_feature)\n",
    "\n",
    "#### Data Configuration: 8 16 32 64 128\n",
    "train_batch_size = 32   # Set the batch size for training data\n",
    "val_batch_size = 16     # Set the batch size for validation data\n",
    "test_batch_size= 16     # Set the batch size for testing data\n",
    "\n",
    "### Filter outliers in video features\n",
    "lower_bound=35\n",
    "upper_bound=197\n",
    "\n",
    "#FIXED CONSTANT\n",
    "max_pad = 197\n",
    "\n",
    "### Hyperparameters\n",
    "threshold = 0.5              # for predictions\n",
    "cl_dropout_rate = 0.4        # for FinalClassifier\n",
    "att_dropout_rate = 0.4       # for MutualCrossAttention\n",
    "num_epochs = 10              # for model training\n",
    "\n",
    "### LR Scheduler Parameters     \n",
    "learning_rate = 1e-4      \n",
    "decay_rate = 0.9\n",
    "\n",
    "### Classifier Configuration\n",
    "isBCELoss = True                          # !!! SET ACCORDINGLY !!!\n",
    "criterion = BCELoss()\n",
    "# criterion = BCEWithLogits()\n",
    "# criterion = CustomLoss(pos_weight=2.94)\n",
    "# criterion = FocalLoss(alpha=0.25, gamma=2, pos_weight=0.34)\n",
    "\n",
    "# !!! Choose Classifier !!! False = Dense Layer, True = Final Classifier\n",
    "isFinalClassifier = True\n",
    "\n",
    "#\n",
    "# ## For cross validation\n",
    "num_folds = 5          # Set the number of folds for cross-validation\n",
    "num_epochs_cv = 10     # Set the number of epochs for cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_indices = []\n",
    "        missing_files = []\n",
    "\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Filter id_label_df to only include valid rows\n",
    "        self.id_label_df = self.id_label_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.missing_files = missing_files\n",
    "        \n",
    "        # Update valid_indices to reflect the new indices after resetting\n",
    "        valid_indices = list(range(len(self.id_label_df)))\n",
    "\n",
    "        # Return valid indices\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return imdbid, text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unpack batch elements\n",
    "    imdbids, text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return imdbids, text_data, audio_data, video_data_padded, label_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_video_features(video_features, lower_bound=lower_bound, upper_bound=upper_bound):\n",
    "    # Assuming video_features is a list of tuples where the second element is the numpy array\n",
    "    filtered_video_features = [v for v in video_features if lower_bound <= v[1].shape[0] <= upper_bound]\n",
    "    return filtered_video_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = '../misc/text_features'\n",
    "audio_features_dir = audio_features_dir\n",
    "video_features_dir = '../misc/video_features'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "video_features = filter_video_features(video_features)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "id_label_df = id_label_df.drop(columns=['Movie Title', 'URL'])\n",
    "\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n",
    "# perform train-test split on the filtered DataFrame\n",
    "train_df, val_test_df = train_test_split(\n",
    "    full_dataset.id_label_df, test_size=0.3, random_state=42, stratify=full_dataset.id_label_df['Label'])\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(\n",
    "    val_test_df, test_size=0.5, random_state=42, stratify=val_test_df['Label'])\n",
    "\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:\", val_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)\n",
    "\n",
    "print(\"Train label distribution:\", train_df['Label'].value_counts())\n",
    "print(\"Validation label distribution:\", val_df['Label'].value_counts())\n",
    "print(\"Test label distribution:\", test_df['Label'].value_counts())\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# create datasets based on these splits\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42), worker_init_fn=set_seed(42))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42), worker_init_fn=set_seed(42))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42), worker_init_fn=set_seed(42))\n",
    "\n",
    "# Function to calculate and print the size of each DataLoader\n",
    "def print_dataloader_sizes(dataloader, name):\n",
    "    total_samples = len(dataloader.dataset)  # Get the size of the dataset\n",
    "    num_batches = len(dataloader)  # Get the number of batches\n",
    "    print(f\"{name} DataLoader: Total Samples = {total_samples}, Number of Batches = {num_batches}\")\n",
    "\n",
    "# Print sizes of each DataLoader\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualCrossAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(MutualCrossAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Move x1 and x2 to the correct device\n",
    "        x1, x2 = x1.to(self.device), x2.to(self.device)\n",
    "\n",
    "        # Ensure inputs are 3D\n",
    "        if x1.dim() == 2:\n",
    "            x1 = x1.unsqueeze(0)\n",
    "        if x2.dim() == 2:\n",
    "            x2 = x2.unsqueeze(0)\n",
    "\n",
    "        # Assign x1 and x2 to query and key\n",
    "        query = x1\n",
    "        key = x2\n",
    "        d = query.shape[-1]\n",
    "\n",
    "        # Basic attention mechanism formula to get intermediate output A\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(d)\n",
    "        output_A = torch.bmm(self.dropout(F.softmax(scores, dim=-1)), x2)\n",
    "\n",
    "        # Make the summation of the two intermediate outputs\n",
    "        output = output_A\n",
    "        \n",
    "        return output.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HadamardProduct(tensor1, tensor2):\n",
    "    # Ensure both tensors have the same shape\n",
    "    if tensor1.shape != tensor2.shape:\n",
    "        raise ValueError(\"Tensors must have the same shape for Hadamard product.\")\n",
    "    \n",
    "    # Compute the Hadamard product\n",
    "    return tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element Add\n",
    "class EmbracementLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbracementLayer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "        \n",
    "    def forward(self, video_features, audio_features, text_features):\n",
    "        combined_features = video_features + audio_features + text_features\n",
    "        norm_features = self.norm(combined_features)\n",
    "        return norm_features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PairCrossAttention(modalityAlpha, modalityBeta, dropout=att_dropout_rate):\n",
    "    mutual_cross_attn = MutualCrossAttention(dropout).to(device)\n",
    "\n",
    "    return mutual_cross_attn(modalityAlpha, modalityBeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simul-parallel Functions and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimultaneousParallel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(SimultaneousParallel, self).__init__()\n",
    "        self.device = device\n",
    "        self.embrace = EmbracementLayer()\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "        \n",
    "    def forward(self, text_feature, audio_feature, video_feature):\n",
    "\n",
    "        # One way cross-attention\n",
    "        text_video = PairCrossAttention(text_feature, video_feature).to(self.device) \n",
    "        text_audio = PairCrossAttention(text_feature, audio_feature).to(self.device)\n",
    "        audio_video = PairCrossAttention(audio_feature, video_feature).to(self.device)\n",
    "        audio_text = PairCrossAttention(audio_feature, text_feature).to(self.device)\n",
    "        video_text = PairCrossAttention(video_feature, text_feature).to(self.device)\n",
    "        video_audio = PairCrossAttention(video_feature, audio_feature).to(self.device)\n",
    "\n",
    "        # Hadamard products for pairs\n",
    "        text_combined = HadamardProduct(text_video, text_audio).to(self.device)\n",
    "        audio_combined = HadamardProduct(audio_video, audio_text).to(self.device)\n",
    "        video_combined = HadamardProduct(video_text, video_audio).to(self.device)\n",
    "\n",
    "        text_combined = self.norm(text_combined)\n",
    "        audio_combined = self.norm(audio_combined)\n",
    "        video_combined = self.norm(video_combined)\n",
    "\n",
    "        # Combine products using element add then normalize\n",
    "        fused = self.embrace(audio_combined, video_combined, text_combined).to(self.device)\n",
    "\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(features, max_pad=max_pad):\n",
    "    # Pad or trim the sequence dimension to `max_pad`\n",
    "    if features.size(1) < max_pad:\n",
    "        # Pad to the right along the sequence dimension\n",
    "        features = F.pad(features, (0, 0, 0, max_pad - features.size(1)))\n",
    "    elif features.size(1) == max_pad:\n",
    "        pass\n",
    "    else:\n",
    "        # Trim if the sequence is longer than `max_pad`\n",
    "        features = features[:, :max_pad, :]\n",
    "        print(\"VIDEO TRIMMED SOMETHING MAY MALI\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=cl_dropout_rate):\n",
    "        super(FinalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 3072)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(3072, 768)          # Second fully connected layer\n",
    "        # self.fc3 = nn.Linear(768, 128)          # Third fully connected layer\n",
    "        # self.fc4 = nn.Linear(128, 64)          # Second fully connected layer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate) # Dropout layer\n",
    "        self.dense = nn.Linear(768, 1)          # Final dense layer for binary classification\n",
    "        self.relu = nn.ReLU()                    # ReLU activation\n",
    "        self.sigmoid = nn.Sigmoid()              # Sigmoid activation for final output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)                         # First fully connected layer\n",
    "        x = self.relu(x)                        # Apply ReLU activation\n",
    "\n",
    "        x = self.fc2(x)                         # second fully connected layer\n",
    "        x = self.relu(x)                        # Apply ReLU activation\n",
    "        x = self.dropout(x)                     # Apply dropout\n",
    "        \n",
    "        # x = self.fc3(x)                         # third fully connected layer\n",
    "        # x = self.relu(x)                        # Apply ReLU activation\n",
    "\n",
    "        # x = self.fc4(x)                         # fourth fully connected layer\n",
    "        # x = self.relu(x)                        # Apply ReLU activation\n",
    "        # x = self.dropout(x)                     # Apply dropout\n",
    "        \n",
    "        x = self.dense(x)                       # Final dense layer\n",
    "        if isBCELoss:\n",
    "            x = self.sigmoid(x)                  # Apply sigmoid activation\n",
    "        return x                                 # Output probabilities for BCELoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    dense_layer, \n",
    "    dataloader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device,\n",
    "    output_dir='results/simulParallel/', \n",
    "    output_filename='train_predictions.csv',\n",
    "):\n",
    "    # model.train()\n",
    "    dense_layer.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "    \n",
    "    # Reset metrics at the start of training\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "    \n",
    "\n",
    "    for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device).view(-1)\n",
    "        )\n",
    "                \n",
    "        # Squeeze the audio features to remove the extra dimension\n",
    "        audio_features = audio_features.squeeze(1).to(device) \n",
    "        text_features = text_features.unsqueeze(1).to(device) \n",
    "\n",
    "        # Apply linear transformations to match dimensions\n",
    "        linear_transform_text = LinearTransformations(text_features.shape[-1], 768).to(device)    \n",
    "        text_features = linear_transform_text(text_features).to(device) \n",
    "        \n",
    "        audio_features = audio_features[:, -1, :].unsqueeze(1).to(device)   # Resulting shape: [batch_size, 1, 768]\n",
    "\n",
    "        video_features = pad_features(video_features).to(device)   # Shape will be [batch_size, max_pad, 768]\n",
    "\n",
    "        outputs = model(text_features, audio_features, video_features).to(device)\n",
    "\n",
    "        outputs = outputs.view(outputs.size(0), -1)  # Shape will be [batch_size, 153600]\n",
    "\n",
    "        # Pass the fused features through the dense layer\n",
    "        predictions = dense_layer(outputs).view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # !!!Apply if BCEWithLogits or CustomLoss!!!\n",
    "        if not isBCELoss:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "\n",
    "        # Apply threshold to get binary predictions\n",
    "        preds = (predictions >= threshold).float()\n",
    "        \n",
    "        # Collect results for each sample\n",
    "        for i in range(len(imdbids)):\n",
    "            results.append({\n",
    "                'IMDBid': imdbids[i],\n",
    "                'Raw Prediction': predictions[i].item(),\n",
    "                'Binary Prediction': preds[i].item(),\n",
    "                'Target': targets[i].item()\n",
    "            })\n",
    "        \n",
    "        # Update metrics for binary classification\n",
    "        precision_metric.update(preds.long(), targets.long())\n",
    "        recall_metric.update(preds.long(), targets.long())\n",
    "        f1_metric.update(preds.long(), targets.long())\n",
    "        accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "    # Compute average precision, recall, F1 score, and accuracy\n",
    "    train_precision = precision_metric.compute().item()\n",
    "    train_recall = recall_metric.compute().item()\n",
    "    train_f1_score = f1_metric.compute().item()\n",
    "    train_accuracy = accuracy_metric.compute().item()  # Compute accuracy\n",
    "    \n",
    "    train_average_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "\n",
    "    return train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model, \n",
    "    dense_layer, \n",
    "    dataloader, \n",
    "    criterion, \n",
    "    device,\n",
    "    output_dir='results/simulParallel/', \n",
    "    output_filename='val_predictions.csv',\n",
    "):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "    \n",
    "    # Reset metrics at the start of training\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "        \n",
    "            # Squeeze the audio features to remove the extra dimension\n",
    "            audio_features = audio_features.squeeze(1).to(device)\n",
    "            text_features = text_features.unsqueeze(1).to(device)\n",
    "\n",
    "            # Apply linear transformations to match dimensions\n",
    "            linear_transform_text = LinearTransformations(text_features.shape[-1], 768).to(device)   \n",
    "            text_features = linear_transform_text(text_features).to(device)\n",
    "            \n",
    "            audio_features = audio_features[:, -1, :].unsqueeze(1).to(device)  # Resulting shape: [batch_size, 1, 768]\n",
    "    \n",
    "            video_features = pad_features(video_features).to(device)  # Shape will be [batch_size, max_pad, 768]\n",
    "                    \n",
    "            outputs = model(text_features, audio_features, video_features).to(device)\n",
    "\n",
    "            outputs = outputs.view(outputs.size(0), -1)  # Shape will be [batch_size, 153600]\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # !!!Apply if BCEWithLogits or CustomLoss!!!\n",
    "            if not isBCELoss:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions >= threshold).float()\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Collect results for each sample\n",
    "            for i in range(len(imdbids)):\n",
    "                results.append({\n",
    "                    'IMDBid': imdbids[i],\n",
    "                    'Raw Prediction': predictions[i].item(),\n",
    "                    'Binary Prediction': preds[i].item(),\n",
    "                    'Target': targets[i].item()\n",
    "                })\n",
    "            \n",
    "            # Update metrics for binary classification\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "            accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "    # Compute average precision, recall, F1 score, and accuracy\n",
    "    val_precision = precision_metric.compute().item()\n",
    "    val_recall = recall_metric.compute().item()\n",
    "    val_f1_score = f1_metric.compute().item()\n",
    "    val_accuracy = accuracy_metric.compute().item() \n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    val_conf_matrix = confusion_matrix(all_targets, np.round(all_predictions))\n",
    "\n",
    "    val_average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "    \n",
    "    return val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    model, \n",
    "    dense_layer, \n",
    "    dataloader, \n",
    "    criterion, \n",
    "    device,\n",
    "    output_dir='results/simulParallel/', \n",
    "    output_filename='test_predictions.csv',\n",
    "):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device) \n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # List to collect results for CSV\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imdbids, text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Squeeze the audio features to remove the extra dimension\n",
    "            audio_features = audio_features.squeeze(1).to(device) \n",
    "            text_features = text_features.unsqueeze(1).to(device) \n",
    "\n",
    "            # Apply linear transformations to match dimensions\n",
    "            linear_transform_text = LinearTransformations(text_features.shape[-1], 768).to(device)    \n",
    "            text_features = linear_transform_text(text_features).to(device) \n",
    "            \n",
    "            audio_features = audio_features[:, -1, :].unsqueeze(1).to(device)   # Resulting shape: [batch_size, 1, 768]\n",
    "    \n",
    "            video_features = pad_features(video_features).to(device)   # Shape will be [batch_size, max_pad, 768]\n",
    "\n",
    "            outputs = model(text_features, audio_features, video_features).to(device) \n",
    "    \n",
    "            outputs = outputs.view(outputs.size(0), -1)  # Shape will be [batch_size, 153600]\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1)\n",
    "                \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # !!!Apply if BCEWithLogits or CustomLoss!!!\n",
    "            if not isBCELoss:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions >= threshold).float()\n",
    "            \n",
    "            # Collect predictions and targets for the confusion matrix\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "            # Collect results for each sample\n",
    "            for i in range(len(imdbids)):\n",
    "                results.append({\n",
    "                    'IMDBid': imdbids[i],\n",
    "                    'Raw Prediction': predictions[i].item(),\n",
    "                    'Binary Prediction': preds[i].item(),\n",
    "                    'Target': targets[i].item()\n",
    "                })\n",
    "            \n",
    "            # Update metrics for binary classification\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "            accuracy_metric.update(preds.long(), targets.long()) \n",
    "\n",
    "     # Compute average precision, recall, F1 score, and accuracy\n",
    "    test_precision = precision_metric.compute().item()\n",
    "    test_recall = recall_metric.compute().item()\n",
    "    test_f1_score = f1_metric.compute().item()\n",
    "    test_accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    test_conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    test_average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False, header=True)\n",
    "\n",
    "    return test_average_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=learning_rate):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names=['Negative', 'Positive']):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and validation loss\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs = range(1, len(train_losses) + 1)  # Generate epoch numbers starting from 1\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', color='#1f77b4', linestyle='-', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss', color='#ff7f0e', linestyle='-', marker='x')\n",
    "    plt.title('Training and Validation Loss over Epochs')    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SimultaneousParallel(device).to(device)  # Dimension for d_out_kq and d_out_v\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Own DenseLayer or FinalClassifier\n",
    "    if isFinalClassifier:\n",
    "        dense_layer = FinalClassifier(output_dim * max_pad).to(device) \n",
    "    else:\n",
    "        dense_layer = DenseLayer(output_dim * max_pad).to(device)\n",
    "\n",
    "    optimizer = get_optimizer(list(dense_layer.parameters()), learning_rate)\n",
    "\n",
    "    scheduler = ExponentialLR(optimizer, gamma=decay_rate)\n",
    "    \n",
    "    # Lists to store the training and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"Number of Epochs:\", num_epochs)\n",
    "    print(\"learning rate:\", learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        train_losses.append(train_average_loss)  # Store training loss\n",
    "        \n",
    "        print(\"-\" * 20, \"Train\", \"-\" * 20)\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Train Precision: {train_precision:.4f}\")\n",
    "        print(f\"Train Recall: {train_recall:.4f}\")\n",
    "        print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
    "        print(f\"Train Loss: {train_average_loss:.4f}\")\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is None:\n",
    "                print(\"After train: model:\", \"No gradient for:\", name)\n",
    "        \n",
    "        for name, param in dense_layer.named_parameters():\n",
    "            if param.grad is None:\n",
    "                print(\"After train: classifier:\", \"No gradient for:\", name)\n",
    "\n",
    "        # Validate step\n",
    "        val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "        val_losses.append(val_average_loss)  # Store validation loss\n",
    "        \n",
    "        print(\"-\" * 20, \"Eval\", \"-\" * 20)\n",
    "        print(f\"Eval Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Eval Precision: {val_precision:.4f}\")\n",
    "        print(f\"Eval Recall: {val_recall:.4f}\")\n",
    "        print(f\"Eval F1 Score: {val_f1_score:.4f}\")\n",
    "        print(f\"Eval Loss: {val_average_loss:.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log current learning rate\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Learning Rate after Epoch {epoch + 1}: {current_lr:.8f}\")\n",
    "        \n",
    "    # Testing the model\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_average_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_conf_matrix = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n",
    "    \n",
    "    print(\"-\" * 20, \"Test\", \"-\" * 20)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1_score:.4f}\")\n",
    "    print(f\"Test Loss: {test_average_loss:.4f}\")\n",
    "    \n",
    "    # Summary of metrics\n",
    "    metrics_summary = {\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Validation Accuracy\": [val_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Train Precision\": [train_precision],\n",
    "        \"Validation Precision\": [val_precision],\n",
    "        \"Test Precision\": [test_precision],\n",
    "        \"Train Recall\": [train_recall],\n",
    "        \"Validation Recall\": [val_recall],\n",
    "        \"Test Recall\": [test_recall],\n",
    "        \"Train F1 Score\": [train_f1_score],\n",
    "        \"Validation F1 Score\": [val_f1_score],\n",
    "        \"Test F1 Score\": [test_f1_score],\n",
    "        \"Train Loss\": [train_average_loss],\n",
    "        \"Validation Loss\": [val_average_loss],\n",
    "        \"Test Loss\": [test_average_loss]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics_summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sizes of each DataLoader (FOR CHECKING)\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_loss_curves(train_losses_folds, val_losses_folds):\n",
    "    # Transpose folds to calculate averages per epoch\n",
    "    num_epochs = len(train_losses_folds[0])\n",
    "    avg_train_losses = [np.mean([fold[epoch] for fold in train_losses_folds]) for epoch in range(num_epochs)]\n",
    "    avg_val_losses = [np.mean([fold[epoch] for fold in val_losses_folds]) for epoch in range(num_epochs)]\n",
    "\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_losses, label='Average Training Loss', color='blue', marker='o')\n",
    "    plt.plot(epochs, avg_val_losses, label='Average Validation Loss', color='orange', marker='o')\n",
    "    \n",
    "    plt.title('Average Training and Validation Loss Across Folds')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_dataloader(dataloader, name=\"Dataloader\"):\n",
    "    total_samples = len(dataloader.sampler)\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"{name}: Total Samples = {total_samples}, Number of Batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(\n",
    "    dataset, \n",
    "    model_class, \n",
    "    dense_layer_class, \n",
    "    num_folds, \n",
    "    num_epochs, \n",
    "    output_dim,\n",
    "    criterion,\n",
    "    learning_rate,\n",
    "    train_batch_size,\n",
    "    val_batch_size,\n",
    "    output_file,\n",
    "    device=None\n",
    "):\n",
    "    # Set device configuration\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    set_seed(42)  # Set global seed at the start\n",
    "    \n",
    "    # Initialize the KFold splitter\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # lists to store metrics for each fold\n",
    "    fold_losses = []\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1_scores = []\n",
    "    \n",
    "    # Lists to store metrics and losses for each fold\n",
    "    train_losses_folds = []  # List of lists: train_losses_folds[fold][epoch]\n",
    "    val_losses_folds = []    # List of lists: val_losses_folds[fold][epoch]\n",
    "    \n",
    "    print(\"Number of Epochs:\", num_epochs)\n",
    "    print(\"learning rate:\", learning_rate)\n",
    "    \n",
    "    # Perform K-Fold Cross-Validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"------------------------- Fold {fold + 1 }/{num_folds} -------------------------\")\n",
    "        \n",
    "        # Create data loaders for the train and validation sets\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=train_batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "        val_dataloader = torch.utils.data.DataLoader(dataset, batch_size=val_batch_size, sampler=val_sampler, collate_fn=collate_fn)\n",
    "        \n",
    "        print_cv_dataloader(train_dataloader, \"Train DataLoader\")\n",
    "        print_cv_dataloader(val_dataloader, \"Validation DataLoader\")\n",
    "        \n",
    "        # Initialize the model, dense layer, criterion, and optimizer for each fold\n",
    "        model = model_class(device).to(device)\n",
    "        \n",
    "        dense_layer = dense_layer_class(output_dim * max_pad).to(device)\n",
    "        criterion = criterion.to(device)\n",
    "        optimizer = get_optimizer(list(dense_layer.parameters()), learning_rate)\n",
    "\n",
    "        scheduler = ExponentialLR(optimizer, gamma=decay_rate)\n",
    "        \n",
    "        # Initialize lists to track losses for this fold\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        epoch_progress = tqdm(range(num_epochs), desc=f\"Fold {fold + 1}/{num_folds} Training\", leave=True)\n",
    "\n",
    "\n",
    "        # Training loop for each fold\n",
    "        for epoch in epoch_progress:\n",
    "            print(f\"------------------------- Epoch {epoch + 1}/{num_epochs} -------------------------\")\n",
    "            \n",
    "            # Train and evaluate the model on the training and validation sets\n",
    "            train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "            val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "            \n",
    "            # Track losses for this fold and epoch\n",
    "            train_losses.append(train_average_loss)\n",
    "            val_losses.append(val_average_loss)\n",
    "            \n",
    "            print(f\"\\nTrain Accuracy: {train_accuracy:.4f}\")\n",
    "            print(f\"Train Precision: {train_precision:.4f}\")\n",
    "            print(f\"Train Recall: {train_recall:.4f}\")\n",
    "            print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
    "            print(f\"Train Loss: {train_average_loss:.4f}\")\n",
    "            \n",
    "            print(f\"\\nEval Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Eval Precision: {val_precision:.4f}\")   \n",
    "            print(f\"Eval Recall: {val_recall:.4f}\")\n",
    "            print(f\"Eval F1 Score: {val_f1_score:.4f}\")\n",
    "            print(f\"Eval Loss: {val_average_loss:.4f}\")\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log current learning rate\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"Learning Rate after Epoch {epoch + 1}: {current_lr:.8f}\")\n",
    "            \n",
    "            # Update tqdm progress bar description\n",
    "            epoch_progress.set_postfix({\n",
    "                \"Train Loss\": f\"{train_average_loss:.4f}\",\n",
    "                \"Val Loss\": f\"{val_average_loss:.4f}\",\n",
    "                \"Train F1\": f\"{train_f1_score:.4f}\",\n",
    "                \"Val F1\": f\"{val_f1_score:.4f}\",\n",
    "            })\n",
    "        \n",
    "        # Store losses for the fold\n",
    "        train_losses_folds.append(train_losses)\n",
    "        val_losses_folds.append(val_losses)\n",
    "           \n",
    "        # Store the validation metrics for this fold\n",
    "        fold_losses.append(val_average_loss)\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        fold_precisions.append(val_precision)\n",
    "        fold_recalls.append(val_recall)\n",
    "        fold_f1_scores.append(val_f1_score)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    avg_precision = np.mean(fold_precisions)\n",
    "    avg_recall = np.mean(fold_recalls)\n",
    "    avg_f1_score = np.mean(fold_f1_scores)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n",
    "\n",
    "    # Plot average loss curves across folds\n",
    "    plot_average_loss_curves(train_losses_folds, val_losses_folds)\n",
    "\n",
    "    results_dict = {\"Metrics\": [\"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]}\n",
    "    for i in range(num_folds):\n",
    "        results_dict[f\"Fold {i + 1}\"] = [fold_losses[i], fold_accuracies[i], fold_precisions[i], fold_recalls[i], fold_f1_scores[i]]\n",
    "    results_dict[\"Average\"] = [avg_loss, avg_accuracy, avg_precision, avg_recall, avg_f1_score]\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  \n",
    "        \n",
    "    # Save results to .csv\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseLayer or FinalClassifier\n",
    "if isFinalClassifier:\n",
    "    dense_layer_class = FinalClassifier\n",
    "else:\n",
    "    dense_layer_class = DenseLayer\n",
    "    \n",
    "output_dim = 768\n",
    "\n",
    "# Run k-fold cross-validation   \n",
    "results_df = cross_validate_model(\n",
    "    dataset=full_dataset,\n",
    "    model_class=SimultaneousParallel,\n",
    "    dense_layer_class=dense_layer_class,\n",
    "    criterion=criterion,\n",
    "    output_dim=output_dim,\n",
    "    num_epochs=num_epochs_cv,    \n",
    "    num_folds=num_folds,\n",
    "    train_batch_size=train_batch_size,\n",
    "    val_batch_size=val_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    output_file=f'results/simulParallel/SimulParallel-CV_scores.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'results/simulParallel/SimulParallel-CV_scores.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
