{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path for module imports\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import custom modules\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.linear_transformation import LinearTransformations\n",
    "from modules.classifier import DenseLayer, BCELoss, CustomLoss, BCEWithLogits, FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible configurations for audio feature directories\n",
    "audio_feature_paths = {\n",
    "    'logmel': 'D:\\Projects\\Thesis\\Audio',\n",
    "    'mfcc': 'D:\\Projects\\Thesis\\Audio 2\\mfcc_extracted'\n",
    "}\n",
    "\n",
    "# Function to get the audio feature path based on the selected configuration\n",
    "def get_audio_feature_path(config_name):\n",
    "    if config_name in audio_feature_paths:\n",
    "        return audio_feature_paths[config_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Configuration '{config_name}' not found. Available options: logmel, mfcc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "### Device configuration\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "### Audio Feature selection: 'logmel' or 'mfcc'\n",
    "selected_config = 'logmel'\n",
    "audio_features_dir = get_audio_feature_path(selected_config)\n",
    "\n",
    "#### Data Configuration: 8 16 32 64 128\n",
    "train_batch_size = 32   # Set the batch size for training data\n",
    "val_batch_size = 16     # Set the batch size for validation data\n",
    "test_batch_size= 16     # Set the batch size for testing data\n",
    "\n",
    "#FIXED CONSTANT\n",
    "max_pad = 197\n",
    "\n",
    "### Hyperparameters\n",
    "threshold = 0.5              # for predictions\n",
    "learning_rate = 1e-5         # For optimizer\n",
    "cl_dropout_rate = 0.4        # for FinalClassifier\n",
    "att_dropout_rate = 0.3       # for MutualCrossAttention\n",
    "num_epochs = 20              # for model training\n",
    "\n",
    "### Classifier Configuration\n",
    "isBCELoss = True                          # !!! SET ACCORDINGLY !!!\n",
    "criterion = BCELoss()\n",
    "# criterion = BCEWithLogits()\n",
    "# criterion = CustomLoss(pos_weight=2.94)\n",
    "# criterion = FocalLoss(alpha=0.25, gamma=2, pos_weight=0.34)\n",
    "\n",
    "# !!! Choose Classifier !!! False = Dense Layer, True = Final Classifier\n",
    "isFinalClassifier = False\n",
    "\n",
    "# ### For cross validation\n",
    "# num_folds = 5           # Set the number of folds for cross-validation\n",
    "# batch_size = 32         # Set the batch size for cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=0.5):\n",
    "        super(FinalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)  # From 2304 to 2048\n",
    "        self.fc2 = nn.Linear(1024, 512)        # From 2048 to 1024\n",
    "        # self.fc3 = nn.Linear(1024, 512)         # Optional 512 or 768\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(512, 1)          # Final output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.fc3(x)\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        if isBCELoss:\n",
    "            x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_indices = []\n",
    "        missing_files = []\n",
    "\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Filter id_label_df to only include valid rows\n",
    "        self.id_label_df = self.id_label_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.missing_files = missing_files\n",
    "        \n",
    "        # Update valid_indices to reflect the new indices after resetting\n",
    "        valid_indices = list(range(len(self.id_label_df)))\n",
    "\n",
    "        # Return valid indices\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return imdbid, text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imdbids, text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Pad video data\n",
    "    max_length = max(v.size(0) for v in video_data)\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    label_data = torch.stack(label_data)\n",
    "\n",
    "    return imdbids, text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_video_features(video_features, lower_bound=35, upper_bound=197):\n",
    "    # Assuming video_features is a list of tuples where the second element is the numpy array\n",
    "    filtered_video_features = [v for v in video_features if lower_bound <= v[1].shape[0] <= upper_bound]\n",
    "    return filtered_video_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1325\n",
      "train_df shape: (927, 2)\n",
      "val_df shape: (199, 2)\n",
      "test_df shape: (199, 2)\n",
      "Train label distribution: Label\n",
      "green    693\n",
      "red      234\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "Test label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Train DataLoader: Total Samples = 927, Number of Batches = 29\n",
      "Validation DataLoader: Total Samples = 199, Number of Batches = 13\n",
      "Test DataLoader: Total Samples = 199, Number of Batches = 13\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\Projects\\Thesis\\Text'\n",
    "audio_features_dir = audio_features_dir\n",
    "video_features_dir = 'D:\\Projects\\Thesis\\Video'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "video_features = filter_video_features(video_features)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "id_label_df = id_label_df.drop(columns=['Movie Title', 'URL'])\n",
    "\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n",
    "# perform train-test split on the filtered DataFrame\n",
    "train_df, val_test_df = train_test_split(\n",
    "    full_dataset.id_label_df, test_size=0.3, random_state=42, stratify=full_dataset.id_label_df['Label'])\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(\n",
    "    val_test_df, test_size=0.5, random_state=42, stratify=val_test_df['Label'])\n",
    "\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:\", val_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)\n",
    "\n",
    "print(\"Train label distribution:\", train_df['Label'].value_counts())\n",
    "print(\"Validation label distribution:\", val_df['Label'].value_counts())\n",
    "print(\"Test label distribution:\", test_df['Label'].value_counts())\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# create datasets based on these splits\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Function to calculate and print the size of each DataLoader\n",
    "def print_dataloader_sizes(dataloader, name):\n",
    "    total_samples = len(dataloader.dataset)  # Get the size of the dataset\n",
    "    num_batches = len(dataloader)  # Get the number of batches\n",
    "    print(f\"{name} DataLoader: Total Samples = {total_samples}, Number of Batches = {num_batches}\")\n",
    "\n",
    "# Print sizes of each DataLoader\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(features, max_pad=197):\n",
    "    # Pad or trim the sequence dimension to `max_pad`\n",
    "    if features.size(1) < max_pad:\n",
    "        # Pad to the right along the sequence dimension\n",
    "        features = F.pad(features, (0, 0, 0, max_pad - features.size(1)))\n",
    "    else:\n",
    "        # Trim if the sequence is longer than `max_pad`\n",
    "        features = features[:, :max_pad, :]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualCrossAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(MutualCrossAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Move x1 and x2 to the correct device\n",
    "        x1, x2 = x1.to(self.device), x2.to(self.device)\n",
    "\n",
    "        # Ensure inputs are 3D\n",
    "        if x1.dim() == 2:\n",
    "            x1 = x1.unsqueeze(0)\n",
    "        if x2.dim() == 2:\n",
    "            x2 = x2.unsqueeze(0)\n",
    "\n",
    "        # Assign x1 and x2 to query and key\n",
    "        query = x1\n",
    "        key = x2\n",
    "        d = query.shape[-1]\n",
    "\n",
    "        # Basic attention mechanism formula to get intermediate output A\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(d)\n",
    "        output_A = torch.bmm(self.dropout(F.softmax(scores, dim=-1)), x2)\n",
    "\n",
    "        # Make the summation of the two intermediate outputs\n",
    "        output = output_A\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HadamardProduct(tensor1, tensor2):\n",
    "    # Ensure both tensors have the same shape\n",
    "    if tensor1.shape != tensor2.shape:\n",
    "        raise ValueError(\"Tensors must have the same shape for Hadamard product.\")\n",
    "    \n",
    "    # Compute the Hadamard product\n",
    "    return tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbracementLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(EmbracementLayer, self).__init__()\n",
    "        self.fc = nn.Linear(d_in, d_out)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, video_features, audio_features, text_features):\n",
    "        # Ensure all inputs are on the same device as the layer\n",
    "        device = next(self.parameters()).device\n",
    "        video_features = video_features.to(device)\n",
    "        audio_features = audio_features.to(device)\n",
    "        text_features = text_features.to(device)\n",
    "\n",
    "        # Print shapes for debugging\n",
    "        print(\"Video features shape:\", video_features.shape)\n",
    "        print(\"Audio features shape:\", audio_features.shape)\n",
    "        print(\"Text features shape:\", text_features.shape)\n",
    "\n",
    "        # Adjust dimensions if necessary\n",
    "        if video_features.dim() == 1:\n",
    "            video_features = video_features.unsqueeze(0)\n",
    "        if audio_features.dim() == 1:\n",
    "            audio_features = audio_features.unsqueeze(0)\n",
    "        if text_features.dim() == 1:\n",
    "            text_features = text_features.unsqueeze(0)\n",
    "\n",
    "        # Ensure all features have the same sequence length\n",
    "        if video_features.shape[0] != audio_features.shape[0] or video_features.shape[0] != text_features.shape[0]:\n",
    "            # Use the maximum sequence length\n",
    "            max_seq_len = max(video_features.shape[0], audio_features.shape[0], text_features.shape[0])\n",
    "            \n",
    "            # Expand tensors to match the maximum sequence length\n",
    "            video_features = video_features.expand(max_seq_len, -1)\n",
    "            audio_features = audio_features.expand(max_seq_len, -1)\n",
    "            text_features = text_features.expand(max_seq_len, -1)\n",
    "\n",
    "        # Concatenate features along the last dimension\n",
    "        combined_features = torch.cat([video_features, audio_features, text_features], dim=1)\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        transformed_features = self.fc(combined_features)\n",
    "        \n",
    "        # Apply normalization and activation\n",
    "        norm_features = self.norm(transformed_features)\n",
    "        output = self.activation(norm_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=learning_rate):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PairCrossAttention(modalityAlpha, modalityBeta, dropout=att_dropout_rate):\n",
    "    mutual_cross_attn = MutualCrossAttention(dropout)\n",
    "    return mutual_cross_attn(modalityAlpha, modalityBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dense_layer, dataloader, criterion, optimizer, device, output_dir='results/', output_filename='train_predictions.csv'):\n",
    "    dense_layer.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "    print(\"-\" * 20, \"Train\", \"-\" * 20)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        imdbids, text_data, audio_data, video_data, targets = batch\n",
    "        batch_size = text_data.size(0)\n",
    "        \n",
    "        # Move all tensors to device\n",
    "        text_data = text_data.to(device)\n",
    "        audio_data = audio_data.to(device)  # [32, 1, 197, 768]\n",
    "        video_data = video_data.to(device)  # [32, 147, 768]\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Process the modalities through mutual cross attention\n",
    "        with torch.no_grad():\n",
    "            fused_features_list = []\n",
    "            \n",
    "            # Process each item in the batch\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                print(\"Before transformation and padding:\")\n",
    "                print(\"Text feature shape:\", text_data[i].shape)  # [1024]\n",
    "                print(\"Audio feature shape:\", audio_data[i].shape)  # [1, 197, 768]\n",
    "                print(\"Video feature shape:\", video_data[i].shape)  # [95, 768]\n",
    "\n",
    "                # Transform text features\n",
    "                linear_transform_text = LinearTransformations(text_data[i].shape[-1], 768)\n",
    "                text_feature = linear_transform_text(text_data[i].unsqueeze(0))  # [1, 768]\n",
    "\n",
    "                # Transform audio features - remove extra dimension and take last token\n",
    "                audio_feature = audio_data[i].squeeze(0)  # Remove batch dim -> [197, 768]\n",
    "                audio_feature = audio_feature[-1:, :]  # Take last token -> [1, 768]\n",
    "\n",
    "                # Transform and pad video features\n",
    "                video_feature = video_data[i]  # [174, 768]\n",
    "                video_feature = pad_features(video_feature.unsqueeze(0))  # Add batch dim and pad to [1, 197, 768]\n",
    "                \n",
    "                print(\"After transformation and padding:\")\n",
    "                print(\"Text feature shape:\", text_feature.shape)  # [1, 768]\n",
    "                print(\"Audio feature shape:\", audio_feature.shape)  # [1, 768]\n",
    "                print(\"Video feature shape:\", video_feature.shape)  # [1, 197, 768]\n",
    "\n",
    "                # Cross attention between modalities\n",
    "                text_video = PairCrossAttention(text_feature, video_feature.squeeze(0))  # video: [197, 768]\n",
    "                text_audio = PairCrossAttention(text_feature, audio_feature)\n",
    "                audio_video = PairCrossAttention(audio_feature, video_feature.squeeze(0))\n",
    "                audio_text = PairCrossAttention(audio_feature, text_feature)\n",
    "                video_text = PairCrossAttention(video_feature.squeeze(0), text_feature)\n",
    "                video_audio = PairCrossAttention(video_feature.squeeze(0), audio_feature)\n",
    "\n",
    "                # Hadamard products\n",
    "                text_combined = HadamardProduct(text_video, text_audio)\n",
    "                audio_combined = HadamardProduct(audio_video, audio_text)\n",
    "                video_combined = HadamardProduct(video_text, video_audio)\n",
    "\n",
    "                # Calculate input dimension for embracement layer\n",
    "                d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "                embracement_layer = EmbracementLayer(d_in, d_in).to(device)\n",
    "\n",
    "                # Get final fused features\n",
    "                fused_features = embracement_layer(\n",
    "                    video_combined.squeeze(0), \n",
    "                    audio_combined.squeeze(0), \n",
    "                    text_combined.squeeze(0)\n",
    "                )\n",
    "                \n",
    "                # Make sure fused_features has the right shape\n",
    "                # if len(fused_features.shape) == 1:\n",
    "                #     fused_features = fused_features.unsqueeze(0)  # Average across sequence dimension if present\n",
    "                \n",
    "                fused_features_list.append(fused_features)\n",
    "\n",
    "                print(\"Fused features shape:\", fused_features.shape)\n",
    "\n",
    "            # Stack all fused features for the batch\n",
    "            batch_fused_features = torch.stack(fused_features_list) # [batch_size, feature_dim]\n",
    "\n",
    "            print(\"Final fused features shape:\", batch_fused_features.shape)\n",
    "\n",
    "        # Forward pass through dense layer\n",
    "        predictions = dense_layer(batch_fused_features).squeeze()  # [batch_size, 1]\n",
    "        \n",
    "        # Ensure predictions and targets have the right shape\n",
    "        predictions = predictions.view(-1)  # Flatten to [batch_size]\n",
    "        targets = targets.float().view(-1)  # Flatten to [batch_size]\n",
    "\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "        print(\"Targets shape:\", targets.shape)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not isBCELoss:\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        preds = (predictions >= threshold).float()\n",
    "\n",
    "        precision_metric.update(preds.long(), targets.long())\n",
    "        recall_metric.update(preds.long(), targets.long())\n",
    "        f1_metric.update(preds.long(), targets.long())\n",
    "        accuracy_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute metrics\n",
    "    train_precision = precision_metric.compute().item()\n",
    "    train_recall = recall_metric.compute().item()\n",
    "    train_f1_score = f1_metric.compute().item()\n",
    "    train_accuracy = accuracy_metric.compute().item()\n",
    "    train_average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"\\nFinal Metrics:\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1_score:.4f}\")\n",
    "    print(f\"Train Loss: {train_average_loss:.4f}\")\n",
    "\n",
    "    return train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dense_layer, dataloader, criterion, device, output_dir='results/', output_filename='val_predictions.csv'):\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    print(\"-\" * 20, \"Eval\", \"-\" * 20)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imdbids, text_data, audio_data, video_data, targets = batch\n",
    "            \n",
    "            # Move all tensors to device\n",
    "            text_data = text_data.to(device)\n",
    "            audio_data = audio_data.to(device)\n",
    "            video_data = video_data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            print(\"Text data shape:\", text_data.shape)\n",
    "            print(\"Audio data shape:\", audio_data.shape)\n",
    "            print(\"Video data shape:\", video_data.shape)\n",
    "\n",
    "            # Transform text features if needed\n",
    "            linear_transform_text = LinearTransformations(text_data.shape[-1], 768)\n",
    "            text_features = linear_transform_text(text_data)\n",
    "\n",
    "            # Use last sequence of audio features\n",
    "            audio_features = audio_data[:, -1, :]\n",
    "\n",
    "            # Process video features\n",
    "            video_features = pad_features(video_data)\n",
    "\n",
    "            print(\"After transformation:\")\n",
    "            print(\"text_features shape:\", text_features.shape)\n",
    "            print(\"audio_features shape:\", audio_features.shape)\n",
    "            print(\"video_features shape:\", video_features.shape, '\\n')\n",
    "\n",
    "            # Cross attention between modalities\n",
    "            text_video = PairCrossAttention(text_features, video_features)\n",
    "            text_audio = PairCrossAttention(text_features, audio_features)\n",
    "            audio_video = PairCrossAttention(audio_features, video_features)\n",
    "            audio_text = PairCrossAttention(audio_features, text_features)\n",
    "            video_text = PairCrossAttention(video_features, text_features)\n",
    "            video_audio = PairCrossAttention(video_features, audio_features)\n",
    "\n",
    "            print(\"After cross attention:\")\n",
    "            print(\"text_video shape:\", text_video.shape)\n",
    "            print(\"text_audio shape:\", text_audio.shape)\n",
    "            print(\"audio_video shape:\", audio_video.shape)\n",
    "            print(\"audio_text shape:\", audio_text.shape)\n",
    "            print(\"video_text shape:\", video_text.shape)\n",
    "            print(\"video_audio shape:\", video_audio.shape, '\\n')\n",
    "\n",
    "            # Hadamard products\n",
    "            text_combined = HadamardProduct(text_video, text_audio)\n",
    "            audio_combined = HadamardProduct(audio_video, audio_text)\n",
    "            video_combined = HadamardProduct(video_text, video_audio)\n",
    "\n",
    "            print(\"After Hadamard product:\")\n",
    "            print(\"text_combined shape:\", text_combined.shape)\n",
    "            print(\"audio_combined shape:\", audio_combined.shape)\n",
    "            print(\"video_combined shape:\", video_combined.shape, '\\n')\n",
    "        \n",
    "\n",
    "            # Calculate input dimension for embracement layer\n",
    "            d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "            embracement_layer = EmbracementLayer(d_in, d_in).to(device)\n",
    "\n",
    "            # Get final fused features\n",
    "            fused_features = embracement_layer(\n",
    "                video_combined.squeeze(1), \n",
    "                audio_combined.squeeze(1), \n",
    "                text_combined.squeeze(1)\n",
    "            )\n",
    "\n",
    "            # Forward pass through dense layer\n",
    "            predictions = dense_layer(fused_features).squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets.squeeze())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if not isBCELoss:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            # Apply threshold\n",
    "            preds = (predictions > threshold).float()\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "            accuracy_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute metrics\n",
    "    val_precision = precision_metric.compute().item()\n",
    "    val_recall = recall_metric.compute().item()\n",
    "    val_f1_score = f1_metric.compute().item()\n",
    "    val_accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    val_conf_matrix = confusion_matrix(all_targets, np.round(all_predictions))\n",
    "    val_average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Eval Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Eval Precision: {val_precision:.4f}\")\n",
    "    print(f\"Eval Recall: {val_recall:.4f}\")\n",
    "    print(f\"Eval F1 Score: {val_f1_score:.4f}\")\n",
    "    print(f\"Eval Loss: {val_average_loss:.4f}\")\n",
    "\n",
    "    return val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dense_layer, dataloader, criterion, device, output_dir='results/', output_filename='test_predictions.csv'):\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    accuracy_metric = BinaryAccuracy().to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    results = []\n",
    "\n",
    "    print(\"-\" * 20, \"Test\", \"-\" * 20)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imdbids, text_data, audio_data, video_data, targets = batch\n",
    "            \n",
    "            # Move all tensors to device\n",
    "            text_data = text_data.to(device)\n",
    "            audio_data = audio_data.to(device)\n",
    "            video_data = video_data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Transform text features if needed\n",
    "            linear_transform_text = LinearTransformations(text_data.shape[-1], 768)\n",
    "            text_features = linear_transform_text(text_data)\n",
    "\n",
    "            # Use last sequence of audio features\n",
    "            audio_features = audio_data[:, -1, :]\n",
    "\n",
    "            # Process video features\n",
    "            video_features = pad_features(video_data)\n",
    "\n",
    "            # Cross attention between modalities\n",
    "            text_video = PairCrossAttention(text_features, video_features)\n",
    "            text_audio = PairCrossAttention(text_features, audio_features)\n",
    "            audio_video = PairCrossAttention(audio_features, video_features)\n",
    "            audio_text = PairCrossAttention(audio_features, text_features)\n",
    "            video_text = PairCrossAttention(video_features, text_features)\n",
    "            video_audio = PairCrossAttention(video_features, audio_features)\n",
    "\n",
    "            # Hadamard products\n",
    "            text_combined = HadamardProduct(text_video, text_audio)\n",
    "            audio_combined = HadamardProduct(audio_video, audio_text)\n",
    "            video_combined = HadamardProduct(video_text, video_audio)\n",
    "\n",
    "            # Calculate input dimension for embracement layer\n",
    "            d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "            embracement_layer = EmbracementLayer(d_in, d_in).to(device)\n",
    "\n",
    "            # Get final fused features\n",
    "            fused_features = embracement_layer(\n",
    "                video_combined.squeeze(1), \n",
    "                audio_combined.squeeze(1), \n",
    "                text_combined.squeeze(1)\n",
    "            )\n",
    "\n",
    "            # Forward pass through dense layer\n",
    "            predictions = dense_layer(fused_features).squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets.squeeze())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if not isBCELoss:\n",
    "                predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            # Apply threshold\n",
    "            preds = (predictions > threshold).float()\n",
    "            \n",
    "            # Store predictions and targets\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "            # Update metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "            accuracy_metric.update(preds.long(), targets.long())\n",
    "\n",
    "            # Store results for CSV\n",
    "            for imdb_id, pred, target in zip(imdbids, predictions.cpu().numpy(), targets.cpu().numpy()):\n",
    "                results.append({\n",
    "                    'IMDBid': imdb_id,\n",
    "                    'Prediction': pred,\n",
    "                    'Target': target\n",
    "                })\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_precision = precision_metric.compute().item()\n",
    "    test_recall = recall_metric.compute().item()\n",
    "    test_f1_score = f1_metric.compute().item()\n",
    "    test_accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    test_conf_matrix = confusion_matrix(all_targets, np.round(all_predictions))\n",
    "    test_average_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1_score:.4f}\")\n",
    "    print(f\"Test Loss: {test_average_loss:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    results_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "    return test_average_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize list to store fused features for each file\n",
    "# fused_features_list = []\n",
    "# labels_list = []\n",
    "# batch_size = 64  # Adjust based on your available memory\n",
    "\n",
    "# for batch_start in range(0, len(text_features), batch_size):\n",
    "#     batch_end = min(batch_start + batch_size, len(text_features))\n",
    "#     batch_text_features = text_features[batch_start:batch_end]\n",
    "#     batch_audio_features = audio_features[batch_start:batch_end]\n",
    "#     batch_video_features = video_features[batch_start:batch_end]\n",
    "#     print(f\"Batch: {batch_start}\")\n",
    "#     # Loop through all the files in the dataset\n",
    "#     for i in range(len(batch_text_features)):\n",
    "#         # Extract features for the current file\n",
    "#         text_file_name, text_feature = text_features[i]  # Renamed to avoid shadowing\n",
    "#         audio_file_name, audio_feature = audio_features[i]  # Renamed to avoid shadowing\n",
    "#         video_file_name, video_feature = video_features[i]  # Renamed to avoid shadowing\n",
    "\n",
    "#         # print(f\"Processing file {text_file_name}\")\n",
    "\n",
    "#         # Check if any features are missing\n",
    "#         if text_feature is None or audio_feature is None or video_feature is None:\n",
    "#             print(f\"Skipping file {i + 1}/{len(text_features)}: Missing features for {text_file_name}, {audio_file_name}, {video_file_name}\")\n",
    "#             continue  # Skip to the next iteration\n",
    "\n",
    "#         # print(\"Text file name:\", text_file_name)\n",
    "#         # print(\"Audio file name:\", audio_file_name)\n",
    "#         # print(\"Video file name:\", video_file_name)\n",
    "\n",
    "#         print(\"Text features shape:\", text_feature.shape)\n",
    "#         print(\"Audio features shape:\", audio_feature.shape)\n",
    "#         print(\"Video features shape:\", video_feature.shape)\n",
    "\n",
    "#         # Transform text features\n",
    "#         linear_transform_text = LinearTransformations(text_feature.shape[-1], 768)\n",
    "#         text_feature = linear_transform_text(text_feature.unsqueeze(0))  # [1, 768]\n",
    "\n",
    "#         # Transform audio features (use last sequence)\n",
    "#         audio_feature = audio_feature[:, -1, :]  # Shape: [1, 768]\n",
    "\n",
    "#         # Pad video features\n",
    "#         video_feature = video_feature.unsqueeze(0)  # [1, 197, 768]\n",
    "#         video_feature = pad_features(video_feature)\n",
    "#         video_feature = video_feature.squeeze(0)  # [1, 197, 768]\n",
    "        \n",
    "#         print(\"After transformation:\")\n",
    "#         print(\"text_features shape:\", text_feature.shape)\n",
    "#         print(\"audio_features shape:\", audio_feature.shape)\n",
    "#         print(\"video_features shape:\", video_feature.shape, '\\n')\n",
    "\n",
    "#         with torch.no_grad():\n",
    "            \n",
    "#             text_video = PairCrossAttention(text_feature, video_feature)\n",
    "#             text_audio = PairCrossAttention(text_feature, audio_feature)\n",
    "#             audio_video = PairCrossAttention(audio_feature, video_feature)\n",
    "#             audio_text = PairCrossAttention(audio_feature, text_feature)\n",
    "#             video_text = PairCrossAttention(video_feature, text_feature)\n",
    "#             video_audio = PairCrossAttention(video_feature, audio_feature)\n",
    "\n",
    "#             text_combined = HadamardProduct(text_video, text_audio)\n",
    "#             audio_combined = HadamardProduct(audio_video, text_audio)\n",
    "#             video_combined = HadamardProduct(video_text, video_audio)\n",
    "\n",
    "#             print(\"Text-Video Shape:\", text_video.shape)\n",
    "#             print(\"Text-Audio Shape:\", text_audio.shape)\n",
    "#             print(\"Audio-Video Shape:\", audio_video.shape)\n",
    "#             print(\"Audio-Text Shape:\", audio_text.shape)\n",
    "#             print(\"Video-Text Shape:\", video_text.shape)\n",
    "#             print(\"Video-Audio Shape:\", video_audio.shape, '\\n')\n",
    "\n",
    "#             print(\"Text-Combined Shape:\", text_combined.shape)\n",
    "#             print(\"Audio-Combined Shape:\", audio_combined.shape)\n",
    "#             print(\"Video-Combined Shape:\", video_combined.shape, '\\n')\n",
    "\n",
    "#             # Calculate d_in\n",
    "#             d_in = video_combined.shape[-1] + audio_combined.shape[-1] + text_combined.shape[-1]\n",
    "#             embracement_layer = EmbracementLayer(d_in, d_in).to(device)\n",
    "\n",
    "#             video_combined = video_combined.to(device)\n",
    "#             audio_combined = audio_combined.to(device)\n",
    "#             text_combined = text_combined.to(device)\n",
    "\n",
    "#             # Process features\n",
    "#             fused_features = embracement_layer(video_combined[0], audio_combined[0], text_combined[0])            \n",
    "#             print('\\n',\"Fused Features Shape:\", fused_features.shape, '\\n')\n",
    "\n",
    "#             print(\"TV and AV: \", text_video.shape, audio_video.shape, '\\n')\n",
    "\n",
    "#             print(\"TA: \", text_audio.shape, '\\n')\n",
    "\n",
    "#             # Append the fused features and the corresponding label to the lists\n",
    "#             fused_features_list.append(fused_features)\n",
    "#             label = id_label_df.iloc[i]['Label']  # Assuming you have a column 'Label'\n",
    "#             labels_list.append(label)\n",
    "\n",
    "#         del text_feature, audio_feature, video_feature\n",
    "#         del text_video, text_audio, audio_video\n",
    "        \n",
    "# # Stack all fused features into a tensor for training\n",
    "# fused_features_tensor = torch.stack(fused_features_list)\n",
    "\n",
    "# # Convert labels to tensor\n",
    "# label_map = {'red': 1, 'green': 0}  # Adjust if your labels differ\n",
    "# labels_tensor = torch.tensor([label_map[label] for label in labels_list], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20 epochs...\n",
      "Training with learning rate: 1e-05\n",
      "Using BCE Loss\n",
      "Using Dense Layer\n",
      "----------------------------------------\n",
      "Epoch 1/20\n",
      "-------------------- Train --------------------\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Before transformation and padding:\n",
      "Text feature shape: torch.Size([1024])\n",
      "Audio feature shape: torch.Size([1, 197, 768])\n",
      "Video feature shape: torch.Size([149, 768])\n",
      "After transformation and padding:\n",
      "Text feature shape: torch.Size([1, 768])\n",
      "Audio feature shape: torch.Size([1, 768])\n",
      "Video feature shape: torch.Size([1, 197, 768])\n",
      "Video features shape: torch.Size([197, 768])\n",
      "Audio features shape: torch.Size([1, 768])\n",
      "Text features shape: torch.Size([1, 768])\n",
      "Fused features shape: torch.Size([197, 2304])\n",
      "Final fused features shape: torch.Size([32, 197, 2304])\n",
      "Predictions shape: torch.Size([6304])\n",
      "Targets shape: torch.Size([32])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32])) that is different to the input size (torch.Size([6304])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m     32\u001b[0m     dense_layer\u001b[38;5;241m=\u001b[39mdense_layer,\n\u001b[0;32m     33\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     34\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     35\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     36\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_average_loss)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Validation step\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[93], line 106\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(dense_layer, dataloader, criterion, optimizer, device, output_dir, output_filename)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, targets\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets)\n\u001b[0;32m    107\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\SMCA\\fusion_classification\\..\\modules\\classifier.py:26\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, output, target)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, output, target):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Compute and return the binary cross-entropy loss\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbce_loss(output, target)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\nn\\functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3148\u001b[0m     )\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([6304])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Initialize model\n",
    "    if isFinalClassifier:\n",
    "        dense_layer = FinalClassifier(input_size=768*3).to(device)\n",
    "    else:\n",
    "        dense_layer = DenseLayer(input_size=768*3).to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = get_optimizer(dense_layer.parameters(), learning_rate)\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "    print(f\"Using {'BCE Loss' if isBCELoss else 'Custom Loss'}\")\n",
    "    print(f\"Using {'Final Classifier' if isFinalClassifier else 'Dense Layer'}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training step\n",
    "        train_average_loss, train_accuracy, train_precision, train_recall, train_f1_score = train_model(\n",
    "            dense_layer=dense_layer,\n",
    "            dataloader=train_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        train_losses.append(train_average_loss)\n",
    "\n",
    "        # Validation step\n",
    "        val_average_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_conf_matrix = evaluate_model(\n",
    "            dense_layer=dense_layer,\n",
    "            dataloader=val_dataloader,\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "        val_losses.append(val_average_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_average_loss < best_val_loss:\n",
    "            best_val_loss = val_average_loss\n",
    "            best_model_state = dense_layer.state_dict().copy()\n",
    "            print(f\"New best model saved! (Validation Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "        print(f\"Training Loss: {train_average_loss:.4f}, Validation Loss: {val_average_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Load best model for testing\n",
    "    print(\"Loading best model for testing...\")\n",
    "    dense_layer.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_average_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_conf_matrix = test_model(\n",
    "        dense_layer=dense_layer,\n",
    "        dataloader=test_dataloader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Create summary of metrics\n",
    "    metrics_summary = {\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Validation Accuracy\": [val_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Train Precision\": [train_precision],\n",
    "        \"Validation Precision\": [val_precision],\n",
    "        \"Test Precision\": [test_precision],\n",
    "        \"Train Recall\": [train_recall],\n",
    "        \"Validation Recall\": [val_recall],\n",
    "        \"Test Recall\": [test_recall],\n",
    "        \"Train F1 Score\": [train_f1_score],\n",
    "        \"Validation F1 Score\": [val_f1_score],\n",
    "        \"Test F1 Score\": [test_f1_score],\n",
    "        \"Train Loss\": [train_average_loss],\n",
    "        \"Validation Loss\": [val_average_loss],\n",
    "        \"Test Loss\": [test_average_loss]\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_summary)\n",
    "    metrics_df.to_csv('results/metrics_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and validation loss\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange', marker='x')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names=['Negative', 'Positive']):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "plot_confusion_matrix(test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sizes of each DataLoader (FOR CHECKING)\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('results/train_predictions.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('results/val_predictions.csv')\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('results/test_predictions.csv')\n",
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMCA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
