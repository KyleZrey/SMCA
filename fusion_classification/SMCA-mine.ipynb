{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "from modules.cross_attentionb import CrossAttentionB\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss, CustomLoss, BCEWithLogits\n",
    "from modules.linear_transformation import LinearTransformations\n",
    "from modules.output_max import output_max\n",
    "from evaluation_validation.train_model import train_model\n",
    "from evaluation_validation.evaluate_model import evaluate_model\n",
    "from evaluation_validation.test_model import test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Print missing files after checking all\n",
    "        if self.missing_files:\n",
    "            print(\"Missing files:\")\n",
    "            for item in self.missing_files:\n",
    "                print(f\"IMDBid: {item['IMDBid']}\")\n",
    "            print(f\"Total IMDB IDs with missing files: {len(self.missing_files)}\")\n",
    "        else:\n",
    "            print(\"No missing files.\")\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Function\n",
    "def PairCrossAttention(modalityAlpha, modalityBeta, d_out_kq=768, d_out_v=768):\n",
    "    cross_attn = CrossAttentionB(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMCA Functions and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "\n",
    "    # Cross-attention: Alpha -> Beta\n",
    "    alphaBeta = cross_attn(modalityAlpha, modalityBeta)  # Shape: (batch_size, num_queries, d_out_v)\n",
    "\n",
    "    # Cross-attention: Beta -> Alpha\n",
    "    betaAlpha = cross_attn(modalityBeta, modalityAlpha)  # Shape: (batch_size, num_kv, d_out_v)\n",
    "\n",
    "    # Get the sequence lengths\n",
    "    seq_len_alpha = alphaBeta.size(1)  # This is num_queries\n",
    "    seq_len_beta = betaAlpha.size(1)    # This is num_kv\n",
    "\n",
    "    # Instead of expanding, use padding or trimming\n",
    "    max_seq_len = max(seq_len_alpha, seq_len_beta)\n",
    "\n",
    "    # Ensure both alphaBeta and betaAlpha are of shape (batch_size, max_seq_len, d_out_v)\n",
    "    if seq_len_alpha < max_seq_len:\n",
    "        alphaBeta = torch.nn.functional.pad(alphaBeta, (0, 0, 0, max_seq_len - seq_len_alpha), value=0)\n",
    "\n",
    "    if seq_len_beta < max_seq_len:\n",
    "        betaAlpha = torch.nn.functional.pad(betaAlpha, (0, 0, 0, max_seq_len - seq_len_beta), value=0)\n",
    "\n",
    "    # Concatenate cross-attention outputs along the feature dimension (-1)\n",
    "    modalityAlphaBeta = torch.cat((alphaBeta, betaAlpha), dim=-1)  # Shape: (batch_size, max_seq_len, 2 * d_out_v)\n",
    "\n",
    "    return modalityAlphaBeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def SMCAStage2(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v, device):\n",
    "    # modalityAlphaBeta: (batch_size, seq_len, 2 * d_out_v) [output of Stage 1]\n",
    "    \n",
    "    # Initialize the projection layer for modalityAlphaBeta\n",
    "    projection_layer = ProjectionLayer(modalityAlphaBeta.shape[-1], d_out_v).to(device)\n",
    "\n",
    "    # Project modalityAlphaBeta to (batch_size, seq_len, d_out_v)\n",
    "    modalityAlphaBetaProjected = projection_layer(modalityAlphaBeta)\n",
    "\n",
    "    # Initialize the cross-attention module\n",
    "    cross_attn = CrossAttentionB(modalityAlphaBetaProjected.shape[-1], modalityGamma.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "\n",
    "    # Cross-attention: AlphaBeta -> Gamma\n",
    "    alphaBetaGamma = cross_attn(modalityAlphaBetaProjected, modalityGamma)  # Shape: (batch_size, seq_len_alphaBeta, d_out_v)\n",
    "\n",
    "    # Cross-attention: Gamma -> AlphaBeta\n",
    "    gammaAlphaBeta = cross_attn(modalityGamma, modalityAlphaBetaProjected)  # Shape: (batch_size, seq_len_gamma, d_out_v)\n",
    "\n",
    "    # Get the sequence lengths for both modalities\n",
    "    seq_len_alphaBeta = alphaBetaGamma.size(1)\n",
    "    seq_len_gamma = gammaAlphaBeta.size(1)\n",
    "\n",
    "    # Pad the smaller sequence to match the larger one (expanding to before)\n",
    "    max_seq_len = max(seq_len_alphaBeta, seq_len_gamma)\n",
    "\n",
    "    if seq_len_alphaBeta < max_seq_len:\n",
    "        alphaBetaGamma = torch.nn.functional.pad(alphaBetaGamma, (0, 0, 0, max_seq_len - seq_len_alphaBeta), value=0)\n",
    "\n",
    "    if seq_len_gamma < max_seq_len:\n",
    "        gammaAlphaBeta = torch.nn.functional.pad(gammaAlphaBeta, (0, 0, 0, max_seq_len - seq_len_gamma), value=0)\n",
    "\n",
    "    # Concatenate along the feature dimension (-1)\n",
    "    multimodal_representation = torch.cat((alphaBetaGamma, gammaAlphaBeta), dim=-1)  # Shape: (batch_size, max(seq_len_alphaBeta, seq_len_gamma), 2 * d_out_v)\n",
    "\n",
    "    # Apply Global Average Pooling across the feature (sequence to before)\n",
    "    GAP = torch.mean(multimodal_representation, dim=1)  # Shape: (batch_size, 2 * d_out_v)\n",
    "\n",
    "    return GAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModel(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v, device):\n",
    "        super(SMCAModel, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v, self.device)\n",
    "\n",
    "        # Stage 2: Cross attention with modalityAlphaBeta (as query) and modalityGamma (as key-value)\n",
    "        multimodal_representation = SMCAStage2(modalityAlphaBeta, modalityGamma, self.d_out_kq, self.d_out_v, self.device)\n",
    " \n",
    "       \n",
    "        return multimodal_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on one instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected File Names:\n",
      "Audio file: C:\\Users\\edjin\\OneDrive\\Documents\\Programming Files\\Thesis\\SMCA\\misc\\audio_fe\\logmel_spectrograms\\feature_tt0042767.npy\n",
      "Video file: C:\\Users\\edjin\\OneDrive\\Documents\\Programming Files\\Thesis\\SMCA\\misc\\visualStream_ViT\\feature_vectors\\tt0042767_features.npy\n",
      "Text file: C:\\Users\\edjin\\OneDrive\\Documents\\Programming Files\\Thesis\\SMCA\\misc\\textStream_BERT\\feature_vectors\\feature_vectors\\tt0042767.npy\n",
      "Audio:  torch.Size([1, 197, 768])\n",
      "Text:  torch.Size([1, 768])\n",
      "Video:  torch.Size([1, 113, 768])\n",
      "Stage 2: torch.Size([1, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SMCAModel(768, 768, device) # Dimension for d_out_kq and d_out_v\n",
    "\n",
    "# Select the first file from each modality directories (for testing)\n",
    "video_file_name, video_feature = video_features[5]\n",
    "audio_file_name, audio_feature = audio_features[5]\n",
    "text_file_name, text_feature = text_features[5]\n",
    "\n",
    "# Print the file names\n",
    "print(\"\\nSelected File Names:\")\n",
    "print(\"Audio file:\", audio_file_name)\n",
    "print(\"Video file:\", video_file_name)\n",
    "print(\"Text file:\", text_file_name)\n",
    "\n",
    "video_feature = video_feature.unsqueeze(0)  # Add batch dimension\n",
    "text_feature = text_feature.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "modalityAlpha=audio_feature.to(device) \n",
    "modalityBeta=text_feature.to(device)\n",
    "modalityGamma=video_feature.to(device)\n",
    "\n",
    "# Apply linear transformation to match dimensions\n",
    "linear_transform_Alpha = LinearTransformations(modalityAlpha.shape[-1], 768).to(device)\n",
    "linear_transform_Beta = LinearTransformations(modalityBeta.shape[-1], 768).to(device)\n",
    "linear_transform_Gamma = LinearTransformations(modalityGamma.shape[-1], 768).to(device)\n",
    "\n",
    "modalityAlpha = linear_transform_Alpha(modalityAlpha).to(device)\n",
    "modalityBeta = linear_transform_Beta(modalityBeta).to(device)\n",
    "modalityGamma = linear_transform_Gamma(modalityGamma).to(device)\n",
    "\n",
    "print(\"Audio: \",modalityAlpha.shape)\n",
    "print(\"Text: \",modalityBeta.shape)\n",
    "print(\"Video: \",modalityGamma.shape)\n",
    "\n",
    "outputs = model(modalityAlpha, modalityBeta, modalityGamma)\n",
    "\n",
    "print(\"Stage 2:\", outputs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n",
      "------------------------------\n",
      "Missing files:\n",
      "IMDBid: tt2494280\n",
      "IMDBid: tt1724962\n",
      "IMDBid: tt1152836\n",
      "IMDBid: tt0389790\n",
      "IMDBid: tt3053228\n",
      "IMDBid: tt1045778\n",
      "IMDBid: tt1758795\n",
      "IMDBid: tt0099385\n",
      "IMDBid: tt2917484\n",
      "IMDBid: tt4769836\n",
      "IMDBid: tt0089652\n",
      "IMDBid: tt0465494\n",
      "IMDBid: tt3675748\n",
      "IMDBid: tt2126362\n",
      "IMDBid: tt0988083\n",
      "IMDBid: tt2101341\n",
      "IMDBid: tt0401997\n",
      "IMDBid: tt1661461\n",
      "IMDBid: tt1313139\n",
      "IMDBid: tt1094661\n",
      "IMDBid: tt5162658\n",
      "IMDBid: tt0104839\n",
      "IMDBid: tt1288558\n",
      "IMDBid: tt5962210\n",
      "IMDBid: tt2937696\n",
      "IMDBid: tt0284363\n",
      "IMDBid: tt5580390\n",
      "IMDBid: tt2293750\n",
      "IMDBid: tt2980472\n",
      "IMDBid: tt0082186\n",
      "IMDBid: tt0924129\n",
      "IMDBid: tt0988595\n",
      "IMDBid: tt1349482\n",
      "IMDBid: tt4158096\n",
      "IMDBid: tt1403241\n",
      "IMDBid: tt2713642\n",
      "IMDBid: tt1682940\n",
      "IMDBid: tt10327354\n",
      "IMDBid: tt1087842\n",
      "IMDBid: tt1800302\n",
      "IMDBid: tt0113855\n",
      "IMDBid: tt2504022\n",
      "IMDBid: tt7248248\n",
      "IMDBid: tt1720164\n",
      "IMDBid: tt1336621\n",
      "IMDBid: tt0266987\n",
      "IMDBid: tt0859635\n",
      "Total IMDB IDs with missing files: 47\n",
      "Missing files:\n",
      "IMDBid: tt2437712\n",
      "IMDBid: tt0099371\n",
      "IMDBid: tt2935564\n",
      "IMDBid: tt0140336\n",
      "IMDBid: tt4687276\n",
      "IMDBid: tt0367085\n",
      "IMDBid: tt0220827\n",
      "IMDBid: tt0465602\n",
      "IMDBid: tt2350496\n",
      "IMDBid: tt1084950\n",
      "IMDBid: tt0110093\n",
      "IMDBid: tt1273235\n",
      "IMDBid: tt4503510\n",
      "IMDBid: tt1772925\n",
      "IMDBid: tt2180411\n",
      "IMDBid: tt0181865\n",
      "IMDBid: tt0200469\n",
      "IMDBid: tt0259288\n",
      "Total IMDB IDs with missing files: 18\n",
      "Missing files:\n",
      "IMDBid: tt0190590\n",
      "IMDBid: tt2383068\n",
      "IMDBid: tt1488555\n",
      "IMDBid: tt0758730\n",
      "IMDBid: tt0389557\n",
      "IMDBid: tt0498353\n",
      "IMDBid: tt5084170\n",
      "IMDBid: tt0405052\n",
      "IMDBid: tt1104733\n",
      "IMDBid: tt1924429\n",
      "IMDBid: tt5027774\n",
      "IMDBid: tt0990413\n",
      "IMDBid: tt5580036\n",
      "IMDBid: tt2524674\n",
      "Total IMDB IDs with missing files: 14\n",
      "Missing files:\n",
      "IMDBid: tt0988595\n",
      "IMDBid: tt1724962\n",
      "IMDBid: tt0758730\n",
      "IMDBid: tt0200469\n",
      "IMDBid: tt0389790\n",
      "IMDBid: tt4503510\n",
      "IMDBid: tt0401997\n",
      "IMDBid: tt1349482\n",
      "IMDBid: tt0082186\n",
      "IMDBid: tt1094661\n",
      "IMDBid: tt0924129\n",
      "IMDBid: tt2437712\n",
      "IMDBid: tt2917484\n",
      "IMDBid: tt3053228\n",
      "IMDBid: tt0099371\n",
      "IMDBid: tt2101341\n",
      "IMDBid: tt0099385\n",
      "IMDBid: tt0259288\n",
      "IMDBid: tt1336621\n",
      "IMDBid: tt1087842\n",
      "IMDBid: tt2937696\n",
      "IMDBid: tt1288558\n",
      "IMDBid: tt2524674\n",
      "IMDBid: tt4158096\n",
      "IMDBid: tt3675748\n",
      "IMDBid: tt1800302\n",
      "IMDBid: tt1104733\n",
      "IMDBid: tt0465494\n",
      "IMDBid: tt0498353\n",
      "IMDBid: tt0110093\n",
      "IMDBid: tt5580036\n",
      "IMDBid: tt5962210\n",
      "IMDBid: tt2180411\n",
      "IMDBid: tt0405052\n",
      "IMDBid: tt2713642\n",
      "IMDBid: tt1772925\n",
      "IMDBid: tt2980472\n",
      "IMDBid: tt0988083\n",
      "IMDBid: tt2935564\n",
      "IMDBid: tt0140336\n",
      "IMDBid: tt7248248\n",
      "IMDBid: tt0104839\n",
      "IMDBid: tt1720164\n",
      "IMDBid: tt0113855\n",
      "IMDBid: tt5084170\n",
      "IMDBid: tt0089652\n",
      "IMDBid: tt2504022\n",
      "IMDBid: tt0220827\n",
      "IMDBid: tt0190590\n",
      "IMDBid: tt0284363\n",
      "IMDBid: tt5162658\n",
      "IMDBid: tt1682940\n",
      "IMDBid: tt10327354\n",
      "IMDBid: tt1152836\n",
      "IMDBid: tt1084950\n",
      "IMDBid: tt4687276\n",
      "IMDBid: tt2293750\n",
      "IMDBid: tt0465602\n",
      "IMDBid: tt0367085\n",
      "IMDBid: tt0266987\n",
      "IMDBid: tt1273235\n",
      "IMDBid: tt2126362\n",
      "IMDBid: tt2494280\n",
      "IMDBid: tt0990413\n",
      "IMDBid: tt0859635\n",
      "IMDBid: tt1488555\n",
      "IMDBid: tt4769836\n",
      "IMDBid: tt2350496\n",
      "IMDBid: tt1313139\n",
      "IMDBid: tt2383068\n",
      "IMDBid: tt5580390\n",
      "IMDBid: tt1758795\n",
      "IMDBid: tt5027774\n",
      "IMDBid: tt0181865\n",
      "IMDBid: tt1924429\n",
      "IMDBid: tt1661461\n",
      "IMDBid: tt1403241\n",
      "IMDBid: tt1045778\n",
      "IMDBid: tt0389557\n",
      "Total IMDB IDs with missing files: 79\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\textStream_BERT\\\\feature_vectors\\\\feature_vectors'\n",
    "audio_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\audio_fe\\\\logmel_spectrograms'\n",
    "video_features_dir = 'C:\\\\Users\\\\edjin\\\\OneDrive\\\\Documents\\\\Programming Files\\\\Thesis\\\\SMCA\\\\misc\\\\visualStream_ViT\\\\feature_vectors'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "id_label_df = id_label_df.drop(columns=['Movie Title', 'URL'])\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: []\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 169, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 169, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 164, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 164, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 162, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 162, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 147, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 147, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 141, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 141, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 177, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 177, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 149, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 149, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 181, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 181, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 142, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 142, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 281, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 281, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 200, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 200, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 144, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 144, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 185, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 185, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 169, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 169, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 156, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 156, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 185, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 185, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 159, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 159, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 155, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 155, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 189, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 189, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 146, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 146, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 169, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 169, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 191, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 191, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 143, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 143, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 191, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 191, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 145, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 145, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 148, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 148, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 166, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 166, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 160, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 160, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 173, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 173, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 162, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 162, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 155, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 155, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 143, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 143, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 144, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 144, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 147, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 147, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 204, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 204, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 167, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 167, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 162, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 162, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 143, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 143, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 244, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 244, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 227, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 227, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 142, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 142, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 302, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 302, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 172, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 172, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 166, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 166, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 167, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 167, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 162, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 162, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 169, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 169, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 154, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 154, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 524, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 524, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 171, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 171, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 174, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 174, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 175, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 175, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 172, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 172, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 159, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 159, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 143, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 143, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 188, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 188, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 149, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 149, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 153, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 153, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([16, 1024])\n",
      "Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Video Dimension:  torch.Size([16, 559, 768])\n",
      "Transformed Audio Dimension:  torch.Size([16, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([16, 768])\n",
      "Transformed Video Dimension:  torch.Size([16, 559, 768])\n",
      "Stage 2: torch.Size([16, 1536])\n",
      "--------\n",
      "Text Dimension:  torch.Size([11, 1024])\n",
      "Audio Dimension:  torch.Size([11, 197, 768])\n",
      "Video Dimension:  torch.Size([11, 131, 768])\n",
      "Transformed Audio Dimension:  torch.Size([11, 197, 768])\n",
      "Transformed Text Dimension:  torch.Size([11, 768])\n",
      "Transformed Video Dimension:  torch.Size([11, 131, 768])\n",
      "Stage 2: torch.Size([11, 1536])\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = SMCAModel(768, 768, device)  # Dimension for d_out_kq and d_out_v\n",
    "\n",
    "print(f\"Model parameters: {list(model.parameters())}\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for text_features, audio_features, video_features, targets in train_dataloader:\n",
    "    # Move features to the specified device\n",
    "    text_features = text_features.to(device)\n",
    "    audio_features = audio_features.to(device)\n",
    "    video_features = video_features.to(device)\n",
    "\n",
    "    # Squeeze the audio features to remove the extra dimension\n",
    "    audio_features = audio_features.squeeze(1) \n",
    "\n",
    "    # Print dimensions for debugging\n",
    "    print(\"Text Dimension: \", text_features.shape)  \n",
    "    print(\"Audio Dimension: \", audio_features.shape)  \n",
    "    print(\"Video Dimension: \", video_features.shape) \n",
    "\n",
    "    # Apply linear transformations to match dimensions\n",
    "    linear_transform_Alpha = LinearTransformations(audio_features.shape[-1], 768).to(device) \n",
    "    linear_transform_Beta = LinearTransformations(text_features.shape[-1], 768).to(device)   \n",
    "    linear_transform_Gamma = LinearTransformations(video_features.shape[-1], 768).to(device)    \n",
    "\n",
    "    # Transform features to match the target dimension\n",
    "    modalityAlpha = linear_transform_Alpha(audio_features).to(device)  \n",
    "    modalityBeta = linear_transform_Beta(text_features).to(device)    \n",
    "    modalityGamma = linear_transform_Gamma(video_features).to(device)\n",
    "\n",
    "    # Print shapes after transformation to verify the batch dimension\n",
    "    print(\"Transformed Audio Dimension: \", modalityAlpha.shape)  \n",
    "    print(\"Transformed Text Dimension: \", modalityBeta.shape)    \n",
    "    print(\"Transformed Video Dimension: \", modalityGamma.shape)  \n",
    "\n",
    "    # Pass inputs through the SMCA model\n",
    "    outputs = model(\n",
    "        modalityAlpha=modalityAlpha,  # Ensure to pass transformed modalities\n",
    "        modalityBeta=modalityBeta,\n",
    "        modalityGamma=modalityGamma,\n",
    "    )\n",
    "\n",
    "    print(\"Stage 2:\", outputs.shape)  # Check the output shape\n",
    "    print(\"--------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dense_layer, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    dense_layer.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for text_features, audio_features, video_features, targets in dataloader:\n",
    "        text_features, audio_features, video_features, targets = (\n",
    "            text_features.to(device),\n",
    "            audio_features.to(device),\n",
    "            video_features.to(device),\n",
    "            targets.to(device).view(-1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass inputs through SMCA model\n",
    "        # Squeeze the audio features to remove the extra dimension\n",
    "        audio_features = audio_features.squeeze(1) \n",
    "\n",
    "        # Apply linear transformations to match dimensions\n",
    "        linear_transform_Alpha = LinearTransformations(audio_features.shape[-1], 768).to(device)   \n",
    "        linear_transform_Beta = LinearTransformations(text_features.shape[-1], 768).to(device)      \n",
    "        linear_transform_Gamma = LinearTransformations(video_features.shape[-1], 768).to(device)       \n",
    "\n",
    "        # Transform features to match the target dimension\n",
    "        modalityAlpha = linear_transform_Alpha(audio_features).to(device)     \n",
    "        modalityBeta = linear_transform_Beta(text_features).to(device)       \n",
    "        modalityGamma = linear_transform_Gamma(video_features).to(device)   \n",
    "        \n",
    "        outputs = model(\n",
    "            modalityAlpha=modalityAlpha,  # Ensure to pass transformed modalities\n",
    "            modalityBeta=modalityBeta,\n",
    "            modalityGamma=modalityGamma,\n",
    "        )\n",
    "\n",
    "        # Pass the fused features through the dense layer\n",
    "        predictions = dense_layer(outputs).view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    f1_metric.reset()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Pass inputs through SMCA model\n",
    "            # Squeeze the audio features to remove the extra dimension\n",
    "            audio_features = audio_features.squeeze(1) \n",
    "\n",
    "            # Apply linear transformations to match dimensions\n",
    "            linear_transform_Alpha = LinearTransformations(audio_features.shape[-1], 768).to(device)    \n",
    "            linear_transform_Beta = LinearTransformations(text_features.shape[-1], 768).to(device)      \n",
    "            linear_transform_Gamma = LinearTransformations(video_features.shape[-1], 768).to(device)       \n",
    "\n",
    "            # Transform features to match the target dimension\n",
    "            modalityAlpha = linear_transform_Alpha(audio_features).to(device)     \n",
    "            modalityBeta = linear_transform_Beta(text_features).to(device)      \n",
    "            modalityGamma = linear_transform_Gamma(video_features).to(device)   \n",
    "            \n",
    "            outputs = model(modalityAlpha=modalityAlpha, modalityBeta=modalityBeta, modalityGamma=modalityGamma)\n",
    "\n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1) \n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return average_loss, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dense_layer, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    dense_layer.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize the metrics for binary classification\n",
    "    precision_metric = BinaryPrecision().to(device)\n",
    "    recall_metric = BinaryRecall().to(device)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            text_features, audio_features, video_features, targets = (\n",
    "                text_features.to(device),\n",
    "                audio_features.to(device),\n",
    "                video_features.to(device),\n",
    "                targets.to(device).view(-1)\n",
    "            )\n",
    "            \n",
    "            # Pass inputs through SMCA model\n",
    "            # Squeeze the audio features to remove the extra dimension\n",
    "            audio_features = audio_features.squeeze(1) \n",
    "\n",
    "            # Apply linear transformations to match dimensions\n",
    "            linear_transform_Alpha = LinearTransformations(audio_features.shape[-1], 768).to(device)    \n",
    "            linear_transform_Beta = LinearTransformations(text_features.shape[-1], 768).to(device)      \n",
    "            linear_transform_Gamma = LinearTransformations(video_features.shape[-1], 768).to(device)       \n",
    "\n",
    "            # Transform features to match the target dimension\n",
    "            modalityAlpha = linear_transform_Alpha(audio_features).to(device)     \n",
    "            modalityBeta = linear_transform_Beta(text_features).to(device)       \n",
    "            modalityGamma = linear_transform_Gamma(video_features).to(device)   \n",
    "            \n",
    "            outputs = model(modalityAlpha=modalityAlpha, modalityBeta=modalityBeta, modalityGamma=modalityGamma)\n",
    "            \n",
    "            # Pass the fused features through the dense layer\n",
    "            predictions = dense_layer(outputs).view(-1)  \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (predictions > 0.5).float()\n",
    "            \n",
    "            # Update the precision, recall, and F1 score metrics\n",
    "            precision_metric.update(preds.long(), targets.long())\n",
    "            recall_metric.update(preds.long(), targets.long())\n",
    "            f1_metric.update(preds.long(), targets.long())\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = precision_metric.compute().item()\n",
    "    recall = recall_metric.compute().item()\n",
    "    f1_score = f1_metric.compute().item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    return average_loss, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-3):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "------------------------------\n",
      "Epoch 1/10\n",
      "Precision: 0.1111\n",
      "Recall: 0.0192\n",
      "F1 Score: 0.0328\n",
      "Training Loss: 0.6867, Validation Loss: 0.6828\n",
      "------------------------------\n",
      "Epoch 2/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6756, Validation Loss: 0.6671\n",
      "------------------------------\n",
      "Epoch 3/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6621, Validation Loss: 0.6585\n",
      "------------------------------\n",
      "Epoch 4/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6514, Validation Loss: 0.6505\n",
      "------------------------------\n",
      "Epoch 5/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6434, Validation Loss: 0.6431\n",
      "------------------------------\n",
      "Epoch 6/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6346, Validation Loss: 0.6410\n",
      "------------------------------\n",
      "Epoch 7/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6239, Validation Loss: 0.6256\n",
      "------------------------------\n",
      "Epoch 8/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6192, Validation Loss: 0.6194\n",
      "------------------------------\n",
      "Epoch 9/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6116, Validation Loss: 0.6148\n",
      "------------------------------\n",
      "Epoch 10/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Training Loss: 0.6085, Validation Loss: 0.6086\n",
      "------------------------------\n",
      "Testing the model on the test set...\n",
      "Test Loss: 0.6220\n",
      "Test Precision: 0.0000\n",
      "Test Recall: 0.0000\n",
      "Test F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModel(768, 768, device)  # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=output_dim*2).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.grad is None:\n",
    "            print(\"No gradient for:\", param)\n",
    "    optimizer = get_optimizer(list(model.parameters()) + list(dense_layer.parameters()))\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "\n",
    "        # Validate step\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Testing the model\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(\n",
    "    dataset, \n",
    "    model_class, \n",
    "    dense_layer_class, \n",
    "    num_folds, \n",
    "    num_epochs, \n",
    "    batch_size, \n",
    "    learning_rate,\n",
    "    device=None\n",
    "):\n",
    "    # Set device configuration\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the KFold splitter\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # lists to store metrics for each fold\n",
    "    fold_losses = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1_scores = []\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = BCELoss()\n",
    "\n",
    "    # Perform K-Fold Cross-Validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # Create data loaders for the train and validation sets\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "        val_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=collate_fn)\n",
    "        \n",
    "        # Initialize the model and dense layer for each fold\n",
    "        model = model_class(768, 768, device).to(device)\n",
    "        dense_layer = dense_layer_class(input_size=output_dim*2).to(device)\n",
    "        \n",
    "        # Initialize the optimizer\n",
    "        optimizer = get_optimizer(list(model.parameters()) + list(dense_layer.parameters()), lr=learning_rate)\n",
    "\n",
    "        # Training loop for each fold\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Train and evaluate the model on the training and validation sets\n",
    "            train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "            val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "        \n",
    "        # Store the validation metrics for this fold\n",
    "        fold_losses.append(val_loss)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_recalls.append(recall)\n",
    "        fold_f1_scores.append(f1_score)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "    avg_precision = np.mean(fold_precisions)\n",
    "    avg_recall = np.mean(fold_recalls)\n",
    "    avg_f1_score = np.mean(fold_f1_scores)\n",
    "\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n",
    "\n",
    "    # Return the average metrics for further analysis or logging\n",
    "    return {\n",
    "        \"average_loss\": avg_loss,\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1_score\": avg_f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Fold 1/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2778\n",
      "Recall: 0.2941\n",
      "F1 Score: 0.2857\n",
      "Train Loss: 0.8482, Validation Loss: 0.8291\n",
      "Validation Precision: 0.2778, Recall: 0.2941, F1 Score: 0.2857\n",
      "Epoch 2/10\n",
      "Precision: 0.3438\n",
      "Recall: 0.3235\n",
      "F1 Score: 0.3333\n",
      "Train Loss: 0.8482, Validation Loss: 0.8549\n",
      "Validation Precision: 0.3438, Recall: 0.3235, F1 Score: 0.3333\n",
      "Epoch 3/10\n",
      "Precision: 0.1250\n",
      "Recall: 0.0294\n",
      "F1 Score: 0.0476\n",
      "Train Loss: 0.8483, Validation Loss: 0.8564\n",
      "Validation Precision: 0.1250, Recall: 0.0294, F1 Score: 0.0476\n",
      "Epoch 4/10\n",
      "Precision: 0.5714\n",
      "Recall: 0.1176\n",
      "F1 Score: 0.1951\n",
      "Train Loss: 0.8477, Validation Loss: 0.8278\n",
      "Validation Precision: 0.5714, Recall: 0.1176, F1 Score: 0.1951\n",
      "Epoch 5/10\n",
      "Precision: 0.2857\n",
      "Recall: 0.0588\n",
      "F1 Score: 0.0976\n",
      "Train Loss: 0.8477, Validation Loss: 0.8469\n",
      "Validation Precision: 0.2857, Recall: 0.0588, F1 Score: 0.0976\n",
      "Epoch 6/10\n",
      "Precision: 0.3889\n",
      "Recall: 0.2059\n",
      "F1 Score: 0.2692\n",
      "Train Loss: 0.8486, Validation Loss: 0.8466\n",
      "Validation Precision: 0.3889, Recall: 0.2059, F1 Score: 0.2692\n",
      "Epoch 7/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8473, Validation Loss: 0.8199\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 8/10\n",
      "Precision: 0.5714\n",
      "Recall: 0.1176\n",
      "F1 Score: 0.1951\n",
      "Train Loss: 0.8484, Validation Loss: 0.8473\n",
      "Validation Precision: 0.5714, Recall: 0.1176, F1 Score: 0.1951\n",
      "Epoch 9/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8355, Validation Loss: 0.8377\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 10/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8485, Validation Loss: 0.8282\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "\n",
      "Fold 2/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2083\n",
      "Recall: 0.8929\n",
      "F1 Score: 0.3378\n",
      "Train Loss: 0.8361, Validation Loss: 0.8870\n",
      "Validation Precision: 0.2083, Recall: 0.8929, F1 Score: 0.3378\n",
      "Epoch 2/10\n",
      "Precision: 0.2241\n",
      "Recall: 0.9286\n",
      "F1 Score: 0.3611\n",
      "Train Loss: 0.8490, Validation Loss: 0.8685\n",
      "Validation Precision: 0.2241, Recall: 0.9286, F1 Score: 0.3611\n",
      "Epoch 3/10\n",
      "Precision: 0.2222\n",
      "Recall: 0.5714\n",
      "F1 Score: 0.3200\n",
      "Train Loss: 0.8358, Validation Loss: 0.8775\n",
      "Validation Precision: 0.2222, Recall: 0.5714, F1 Score: 0.3200\n",
      "Epoch 4/10\n",
      "Precision: 0.1786\n",
      "Recall: 0.7143\n",
      "F1 Score: 0.2857\n",
      "Train Loss: 0.8487, Validation Loss: 0.8880\n",
      "Validation Precision: 0.1786, Recall: 0.7143, F1 Score: 0.2857\n",
      "Epoch 5/10\n",
      "Precision: 0.2237\n",
      "Recall: 0.6071\n",
      "F1 Score: 0.3269\n",
      "Train Loss: 0.8361, Validation Loss: 0.8770\n",
      "Validation Precision: 0.2237, Recall: 0.6071, F1 Score: 0.3269\n",
      "Epoch 6/10\n",
      "Precision: 0.2198\n",
      "Recall: 0.7143\n",
      "F1 Score: 0.3361\n",
      "Train Loss: 0.8485, Validation Loss: 0.8768\n",
      "Validation Precision: 0.2198, Recall: 0.7143, F1 Score: 0.3361\n",
      "Epoch 7/10\n",
      "Precision: 0.1341\n",
      "Recall: 0.3929\n",
      "F1 Score: 0.2000\n",
      "Train Loss: 0.8482, Validation Loss: 0.8787\n",
      "Validation Precision: 0.1341, Recall: 0.3929, F1 Score: 0.2000\n",
      "Epoch 8/10\n",
      "Precision: 0.2405\n",
      "Recall: 0.6786\n",
      "F1 Score: 0.3551\n",
      "Train Loss: 0.8482, Validation Loss: 0.8685\n",
      "Validation Precision: 0.2405, Recall: 0.6786, F1 Score: 0.3551\n",
      "Epoch 9/10\n",
      "Precision: 0.0656\n",
      "Recall: 0.1429\n",
      "F1 Score: 0.0899\n",
      "Train Loss: 0.8481, Validation Loss: 0.8672\n",
      "Validation Precision: 0.0656, Recall: 0.1429, F1 Score: 0.0899\n",
      "Epoch 10/10\n",
      "Precision: 0.1613\n",
      "Recall: 0.5357\n",
      "F1 Score: 0.2479\n",
      "Train Loss: 0.8479, Validation Loss: 0.8780\n",
      "Validation Precision: 0.1613, Recall: 0.5357, F1 Score: 0.2479\n",
      "\n",
      "Fold 3/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2642\n",
      "Recall: 0.8235\n",
      "F1 Score: 0.4000\n",
      "Train Loss: 0.8500, Validation Loss: 0.8408\n",
      "Validation Precision: 0.2642, Recall: 0.8235, F1 Score: 0.4000\n",
      "Epoch 2/10\n",
      "Precision: 0.2578\n",
      "Recall: 0.9706\n",
      "F1 Score: 0.4074\n",
      "Train Loss: 0.8508, Validation Loss: 0.8599\n",
      "Validation Precision: 0.2578, Recall: 0.9706, F1 Score: 0.4074\n",
      "Epoch 3/10\n",
      "Precision: 0.2315\n",
      "Recall: 0.7353\n",
      "F1 Score: 0.3521\n",
      "Train Loss: 0.8506, Validation Loss: 0.8411\n",
      "Validation Precision: 0.2315, Recall: 0.7353, F1 Score: 0.3521\n",
      "Epoch 4/10\n",
      "Precision: 0.2427\n",
      "Recall: 0.7353\n",
      "F1 Score: 0.3650\n",
      "Train Loss: 0.8503, Validation Loss: 0.8509\n",
      "Validation Precision: 0.2427, Recall: 0.7353, F1 Score: 0.3650\n",
      "Epoch 5/10\n",
      "Precision: 0.2286\n",
      "Recall: 0.2353\n",
      "F1 Score: 0.2319\n",
      "Train Loss: 0.8506, Validation Loss: 0.8300\n",
      "Validation Precision: 0.2286, Recall: 0.2353, F1 Score: 0.2319\n",
      "Epoch 6/10\n",
      "Precision: 0.3021\n",
      "Recall: 0.8529\n",
      "F1 Score: 0.4462\n",
      "Train Loss: 0.8383, Validation Loss: 0.8675\n",
      "Validation Precision: 0.3021, Recall: 0.8529, F1 Score: 0.4462\n",
      "Epoch 7/10\n",
      "Precision: 0.2727\n",
      "Recall: 0.2647\n",
      "F1 Score: 0.2687\n",
      "Train Loss: 0.8385, Validation Loss: 0.8390\n",
      "Validation Precision: 0.2727, Recall: 0.2647, F1 Score: 0.2687\n",
      "Epoch 8/10\n",
      "Precision: 0.2308\n",
      "Recall: 0.7059\n",
      "F1 Score: 0.3478\n",
      "Train Loss: 0.8506, Validation Loss: 0.8592\n",
      "Validation Precision: 0.2308, Recall: 0.7059, F1 Score: 0.3478\n",
      "Epoch 9/10\n",
      "Precision: 0.2523\n",
      "Recall: 0.7941\n",
      "F1 Score: 0.3830\n",
      "Train Loss: 0.8508, Validation Loss: 0.8595\n",
      "Validation Precision: 0.2523, Recall: 0.7941, F1 Score: 0.3830\n",
      "Epoch 10/10\n",
      "Precision: 0.2561\n",
      "Recall: 0.6176\n",
      "F1 Score: 0.3621\n",
      "Train Loss: 0.8383, Validation Loss: 0.8306\n",
      "Validation Precision: 0.2561, Recall: 0.6176, F1 Score: 0.3621\n",
      "\n",
      "Fold 4/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2619\n",
      "Recall: 0.2973\n",
      "F1 Score: 0.2785\n",
      "Train Loss: 0.8528, Validation Loss: 0.8243\n",
      "Validation Precision: 0.2619, Recall: 0.2973, F1 Score: 0.2785\n",
      "Epoch 2/10\n",
      "Precision: 0.2479\n",
      "Recall: 0.7838\n",
      "F1 Score: 0.3766\n",
      "Train Loss: 0.8533, Validation Loss: 0.8259\n",
      "Validation Precision: 0.2479, Recall: 0.7838, F1 Score: 0.3766\n",
      "Epoch 3/10\n",
      "Precision: 0.2913\n",
      "Recall: 0.8108\n",
      "F1 Score: 0.4286\n",
      "Train Loss: 0.8527, Validation Loss: 0.8489\n",
      "Validation Precision: 0.2913, Recall: 0.8108, F1 Score: 0.4286\n",
      "Epoch 4/10\n",
      "Precision: 0.2805\n",
      "Recall: 0.6216\n",
      "F1 Score: 0.3866\n",
      "Train Loss: 0.8468, Validation Loss: 0.8256\n",
      "Validation Precision: 0.2805, Recall: 0.6216, F1 Score: 0.3866\n",
      "Epoch 5/10\n",
      "Precision: 0.2817\n",
      "Recall: 0.5405\n",
      "F1 Score: 0.3704\n",
      "Train Loss: 0.8528, Validation Loss: 0.8475\n",
      "Validation Precision: 0.2817, Recall: 0.5405, F1 Score: 0.3704\n",
      "Epoch 6/10\n",
      "Precision: 0.2727\n",
      "Recall: 0.9730\n",
      "F1 Score: 0.4260\n",
      "Train Loss: 0.8456, Validation Loss: 0.8486\n",
      "Validation Precision: 0.2727, Recall: 0.9730, F1 Score: 0.4260\n",
      "Epoch 7/10\n",
      "Precision: 0.2935\n",
      "Recall: 0.7297\n",
      "F1 Score: 0.4186\n",
      "Train Loss: 0.8460, Validation Loss: 0.8367\n",
      "Validation Precision: 0.2935, Recall: 0.7297, F1 Score: 0.4186\n",
      "Epoch 8/10\n",
      "Precision: 0.2857\n",
      "Recall: 0.8649\n",
      "F1 Score: 0.4295\n",
      "Train Loss: 0.8469, Validation Loss: 0.8488\n",
      "Validation Precision: 0.2857, Recall: 0.8649, F1 Score: 0.4295\n",
      "Epoch 9/10\n",
      "Precision: 0.2727\n",
      "Recall: 0.9730\n",
      "F1 Score: 0.4260\n",
      "Train Loss: 0.8464, Validation Loss: 0.8498\n",
      "Validation Precision: 0.2727, Recall: 0.9730, F1 Score: 0.4260\n",
      "Epoch 10/10\n",
      "Precision: 0.3165\n",
      "Recall: 0.6757\n",
      "F1 Score: 0.4310\n",
      "Train Loss: 0.8467, Validation Loss: 0.8480\n",
      "Validation Precision: 0.3165, Recall: 0.6757, F1 Score: 0.4310\n",
      "\n",
      "Fold 5/10\n",
      "Epoch 1/10\n",
      "Precision: 0.1875\n",
      "Recall: 0.1765\n",
      "F1 Score: 0.1818\n",
      "Train Loss: 0.8481, Validation Loss: 0.8433\n",
      "Validation Precision: 0.1875, Recall: 0.1765, F1 Score: 0.1818\n",
      "Epoch 2/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8417, Validation Loss: 0.8541\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 3/10\n",
      "Precision: 0.5000\n",
      "Recall: 0.1471\n",
      "F1 Score: 0.2273\n",
      "Train Loss: 0.8478, Validation Loss: 0.8209\n",
      "Validation Precision: 0.5000, Recall: 0.1471, F1 Score: 0.2273\n",
      "Epoch 4/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8475, Validation Loss: 0.8430\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 5/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8479, Validation Loss: 0.8423\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 6/10\n",
      "Precision: 0.2174\n",
      "Recall: 0.1471\n",
      "F1 Score: 0.1754\n",
      "Train Loss: 0.8419, Validation Loss: 0.8324\n",
      "Validation Precision: 0.2174, Recall: 0.1471, F1 Score: 0.1754\n",
      "Epoch 7/10\n",
      "Precision: 0.1905\n",
      "Recall: 0.1176\n",
      "F1 Score: 0.1455\n",
      "Train Loss: 0.8426, Validation Loss: 0.8322\n",
      "Validation Precision: 0.1905, Recall: 0.1176, F1 Score: 0.1455\n",
      "Epoch 8/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8419, Validation Loss: 0.8644\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 9/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8412, Validation Loss: 0.8305\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 10/10\n",
      "Precision: 0.2821\n",
      "Recall: 0.3235\n",
      "F1 Score: 0.3014\n",
      "Train Loss: 0.8417, Validation Loss: 0.8658\n",
      "Validation Precision: 0.2821, Recall: 0.3235, F1 Score: 0.3014\n",
      "\n",
      "Fold 6/10\n",
      "Epoch 1/10\n",
      "Precision: 0.1321\n",
      "Recall: 0.2258\n",
      "F1 Score: 0.1667\n",
      "Train Loss: 0.8485, Validation Loss: 0.8548\n",
      "Validation Precision: 0.1321, Recall: 0.2258, F1 Score: 0.1667\n",
      "Epoch 2/10\n",
      "Precision: 0.2297\n",
      "Recall: 0.5484\n",
      "F1 Score: 0.3238\n",
      "Train Loss: 0.8428, Validation Loss: 0.8648\n",
      "Validation Precision: 0.2297, Recall: 0.5484, F1 Score: 0.3238\n",
      "Epoch 3/10\n",
      "Precision: 0.2360\n",
      "Recall: 0.6774\n",
      "F1 Score: 0.3500\n",
      "Train Loss: 0.8480, Validation Loss: 0.8442\n",
      "Validation Precision: 0.2360, Recall: 0.6774, F1 Score: 0.3500\n",
      "Epoch 4/10\n",
      "Precision: 0.2500\n",
      "Recall: 0.4516\n",
      "F1 Score: 0.3218\n",
      "Train Loss: 0.8483, Validation Loss: 0.8537\n",
      "Validation Precision: 0.2500, Recall: 0.4516, F1 Score: 0.3218\n",
      "Epoch 5/10\n",
      "Precision: 0.2121\n",
      "Recall: 0.2258\n",
      "F1 Score: 0.2188\n",
      "Train Loss: 0.8481, Validation Loss: 0.8426\n",
      "Validation Precision: 0.2121, Recall: 0.2258, F1 Score: 0.2188\n",
      "Epoch 6/10\n",
      "Precision: 0.2308\n",
      "Recall: 0.2903\n",
      "F1 Score: 0.2571\n",
      "Train Loss: 0.8482, Validation Loss: 0.8535\n",
      "Validation Precision: 0.2308, Recall: 0.2903, F1 Score: 0.2571\n",
      "Epoch 7/10\n",
      "Precision: 0.2424\n",
      "Recall: 0.2581\n",
      "F1 Score: 0.2500\n",
      "Train Loss: 0.8483, Validation Loss: 0.8428\n",
      "Validation Precision: 0.2424, Recall: 0.2581, F1 Score: 0.2500\n",
      "Epoch 8/10\n",
      "Precision: 0.4000\n",
      "Recall: 0.0645\n",
      "F1 Score: 0.1111\n",
      "Train Loss: 0.8481, Validation Loss: 0.8512\n",
      "Validation Precision: 0.4000, Recall: 0.0645, F1 Score: 0.1111\n",
      "Epoch 9/10\n",
      "Precision: 0.2131\n",
      "Recall: 0.4194\n",
      "F1 Score: 0.2826\n",
      "Train Loss: 0.8482, Validation Loss: 0.8435\n",
      "Validation Precision: 0.2131, Recall: 0.4194, F1 Score: 0.2826\n",
      "Epoch 10/10\n",
      "Precision: 0.3077\n",
      "Recall: 0.6452\n",
      "F1 Score: 0.4167\n",
      "Train Loss: 0.8483, Validation Loss: 0.8651\n",
      "Validation Precision: 0.3077, Recall: 0.6452, F1 Score: 0.4167\n",
      "\n",
      "Fold 7/10\n",
      "Epoch 1/10\n",
      "Precision: 0.1290\n",
      "Recall: 0.1379\n",
      "F1 Score: 0.1333\n",
      "Train Loss: 0.8465, Validation Loss: 0.8477\n",
      "Validation Precision: 0.1290, Recall: 0.1379, F1 Score: 0.1333\n",
      "Epoch 2/10\n",
      "Precision: 0.2308\n",
      "Recall: 0.1034\n",
      "F1 Score: 0.1429\n",
      "Train Loss: 0.8406, Validation Loss: 0.8699\n",
      "Validation Precision: 0.2308, Recall: 0.1034, F1 Score: 0.1429\n",
      "Epoch 3/10\n",
      "Precision: 0.3214\n",
      "Recall: 0.3103\n",
      "F1 Score: 0.3158\n",
      "Train Loss: 0.8457, Validation Loss: 0.8598\n",
      "Validation Precision: 0.3214, Recall: 0.3103, F1 Score: 0.3158\n",
      "Epoch 4/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8463, Validation Loss: 0.8693\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 5/10\n",
      "Precision: 0.1667\n",
      "Recall: 0.1724\n",
      "F1 Score: 0.1695\n",
      "Train Loss: 0.8462, Validation Loss: 0.8692\n",
      "Validation Precision: 0.1667, Recall: 0.1724, F1 Score: 0.1695\n",
      "Epoch 6/10\n",
      "Precision: 0.2500\n",
      "Recall: 0.0345\n",
      "F1 Score: 0.0606\n",
      "Train Loss: 0.8404, Validation Loss: 0.8704\n",
      "Validation Precision: 0.2500, Recall: 0.0345, F1 Score: 0.0606\n",
      "Epoch 7/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8464, Validation Loss: 0.8697\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 8/10\n",
      "Precision: 0.1613\n",
      "Recall: 0.1724\n",
      "F1 Score: 0.1667\n",
      "Train Loss: 0.8462, Validation Loss: 0.8706\n",
      "Validation Precision: 0.1613, Recall: 0.1724, F1 Score: 0.1667\n",
      "Epoch 9/10\n",
      "Precision: 0.1739\n",
      "Recall: 0.1379\n",
      "F1 Score: 0.1538\n",
      "Train Loss: 0.8458, Validation Loss: 0.8360\n",
      "Validation Precision: 0.1739, Recall: 0.1379, F1 Score: 0.1538\n",
      "Epoch 10/10\n",
      "Precision: 0.2344\n",
      "Recall: 0.5172\n",
      "F1 Score: 0.3226\n",
      "Train Loss: 0.8396, Validation Loss: 0.8597\n",
      "Validation Precision: 0.2344, Recall: 0.5172, F1 Score: 0.3226\n",
      "\n",
      "Fold 8/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2615\n",
      "Recall: 0.3864\n",
      "F1 Score: 0.3119\n",
      "Train Loss: 0.8518, Validation Loss: 0.8032\n",
      "Validation Precision: 0.2615, Recall: 0.3864, F1 Score: 0.3119\n",
      "Epoch 2/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8460, Validation Loss: 0.8234\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 3/10\n",
      "Precision: 0.3667\n",
      "Recall: 0.2500\n",
      "F1 Score: 0.2973\n",
      "Train Loss: 0.8521, Validation Loss: 0.8014\n",
      "Validation Precision: 0.3667, Recall: 0.2500, F1 Score: 0.2973\n",
      "Epoch 4/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8464, Validation Loss: 0.8014\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 5/10\n",
      "Precision: 0.1500\n",
      "Recall: 0.0682\n",
      "F1 Score: 0.0938\n",
      "Train Loss: 0.8457, Validation Loss: 0.8130\n",
      "Validation Precision: 0.1500, Recall: 0.0682, F1 Score: 0.0938\n",
      "Epoch 6/10\n",
      "Precision: 1.0000\n",
      "Recall: 0.0682\n",
      "F1 Score: 0.1277\n",
      "Train Loss: 0.8516, Validation Loss: 0.8234\n",
      "Validation Precision: 1.0000, Recall: 0.0682, F1 Score: 0.1277\n",
      "Epoch 7/10\n",
      "Precision: 0.2432\n",
      "Recall: 0.2045\n",
      "F1 Score: 0.2222\n",
      "Train Loss: 0.8514, Validation Loss: 0.8024\n",
      "Validation Precision: 0.2432, Recall: 0.2045, F1 Score: 0.2222\n",
      "Epoch 8/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8456, Validation Loss: 0.8338\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 9/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8514, Validation Loss: 0.7896\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 10/10\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Train Loss: 0.8517, Validation Loss: 0.8230\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "\n",
      "Fold 9/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2680\n",
      "Recall: 0.7222\n",
      "F1 Score: 0.3910\n",
      "Train Loss: 0.8527, Validation Loss: 0.8516\n",
      "Validation Precision: 0.2680, Recall: 0.7222, F1 Score: 0.3910\n",
      "Epoch 2/10\n",
      "Precision: 0.2667\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4211\n",
      "Train Loss: 0.8531, Validation Loss: 0.8532\n",
      "Validation Precision: 0.2667, Recall: 1.0000, F1 Score: 0.4211\n",
      "Epoch 3/10\n",
      "Precision: 0.2458\n",
      "Recall: 0.8056\n",
      "F1 Score: 0.3766\n",
      "Train Loss: 0.8527, Validation Loss: 0.8293\n",
      "Validation Precision: 0.2458, Recall: 0.8056, F1 Score: 0.3766\n",
      "Epoch 4/10\n",
      "Precision: 0.3107\n",
      "Recall: 0.8889\n",
      "F1 Score: 0.4604\n",
      "Train Loss: 0.8468, Validation Loss: 0.8292\n",
      "Validation Precision: 0.3107, Recall: 0.8889, F1 Score: 0.4604\n",
      "Epoch 5/10\n",
      "Precision: 0.2783\n",
      "Recall: 0.8889\n",
      "F1 Score: 0.4238\n",
      "Train Loss: 0.8459, Validation Loss: 0.8409\n",
      "Validation Precision: 0.2783, Recall: 0.8889, F1 Score: 0.4238\n",
      "Epoch 6/10\n",
      "Precision: 0.2817\n",
      "Recall: 0.5556\n",
      "F1 Score: 0.3738\n",
      "Train Loss: 0.8464, Validation Loss: 0.8402\n",
      "Validation Precision: 0.2817, Recall: 0.5556, F1 Score: 0.3738\n",
      "Epoch 7/10\n",
      "Precision: 0.2623\n",
      "Recall: 0.8889\n",
      "F1 Score: 0.4051\n",
      "Train Loss: 0.8526, Validation Loss: 0.8514\n",
      "Validation Precision: 0.2623, Recall: 0.8889, F1 Score: 0.4051\n",
      "Epoch 8/10\n",
      "Precision: 0.2787\n",
      "Recall: 0.9444\n",
      "F1 Score: 0.4304\n",
      "Train Loss: 0.8525, Validation Loss: 0.8621\n",
      "Validation Precision: 0.2787, Recall: 0.9444, F1 Score: 0.4304\n",
      "Epoch 9/10\n",
      "Precision: 0.2881\n",
      "Recall: 0.9444\n",
      "F1 Score: 0.4416\n",
      "Train Loss: 0.8524, Validation Loss: 0.8402\n",
      "Validation Precision: 0.2881, Recall: 0.9444, F1 Score: 0.4416\n",
      "Epoch 10/10\n",
      "Precision: 0.2612\n",
      "Recall: 0.9722\n",
      "F1 Score: 0.4118\n",
      "Train Loss: 0.8524, Validation Loss: 0.8522\n",
      "Validation Precision: 0.2612, Recall: 0.9722, F1 Score: 0.4118\n",
      "\n",
      "Fold 10/10\n",
      "Epoch 1/10\n",
      "Precision: 0.2500\n",
      "Recall: 0.1667\n",
      "F1 Score: 0.2000\n",
      "Train Loss: 0.8443, Validation Loss: 0.8497\n",
      "Validation Precision: 0.2500, Recall: 0.1667, F1 Score: 0.2000\n",
      "Epoch 2/10\n",
      "Precision: 0.3235\n",
      "Recall: 0.3056\n",
      "F1 Score: 0.3143\n",
      "Train Loss: 0.8503, Validation Loss: 0.8600\n",
      "Validation Precision: 0.3235, Recall: 0.3056, F1 Score: 0.3143\n",
      "Epoch 3/10\n",
      "Precision: 0.2564\n",
      "Recall: 0.2778\n",
      "F1 Score: 0.2667\n",
      "Train Loss: 0.8441, Validation Loss: 0.8388\n",
      "Validation Precision: 0.2564, Recall: 0.2778, F1 Score: 0.2667\n",
      "Epoch 4/10\n",
      "Precision: 0.2143\n",
      "Recall: 0.0833\n",
      "F1 Score: 0.1200\n",
      "Train Loss: 0.8499, Validation Loss: 0.8156\n",
      "Validation Precision: 0.2143, Recall: 0.0833, F1 Score: 0.1200\n",
      "Epoch 5/10\n",
      "Precision: 0.3571\n",
      "Recall: 0.4167\n",
      "F1 Score: 0.3846\n",
      "Train Loss: 0.8503, Validation Loss: 0.8277\n",
      "Validation Precision: 0.3571, Recall: 0.4167, F1 Score: 0.3846\n",
      "Epoch 6/10\n",
      "Precision: 0.7143\n",
      "Recall: 0.1389\n",
      "F1 Score: 0.2326\n",
      "Train Loss: 0.8502, Validation Loss: 0.8032\n",
      "Validation Precision: 0.7143, Recall: 0.1389, F1 Score: 0.2326\n",
      "Epoch 7/10\n",
      "Precision: 0.2247\n",
      "Recall: 0.5556\n",
      "F1 Score: 0.3200\n",
      "Train Loss: 0.8439, Validation Loss: 0.8169\n",
      "Validation Precision: 0.2247, Recall: 0.5556, F1 Score: 0.3200\n",
      "Epoch 8/10\n",
      "Precision: 0.1667\n",
      "Recall: 0.1667\n",
      "F1 Score: 0.1667\n",
      "Train Loss: 0.8443, Validation Loss: 0.8167\n",
      "Validation Precision: 0.1667, Recall: 0.1667, F1 Score: 0.1667\n",
      "Epoch 9/10\n",
      "Precision: 0.3404\n",
      "Recall: 0.4444\n",
      "F1 Score: 0.3855\n",
      "Train Loss: 0.8443, Validation Loss: 0.8494\n",
      "Validation Precision: 0.3404, Recall: 0.4444, F1 Score: 0.3855\n",
      "Epoch 10/10\n",
      "Precision: 0.1892\n",
      "Recall: 0.1944\n",
      "F1 Score: 0.1918\n",
      "Train Loss: 0.8385, Validation Loss: 0.8492\n",
      "Validation Precision: 0.1892, Recall: 0.1944, F1 Score: 0.1918\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Loss: 0.8500\n",
      "Average Precision: 0.2008\n",
      "Average Recall: 0.4482\n",
      "Average F1 Score: 0.2685\n"
     ]
    }
   ],
   "source": [
    "# Run k-fold cross-validation   \n",
    "results = cross_validate_model(\n",
    "    dataset=full_dataset,\n",
    "    model_class=SMCAModel,\n",
    "    dense_layer_class=DenseLayer,\n",
    "    num_folds=10,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-5,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
