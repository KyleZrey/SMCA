{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "\n",
    "from modules.cross_attentionb import CrossAttentionB\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss\n",
    "from modules.linear_transformation import LinearTransformations\n",
    "from evaluation_validation.train_model import train_model\n",
    "from evaluation_validation.evaluate_model import evaluate_model\n",
    "from evaluation_validation.test_model import test_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features):\n",
    "        self.id_label_df = id_label_df\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_files = []\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_files.append(idx)\n",
    "            else:\n",
    "                self.missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # # Print missing files after checking all\n",
    "        # if self.missing_files:\n",
    "        #     print(\"Missing files:\")\n",
    "        #     for item in self.missing_files:\n",
    "        #         print(f\"IMDBid: {item['IMDBid']}\")\n",
    "        #     print(f\"Total IMDB IDs with missing files: {len(self.missing_files)}\")\n",
    "        # else:\n",
    "        #     print(\"No missing files.\")\n",
    "\n",
    "        return valid_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 0, 'green': 1} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)  # Ensure labels are integers\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "        \n",
    "        # Debugging output\n",
    "        if label_data.shape[0] == 0:\n",
    "            print(f\"Empty target for IMDBid {imdbid} at index {idx}\")\n",
    "\n",
    "        return text_data, audio_data, video_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    video_lengths = [v.size(0) for v in video_data]\n",
    "    max_length = max(video_lengths)\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Text'\n",
    "audio_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Audio'\n",
    "video_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Video'\n",
    "\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Splitting data for training, validation, and testing\n",
    "train_df, val_test_df = train_test_split(id_label_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Combine all data for K-fold cross-validation\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMCA Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 1 of SMCA\n",
    "def SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(modalityAlpha.shape[-1], modalityBeta.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    modalityAlphaBeta = cross_attn(modalityAlpha, modalityBeta)\n",
    "    return modalityAlphaBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Stage 2 of SMCA - Model A: Stage 1 Output as Query\n",
    "def SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(modalityAlphaBeta.shape[-1], modalityGamma.shape[-1], d_out_kq, d_out_v).to(device)\n",
    "    multimodal_representation = cross_attn(modalityAlphaBeta, modalityGamma)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelA(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v, device):\n",
    "        super(SMCAModelA, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma, device):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityAlphaBeta (as query) and modalityGamma (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Flatten the output\n",
    "        return torch.flatten(multimodal_representation, start_dim=1)  # Flatten all dimensions except batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v, device):\n",
    "    cross_attn = CrossAttentionB(\n",
    "        d_in_query=modalityGamma.shape[-1],\n",
    "        d_in_kv=modalityAlphaBeta.shape[-1],\n",
    "        d_out_kq=d_out_kq,\n",
    "        d_out_v=d_out_v\n",
    "    ).to(device)\n",
    "    multimodal_representation = cross_attn(modalityGamma, modalityAlphaBeta)\n",
    "    return multimodal_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMCAModelB(nn.Module):\n",
    "    def __init__(self, d_out_kq, d_out_v, device):\n",
    "        super(SMCAModelB, self).__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.d_out_v = d_out_v\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, modalityAlpha, modalityBeta, modalityGamma, device):\n",
    "        # Stage 1: Cross attention between modalityAlpha and modalityBeta\n",
    "        modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Stage 2: Cross attention with modalityGamma (as query) and modalityAlphaBeta (as key-value)\n",
    "        multimodal_representation = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, self.d_out_kq, self.d_out_v, device)\n",
    "        \n",
    "        # Flatten the output\n",
    "        return torch.flatten(multimodal_representation, start_dim=1)  # Flatten all dimensions except batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model (For Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "--------------------------------------------------\n",
      "SMCA Output Shape: torch.Size([8, 417792])\n"
     ]
    }
   ],
   "source": [
    "# Test the SMCA model using the items from dataloader as input\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Determine the output dimensions\n",
    "d_out_kq = 512\n",
    "d_out_v = 256\n",
    "\n",
    "# Initialize the SMCA model A\n",
    "model = SMCAModelB(d_out_kq, d_out_v, device)\n",
    "model.to(device)\n",
    "\n",
    "# Use DataLoader to get a batch of data\n",
    "for batch in train_dataloader:  # You can use any DataLoader (train_dataloader, val_dataloader, etc.)\n",
    "    text_data, audio_data, video_data, labels = batch\n",
    "    \n",
    "    text_data = text_data.to(device)\n",
    "    audio_data = audio_data.to(device)\n",
    "    video_data = video_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Feed the entire batch to the GMU model\n",
    "    with torch.no_grad():\n",
    "        output = model(modalityAlpha=audio_data, modalityBeta=text_data, modalityGamma=video_data, device=device)\n",
    "        \n",
    "    # Print the output shape\n",
    "    print('-'*50)\n",
    "    print(\"SMCA Output Shape:\", output.shape)    \n",
    "    # Break after the first batch for testing purposes\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File:\n",
      "Text file: tt0021814.npy\n",
      "Audio file: feature_tt0021814.npy\n",
      "Video file: tt0021814_features.npy\n",
      "--------------------------------------------------\n",
      "Text Feature Shape: torch.Size([1, 1024])\n",
      "Audio Feature Shape: torch.Size([1, 197, 768])\n",
      "Video Feature Shape: torch.Size([1, 95, 768])\n",
      "--------------------------------------------------\n",
      "Model output shape: torch.Size([1, 50432]) ###[batch_size, output_dim]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the SMCA model using a single file from each feature directory\n",
    "\n",
    "from modules.dataloader import load_npy_files\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Text'\n",
    "audio_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Audio'\n",
    "video_features_dir = 'D:\\\\Projects\\\\Thesis\\\\Video'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Select the first file from each modality directories (for testing)\n",
    "text_file_name, text_features = text_features[index]\n",
    "audio_file_name, audio_features = audio_features[index]\n",
    "video_file_name, video_features = video_features[index]\n",
    "\n",
    "print(\"Selected File:\")\n",
    "print(\"Text file:\", os.path.basename(text_file_name))\n",
    "print(\"Audio file:\", os.path.basename(audio_file_name))\n",
    "print(\"Video file:\", os.path.basename(video_file_name))\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Determine the output dimensions\n",
    "d_out_kq = 512 \n",
    "d_out_v = 256\n",
    "\n",
    "# Initialize the SMCA model A\n",
    "model = SMCAModelB(d_out_kq, d_out_v, device)\n",
    "\n",
    "# Move model to the same device as your data (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Reshape features\n",
    "video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "print(\"Text Feature Shape:\", text_features.shape)\n",
    "print(\"Audio Feature Shape:\", audio_features.shape)\n",
    "print(\"Video Feature Shape:\", video_features.shape)\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Pass the data through the SMCA model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    output = model(modalityAlpha=text_features.to(device), modalityBeta=video_features.to(device), modalityGamma=audio_features.to(device), device=device)\n",
    "\n",
    "# Print the output shape and the output itself\n",
    "print(\"Model output shape:\", output.shape, \"###[batch_size, output_dim]\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected File Names:\n",
      "Audio file: D:\\\\Projects\\\\Thesis\\\\Audio\\feature_tt0021814.npy\n",
      "Video file: D:\\\\Projects\\\\Thesis\\\\Video\\tt0021814_features.npy\n",
      "Text file: D:\\\\Projects\\\\Thesis\\\\Text\\tt0021814.npy\n",
      "Modality Alpha Shape: torch.Size([1, 197, 768])\n",
      "Modality Beta Shape: torch.Size([1, 768])\n",
      "Modality Gamma Shape: torch.Size([1, 95, 768])\n",
      "Stage 1 Bimodal Representation Shape: torch.Size([1, 197, 768])\n",
      "Final Multimodal Representation (Model A) Shape: torch.Size([1, 197, 768])\n",
      "Final Multimodal Representation (Model B) Shape: torch.Size([1, 95, 768])\n"
     ]
    }
   ],
   "source": [
    "# Original Sample Test for the SMCA  (Non-Classes)\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load .npy files\n",
    "    video_features = load_npy_files(r'D:\\\\Projects\\\\Thesis\\\\Video')\n",
    "    audio_features = load_npy_files(r'D:\\\\Projects\\\\Thesis\\\\Audio')\n",
    "    text_features = load_npy_files(r'D:\\\\Projects\\\\Thesis\\\\Text')\n",
    "    \n",
    "    # Select the first file from each modality directories (for testing)\n",
    "    video_file_name, video_features = video_features[0]\n",
    "    audio_file_name, audio_features = audio_features[0]\n",
    "    text_file_name, text_features = text_features[0]\n",
    "\n",
    "    # Print the file names\n",
    "    print(\"\\nSelected File Names:\")\n",
    "    print(\"Audio file:\", audio_file_name)\n",
    "    print(\"Video file:\", video_file_name)\n",
    "    print(\"Text file:\", text_file_name)\n",
    "    \n",
    "    # Reshape features\n",
    "    video_features = video_features.unsqueeze(0)  # Add batch dimension\n",
    "    text_features = text_features.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "    # # Randomize assignment of Alpha, Beta, Gamma\n",
    "    # modalityAlpha, modalityBeta, modalityGamma = randomize_modalities(audio_features, video_features, text_features)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Manual assignment of modalities\n",
    "    modalityAlpha = audio_features.to(device)\n",
    "    modalityBeta = text_features.to(device)\n",
    "    modalityGamma = video_features.to(device)\n",
    "\n",
    "    # Apply linear transformation to match dimensions\n",
    "    linear_transform_Alpha = LinearTransformations(modalityAlpha.shape[-1], 768).to(device)\n",
    "    linear_transform_Beta = LinearTransformations(modalityBeta.shape[-1], 768).to(device)\n",
    "    linear_transform_Gamma = LinearTransformations(modalityGamma.shape[-1], 768).to(device)\n",
    "\n",
    "    modalityAlpha = linear_transform_Alpha(modalityAlpha)\n",
    "    modalityBeta = linear_transform_Beta(modalityBeta)\n",
    "    modalityGamma = linear_transform_Gamma(modalityGamma)\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    d_out_kq = 768  # Final transformed dimension\n",
    "    d_out_v = 768\n",
    "\n",
    "    # Stage 1: Bimodal Representation\n",
    "    modalityAlphaBeta = SMCAStage1(modalityAlpha, modalityBeta, d_out_kq, d_out_v, device)\n",
    "    \n",
    "    # Stage 2, Model A: Multimodal Representation (using AlphaBeta as Query)\n",
    "    final_representation_A = SMCAStage2_ModelA(modalityAlphaBeta, modalityGamma, d_out_kq, d_out_v, device)\n",
    "    \n",
    "    # Stage 2, Model B: Multimodal Representation (using AlphaBeta as Key-Value)\n",
    "    final_representation_B = SMCAStage2_ModelB(modalityGamma, modalityAlphaBeta, d_out_kq, d_out_v, device)\n",
    "\n",
    "    print(\"Modality Alpha Shape:\", modalityAlpha.shape)\n",
    "    print(\"Modality Beta Shape:\", modalityBeta.shape)\n",
    "    print(\"Modality Gamma Shape:\", modalityGamma.shape)\n",
    "    print(\"Stage 1 Bimodal Representation Shape:\", modalityAlphaBeta.shape)\n",
    "    print(\"Final Multimodal Representation (Model A) Shape:\", final_representation_A.shape)\n",
    "    print(\"Final Multimodal Representation (Model B) Shape:\", final_representation_B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, lr=1e-4):\n",
    "    # Create an optimizer, for example, Adam\n",
    "    return optim.Adam(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest output size from the SMCA model\n",
    "def find_largest_output_size(model, dataloader, device):\n",
    "    max_output_size = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text_features, audio_features, video_features, targets in dataloader:\n",
    "            # Move features to device\n",
    "            text_features = text_features.to(device)\n",
    "            audio_features = audio_features.to(device)\n",
    "            video_features = video_features.to(device)\n",
    "\n",
    "            # Pass inputs through the SMCA model\n",
    "            outputs = model(\n",
    "                modalityAlpha=audio_features, \n",
    "                modalityBeta=text_features, \n",
    "                modalityGamma=video_features,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # Compare and store the maximum output size\n",
    "            if outputs.size(1) > max_output_size:\n",
    "                max_output_size = outputs.size(1)\n",
    "\n",
    "    return max_output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/10\n",
      "Evaluation Loss: 0.6982\n",
      "Precision: 0.7188\n",
      "Recall: 0.1586\n",
      "F1 Score: 0.2599\n",
      "Training Loss: 0.6995, Validation Loss: 0.6982\n",
      "------------------------------\n",
      "Epoch 2/10\n",
      "Evaluation Loss: 0.6959\n",
      "Precision: 0.8438\n",
      "Recall: 0.3724\n",
      "F1 Score: 0.5167\n",
      "Training Loss: 0.6983, Validation Loss: 0.6959\n",
      "------------------------------\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Ensure you have a dataloader that yields inputs and targets\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model(model\u001b[38;5;241m=\u001b[39mmodel, dense_layer\u001b[38;5;241m=\u001b[39mdense_layer, dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader, criterion\u001b[38;5;241m=\u001b[39mcriterion, optimizer\u001b[38;5;241m=\u001b[39moptimizer, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Validate model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m val_loss, precision, recall, f1_score \u001b[38;5;241m=\u001b[39m evaluate_model(model\u001b[38;5;241m=\u001b[39mmodel, dense_layer\u001b[38;5;241m=\u001b[39mdense_layer, dataloader\u001b[38;5;241m=\u001b[39mval_dataloader, criterion\u001b[38;5;241m=\u001b[39mcriterion, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\Projects\\SMCA\\fusion_classification\\SMCA\\../..\\evaluation_validation\\train_model.py:9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dense_layer, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m dense_layer\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_features, audio_features, video_features, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     10\u001b[0m     text_features, audio_features, video_features, targets \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     11\u001b[0m         text_features\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     12\u001b[0m         audio_features\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     13\u001b[0m         video_features\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     14\u001b[0m         targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:627\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\autograd\\profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 622\u001b[0m         torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit\u001b[38;5;241m.\u001b[39m_RecordFunction(record)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\Chy\\miniconda3\\envs\\smca\\Lib\\site-packages\\torch\\_ops.py:594\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelA(512, 256, device) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = find_largest_output_size(model, train_dataloader, device)\n",
    "    max_output_size_val = find_largest_output_size(model, val_dataloader, device)\n",
    "    max_output_size_test = find_largest_output_size(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=512).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(list(dense_layer.parameters())+ list(model.parameters()))  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Determine the output dimensions\n",
    "    output_dim = 768\n",
    "\n",
    "    # Initialize the SMCA model A\n",
    "    model = SMCAModelB(512, 256, device) # Dimension for d_out_kq and d_out_v\n",
    "    model.to(device)  # Move the model to the correct device\n",
    "\n",
    "\n",
    "    # Loop through the dataloaders to determine the largest output size\n",
    "    max_output_size_train = find_largest_output_size(model, train_dataloader, device)\n",
    "    max_output_size_val = find_largest_output_size(model, val_dataloader, device)\n",
    "    max_output_size_test = find_largest_output_size(model, test_dataloader, device)\n",
    "\n",
    "    # Get the overall largest output size\n",
    "    max_output_size = max(max_output_size_train, max_output_size_val, max_output_size_test)\n",
    "\n",
    "    # Initialize the DenseLayer with the largest output size\n",
    "    dense_layer = DenseLayer(input_size=512).to(device)  # Initialize and move to the correct device\n",
    "\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BCELoss()  # Use appropriate loss function\n",
    "    optimizer = get_optimizer(dense_layer.parameters())  # Pass only the dense layer parameters\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Set the number of epochs you want to train for\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_model(model=model, dense_layer=dense_layer, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Validate model\n",
    "        val_loss, precision, recall, f1_score = evaluate_model(model=model, dense_layer=dense_layer, dataloader=val_dataloader, criterion=criterion, device=device)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Testing the model\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_model(model=model, dense_layer=dense_layer, dataloader=test_dataloader, criterion=criterion, device=device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
