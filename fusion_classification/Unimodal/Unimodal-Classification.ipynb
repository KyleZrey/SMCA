{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torcheval.metrics import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "\n",
    "from modules.cross_attentionb import CrossAttentionB\n",
    "from modules.dataloader import load_npy_files\n",
    "from modules.classifier import DenseLayer, BCELoss, CustomLoss, BCEWithLogits\n",
    "from modules.linear_transformation import LinearTransformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, id_label_df, text_features, audio_features, video_features, lower_bound=35, upper_bound=197):\n",
    "        self.id_label_df = id_label_df\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        \n",
    "        # Convert feature lists to dictionaries for fast lookup\n",
    "        self.text_features = {os.path.basename(file).split('.')[0]: tensor for file, tensor in text_features}\n",
    "        self.audio_features = {os.path.basename(file).split('_')[1].split('.')[0]: tensor for file, tensor in audio_features}\n",
    "        self.video_features = {os.path.basename(file).split('_')[0]: tensor for file, tensor in video_features}\n",
    "\n",
    "        # Find and remove outliers\n",
    "        self.id_label_df, self.text_features, self.audio_features, self.video_features = self.remove_outliers()\n",
    "\n",
    "        # List to store missing files\n",
    "        self.missing_files = []\n",
    "\n",
    "        # Filter out entries with missing files\n",
    "        self.valid_files = self._filter_valid_files()\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        # Identify outliers based on video sequence length\n",
    "        outlier_ids = []\n",
    "        \n",
    "        for imdbid, video_tensor in self.video_features.items():\n",
    "            seq_length = video_tensor.size(0)\n",
    "            \n",
    "            # Check if the sequence length is an outlier\n",
    "            if seq_length < self.lower_bound or seq_length > self.upper_bound:\n",
    "                outlier_ids.append(imdbid)\n",
    "        \n",
    "        # Filter out outliers from the DataFrame and features\n",
    "        self.id_label_df = self.id_label_df[~self.id_label_df['IMDBid'].isin(outlier_ids)].reset_index(drop=True)\n",
    "        self.text_features = {k: v for k, v in self.text_features.items() if k not in outlier_ids}\n",
    "        self.audio_features = {k: v for k, v in self.audio_features.items() if k not in outlier_ids}\n",
    "        self.video_features = {k: v for k, v in self.video_features.items() if k not in outlier_ids}\n",
    "\n",
    "        return self.id_label_df, self.text_features, self.audio_features, self.video_features\n",
    "\n",
    "    def _filter_valid_files(self):\n",
    "        valid_indices = []\n",
    "        missing_files = []\n",
    "\n",
    "        for idx in range(len(self.id_label_df)):\n",
    "            imdbid = self.id_label_df.iloc[idx]['IMDBid']\n",
    "\n",
    "            # Check if the IMDBid exists in each modality's features\n",
    "            if imdbid in self.text_features and imdbid in self.audio_features and imdbid in self.video_features:\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_files.append({'IMDBid': imdbid})\n",
    "\n",
    "        # Filter id_label_df to only include valid rows\n",
    "        self.id_label_df = self.id_label_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        self.missing_files = missing_files\n",
    "        \n",
    "        # Update valid_indices to reflect the new indices after resetting\n",
    "        valid_indices = list(range(len(self.id_label_df)))\n",
    "\n",
    "        # Return valid indices\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original index from the filtered valid files\n",
    "        original_idx = self.valid_files[idx]\n",
    "        imdbid = self.id_label_df.iloc[original_idx]['IMDBid']\n",
    "        label = self.id_label_df.iloc[original_idx]['Label']\n",
    "\n",
    "        # Retrieve data from the loaded features\n",
    "        text_data = self.text_features.get(imdbid, torch.zeros((1024,)))\n",
    "        audio_data = self.audio_features.get(imdbid, torch.zeros((1, 197, 768)))\n",
    "        video_data = self.video_features.get(imdbid, torch.zeros((95, 768)))\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'red': 1, 'green': 0} \n",
    "        \n",
    "        # Convert labels to tensor using label_map\n",
    "        try:\n",
    "            label_data = torch.tensor([label_map[label]], dtype=torch.float32)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Label '{e}' not found in label_map.\")\n",
    "            raise\n",
    "\n",
    "        return imdbid, text_data, audio_data, video_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unpack batch elements\n",
    "    imdbids, text_data, audio_data, video_data, label_data = zip(*batch)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    text_data = torch.stack(text_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "\n",
    "    # Padding for video data\n",
    "    # Determine maximum length of video sequences in the batch\n",
    "    max_length = 197\n",
    "\n",
    "    # Pad video sequences to the maximum length\n",
    "    video_data_padded = torch.stack([\n",
    "        F.pad(v, (0, 0, 0, max_length - v.size(0)), \"constant\", 0)\n",
    "        for v in video_data\n",
    "    ])\n",
    "\n",
    "    # Convert labels to tensor and ensure the shape [batch_size, 1]\n",
    "    label_data = torch.stack(label_data)  # Convert list of tensors to a single tensor\n",
    "\n",
    "    return imdbids, text_data, audio_data, video_data_padded, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text feature vectors loaded: 1353\n",
      "Number of audio feature vectors loaded: 1353\n",
      "Number of video feature vectors loaded: 1353\n",
      "Train label distribution: Label\n",
      "green    693\n",
      "red      234\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "Test label distribution: Label\n",
      "green    149\n",
      "red       50\n",
      "Name: count, dtype: int64\n",
      "Train DataLoader: Total Samples = 927, Number of Batches = 29\n",
      "Validation DataLoader: Total Samples = 199, Number of Batches = 13\n",
      "Test DataLoader: Total Samples = 199, Number of Batches = 13\n"
     ]
    }
   ],
   "source": [
    "# Load the labels DataFrame\n",
    "id_label_df = pd.read_excel('../../misc/MM-Trailer_dataset.xlsx')\n",
    "\n",
    "# Define the directories\n",
    "text_features_dir = '/Users/david/Documents/THESIS/DATA/TEXT'\n",
    "audio_features_dir = '/Users/david/Documents/THESIS/DATA/AUDIO'\n",
    "video_features_dir = '/Users/david/Documents/THESIS/DATA/VIDEO'\n",
    "\n",
    "# Load the feature vectors from each directory\n",
    "text_features = load_npy_files(text_features_dir)\n",
    "audio_features = load_npy_files(audio_features_dir)\n",
    "video_features = load_npy_files(video_features_dir)\n",
    "\n",
    "print(f\"Number of text feature vectors loaded: {len(text_features)}\")\n",
    "print(f\"Number of audio feature vectors loaded: {len(audio_features)}\")\n",
    "print(f\"Number of video feature vectors loaded: {len(video_features)}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "id_label_df = id_label_df.drop(columns=['Movie Title', 'URL'])\n",
    "\n",
    "# Create dataset with outliers removed\n",
    "full_dataset = MultimodalDataset(id_label_df, text_features, audio_features, video_features)\n",
    "\n",
    "# perform train-test split on the filtered DataFrame\n",
    "train_df, val_test_df = train_test_split(\n",
    "    full_dataset.id_label_df, test_size=0.3, random_state=42, stratify=full_dataset.id_label_df['Label'])\n",
    "\n",
    "# Further splitting remaining set into validation and test sets\n",
    "val_df, test_df = train_test_split(\n",
    "    val_test_df, test_size=0.5, random_state=42, stratify=val_test_df['Label'])\n",
    "\n",
    "print(\"Train label distribution:\", train_df['Label'].value_counts())\n",
    "print(\"Validation label distribution:\", val_df['Label'].value_counts())\n",
    "print(\"Test label distribution:\", test_df['Label'].value_counts())\n",
    "\n",
    "# Create datasets based on these splits\n",
    "train_dataset = MultimodalDataset(train_df, text_features, audio_features, video_features)\n",
    "val_dataset = MultimodalDataset(val_df, text_features, audio_features, video_features)\n",
    "test_dataset = MultimodalDataset(test_df, text_features, audio_features, video_features)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Function to calculate and print the size of each DataLoader\n",
    "def print_dataloader_sizes(dataloader, name):\n",
    "    total_samples = len(dataloader.dataset)  # Get the size of the dataset\n",
    "    num_batches = len(dataloader)  # Get the number of batches\n",
    "    print(f\"{name} DataLoader: Total Samples = {total_samples}, Number of Batches = {num_batches}\")\n",
    "\n",
    "# Print sizes of each DataLoader\n",
    "print_dataloader_sizes(train_dataloader, \"Train\")\n",
    "print_dataloader_sizes(val_dataloader, \"Validation\")\n",
    "print_dataloader_sizes(test_dataloader, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Functions for Indiv Modality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, input_size=1024):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_text_model(model, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imdbids, text_features, _, _, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(text_features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text_model(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for imdbids, text_features, _, _, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(text_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Text Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_model(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for imdbids, text_features, _, _, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(text_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"Text Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: No gradient for: fc1.weight\n",
      "model: No gradient for: fc1.bias\n",
      "model: No gradient for: fc2.weight\n",
      "model: No gradient for: fc2.bias\n",
      "\n",
      "Training Text Model...\n",
      "----------------------------------------\n",
      "Epoch 1/10\n",
      "Text Model Evaluation - Loss: 0.9480, Recall: 0.8600, Precision: 0.3772, F1: 0.5244\n",
      "Training Loss: 0.9990, Validation Loss: 0.9480\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Text Model Evaluation - Loss: 0.8899, Recall: 0.8000, Precision: 0.4706, F1: 0.5926\n",
      "Training Loss: 0.9052, Validation Loss: 0.8899\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Text Model Evaluation - Loss: 0.8552, Recall: 0.6800, Precision: 0.5484, F1: 0.6071\n",
      "Training Loss: 0.8376, Validation Loss: 0.8552\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Text Model Evaluation - Loss: 0.8084, Recall: 0.7600, Precision: 0.4935, F1: 0.5984\n",
      "Training Loss: 0.7812, Validation Loss: 0.8084\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Text Model Evaluation - Loss: 0.8037, Recall: 0.6600, Precision: 0.5690, F1: 0.6111\n",
      "Training Loss: 0.7280, Validation Loss: 0.8037\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Text Model Evaluation - Loss: 0.7961, Recall: 0.6800, Precision: 0.5763, F1: 0.6239\n",
      "Training Loss: 0.6980, Validation Loss: 0.7961\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Text Model Evaluation - Loss: 0.7645, Recall: 0.6600, Precision: 0.5789, F1: 0.6168\n",
      "Training Loss: 0.6594, Validation Loss: 0.7645\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Text Model Evaluation - Loss: 0.8064, Recall: 0.5800, Precision: 0.6591, F1: 0.6170\n",
      "Training Loss: 0.6331, Validation Loss: 0.8064\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Text Model Evaluation - Loss: 0.7541, Recall: 0.8800, Precision: 0.4583, F1: 0.6027\n",
      "Training Loss: 0.6235, Validation Loss: 0.7541\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Text Model Evaluation - Loss: 0.7222, Recall: 0.7600, Precision: 0.5352, F1: 0.6281\n",
      "Training Loss: 0.5961, Validation Loss: 0.7222\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "Text Model Test - Loss: 0.7366, Test Recall: 0.7000, Test Precision: 0.5224, Test F1: 0.5983\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Define models for each modality\n",
    "    text_model = TextModel()\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "\n",
    "    for name, param in text_model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            print(\"model:\", \"No gradient for:\", name)\n",
    "            \n",
    "    # Train each modality\n",
    "    optimizer = optim.Adam(text_model.parameters(), lr=0.0001)\n",
    "    num_epochs = 10\n",
    "\n",
    "    print(\"\\nTraining Text Model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_text_model(text_model, train_dataloader, criterion, optimizer, device='cpu')\n",
    "\n",
    "        # Validate step\n",
    "        val_loss, precision, recall, f1 = evaluate_text_model(text_model, val_dataloader, criterion, device='cpu')\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Testing the model\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_text_model(text_model, test_dataloader, criterion, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUDIO MODEL ONLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)  # Input size should match the last token dimension\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 1)    # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x has shape [batch_size, 1, 197, 768]\n",
    "\n",
    "        # Remove the extra dimension\n",
    "        x = x.squeeze(1)  # Shape should now be [batch_size, 197, 768]\n",
    "\n",
    "        # Select only the last token\n",
    "        x = x[:, -1, :]  # Shape should now be [batch_size, 768]\n",
    "        \n",
    "        # First fully connected layer\n",
    "        x = self.relu(self.fc1(x))  # Shape should be [batch_size, 512]\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.fc2(x)  # Shape should be [batch_size, 1]\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_audio_model(model, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imdbids, _, audio_features, _, labels in dataloader:  # Only use text data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(audio_features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_audio_model(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for imdbids, _, audio_features, _, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(audio_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Audio Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_audio_model(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for imdbids, _, audio_features, _, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(audio_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"Audio Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: No gradient for: fc1.weight\n",
      "model: No gradient for: fc1.bias\n",
      "model: No gradient for: fc2.weight\n",
      "model: No gradient for: fc2.bias\n",
      "\n",
      "Training on Audio Model...\n",
      "----------------------------------------\n",
      "Epoch 1/10\n",
      "Audio Model Evaluation - Loss: 0.9953, Recall: 0.5400, Precision: 0.5192, F1: 0.5294\n",
      "Training Loss: 1.0183, Validation Loss: 0.9953\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Audio Model Evaluation - Loss: 0.9441, Recall: 0.7600, Precision: 0.4130, F1: 0.5352\n",
      "Training Loss: 0.9782, Validation Loss: 0.9441\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Audio Model Evaluation - Loss: 0.8988, Recall: 0.6000, Precision: 0.5000, F1: 0.5455\n",
      "Training Loss: 0.9455, Validation Loss: 0.8988\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Audio Model Evaluation - Loss: 0.8898, Recall: 0.7600, Precision: 0.4471, F1: 0.5630\n",
      "Training Loss: 0.9155, Validation Loss: 0.8898\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Audio Model Evaluation - Loss: 0.8739, Recall: 0.8200, Precision: 0.4020, F1: 0.5395\n",
      "Training Loss: 0.8892, Validation Loss: 0.8739\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Audio Model Evaluation - Loss: 0.8779, Recall: 0.8200, Precision: 0.4059, F1: 0.5430\n",
      "Training Loss: 0.8760, Validation Loss: 0.8779\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Audio Model Evaluation - Loss: 0.8354, Recall: 0.6200, Precision: 0.5082, F1: 0.5586\n",
      "Training Loss: 0.8423, Validation Loss: 0.8354\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Audio Model Evaluation - Loss: 0.8215, Recall: 0.7000, Precision: 0.4861, F1: 0.5738\n",
      "Training Loss: 0.8290, Validation Loss: 0.8215\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Audio Model Evaluation - Loss: 0.8335, Recall: 0.7400, Precision: 0.4405, F1: 0.5522\n",
      "Training Loss: 0.8154, Validation Loss: 0.8335\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Audio Model Evaluation - Loss: 0.8200, Recall: 0.8200, Precision: 0.4316, F1: 0.5655\n",
      "Training Loss: 0.7983, Validation Loss: 0.8200\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "Audio Model Test - Loss: 0.8488, Test Recall: 0.8600, Test Precision: 0.4343, Test F1: 0.5772\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Define models for each modality\n",
    "    audio_model = AudioModel()\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "\n",
    "    for name, param in audio_model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            print(\"model:\", \"No gradient for:\", name)\n",
    "            \n",
    "    # Train each modality\n",
    "    optimizer = optim.Adam(audio_model.parameters(), lr=0.0001)\n",
    "    num_epochs = 10\n",
    "\n",
    "    print(\"\\nTraining on Audio Model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_audio_model(audio_model, train_dataloader, criterion, optimizer, device='cpu')\n",
    "\n",
    "        # Validate step\n",
    "        val_loss, precision, recall, f1 = evaluate_audio_model(audio_model, val_dataloader, criterion, device='cpu')\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Testing the model\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_audio_model(audio_model, test_dataloader, criterion, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIDEO ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VideoModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(VideoModel, self).__init__()\n",
    "        \n",
    "        # Define the layers in a simpler structure\n",
    "        self.fc1 = nn.Linear(768, 256)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(256, 1)    # Output layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.mean(dim=1)  # Global Average Pooling over sequence dimension\n",
    "        x = self.relu(self.fc1(x))  # Pass through the first layer with ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout after the first layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x  # Final output shape: [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_video_model(model, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, _, _, video_features, labels in dataloader:  # Only use video data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(video_features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_video_model(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, _, _, video_features, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(video_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Audio Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_video_model(model, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, _, _, video_features, labels in dataloader:  # Only use text data\n",
    "            #text_features, labels = text_features.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(video_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"Video Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: No gradient for: fc1.weight\n",
      "model: No gradient for: fc1.bias\n",
      "model: No gradient for: fc2.weight\n",
      "model: No gradient for: fc2.bias\n",
      "\n",
      "Training on Video Model...\n",
      "----------------------------------------\n",
      "Epoch 1/10\n",
      "Audio Model Evaluation - Loss: 1.0321, Recall: 0.1400, Precision: 0.8750, F1: 0.2414\n",
      "Training Loss: 1.0299, Validation Loss: 1.0321\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Audio Model Evaluation - Loss: 1.0099, Recall: 0.7200, Precision: 0.6316, F1: 0.6729\n",
      "Training Loss: 1.0201, Validation Loss: 1.0099\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Audio Model Evaluation - Loss: 1.0098, Recall: 0.9000, Precision: 0.5488, F1: 0.6818\n",
      "Training Loss: 1.0097, Validation Loss: 1.0098\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Audio Model Evaluation - Loss: 0.9745, Recall: 0.9000, Precision: 0.4839, F1: 0.6294\n",
      "Training Loss: 0.9971, Validation Loss: 0.9745\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Audio Model Evaluation - Loss: 0.9370, Recall: 0.9000, Precision: 0.5625, F1: 0.6923\n",
      "Training Loss: 0.9812, Validation Loss: 0.9370\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Audio Model Evaluation - Loss: 0.9330, Recall: 0.9400, Precision: 0.4947, F1: 0.6483\n",
      "Training Loss: 0.9632, Validation Loss: 0.9330\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Audio Model Evaluation - Loss: 0.8990, Recall: 0.9200, Precision: 0.5349, F1: 0.6765\n",
      "Training Loss: 0.9432, Validation Loss: 0.8990\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Audio Model Evaluation - Loss: 0.8760, Recall: 0.8800, Precision: 0.5867, F1: 0.7040\n",
      "Training Loss: 0.9260, Validation Loss: 0.8760\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Audio Model Evaluation - Loss: 0.8486, Recall: 0.9200, Precision: 0.5610, F1: 0.6970\n",
      "Training Loss: 0.9062, Validation Loss: 0.8486\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Audio Model Evaluation - Loss: 0.8303, Recall: 0.9000, Precision: 0.5625, F1: 0.6923\n",
      "Training Loss: 0.8859, Validation Loss: 0.8303\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "Video Model Test - Loss: 0.9031, Test Recall: 0.8800, Test Precision: 0.4944, Test F1: 0.6331\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Define models for each modality\n",
    "    video_model = VideoModel()\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "\n",
    "    for name, param in video_model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            print(\"model:\", \"No gradient for:\", name)\n",
    "            \n",
    "    # Train each modality\n",
    "    optimizer = optim.Adam(video_model.parameters(), lr=0.0001)\n",
    "    num_epochs = 10\n",
    "\n",
    "    print(\"\\nTraining on Video Model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Ensure you have a dataloader that yields inputs and targets\n",
    "        train_loss = train_video_model(video_model, train_dataloader, criterion, optimizer, device='cpu')\n",
    "\n",
    "        # Validate step\n",
    "        val_loss, precision, recall, f1 = evaluate_video_model(video_model, val_dataloader, criterion, device='cpu')\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Testing the model\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing the model on the test set...\")\n",
    "    test_loss, test_precision, test_recall, test_f1_score = test_video_model(video_model, test_dataloader, criterion, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bimodal Fusion Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiConcatFusionModule(nn.Module):\n",
    "    def __init__(self, common_dim=768, hidden_dim=768):\n",
    "        super(BiConcatFusionModule, self).__init__()\n",
    "        \n",
    "        self.common_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define a fully connected layer to project modalities to common dimension\n",
    "        self.fc = nn.Linear(common_dim * 2, hidden_dim)  # Concatenated output of two modalities\n",
    "    \n",
    "    def project_to_common_dim(self, x, target_dim):\n",
    "        \"\"\"Project input to the common dimension if required.\"\"\"\n",
    "        if x.size(-1) != target_dim:\n",
    "            return nn.Linear(x.size(-1), target_dim)(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, modality1, modality2):\n",
    "        \"\"\"\n",
    "        Accepts two modalities and fuses them.\n",
    "        \n",
    "        Parameters:\n",
    "        modality1 (torch.Tensor): First modality with shape (batch_size, seq_len, features).\n",
    "        modality2 (torch.Tensor): Second modality with shape (batch_size, seq_len, features).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Fused representation.\n",
    "        \"\"\"\n",
    "        # Ensure modalities are projected to the common dimension (768)\n",
    "        modality1 = self.project_to_common_dim(modality1, self.common_dim)\n",
    "        modality2 = self.project_to_common_dim(modality2, self.common_dim)\n",
    "        \n",
    "        # Find the maximum sequence length to handle varying sequence lengths\n",
    "        max_seq_len = max(modality1.size(1), modality2.size(1))\n",
    "\n",
    "        # Pad modalities to the same sequence length\n",
    "        if modality1.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality1.size(1)\n",
    "            modality1 = torch.nn.functional.pad(modality1, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        if modality2.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality2.size(1)\n",
    "            modality2 = torch.nn.functional.pad(modality2, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        # Concatenate the modalities along the feature dimension\n",
    "        combined_output = torch.cat((modality1, modality2), dim=-1)  # Shape: (batch_size, max_seq_len, common_dim * 2)\n",
    "\n",
    "        # Apply fully connected layer to get the fused output\n",
    "        fused_output = self.fc(combined_output)  # Shape: (batch_size, max_seq_len, hidden_dim)\n",
    "        \n",
    "        return combined_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  torch.Size([32, 1, 1024])\n",
      "Audio:  torch.Size([32, 197, 768])\n",
      "Video:  torch.Size([32, 50, 768])\n",
      "Fused Output Shape: torch.Size([32, 197, 1536])\n",
      "Fused Output Shape: torch.Size([32, 197, 1536])\n",
      "Fused Output Shape: torch.Size([32, 50, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Assume we have the following three modality inputs:\n",
    "modality1 = torch.randn(32, 1024)         # Shape: (batch_size, 1, 1024)\n",
    "modality2 = torch.randn(32, 1, 197, 768)       # Shape: (batch_size, seq_len, 768)\n",
    "modality3 = torch.randn(32, 50, 768)        # Shape: (batch_size, frames, 768)\n",
    "\n",
    "modality1 = modality1.unsqueeze(1)\n",
    "modality2 = modality2.squeeze(1)\n",
    "\n",
    "print(\"Text: \", modality1.shape)\n",
    "print(\"Audio: \", modality2.shape)\n",
    "print(\"Video: \", modality3.shape)\n",
    "\n",
    "# Initialize the fusion module\n",
    "fusion_module = BiConcatFusionModule()\n",
    "\n",
    "# Example fusion: Fuse modality2 and modality3\n",
    "fused_output = fusion_module(modality2, modality3)\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")\n",
    "\n",
    "# Example fusion: Fuse modality1 and modality2\n",
    "fused_output = fusion_module(modality1, modality2)\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")\n",
    "\n",
    "# Example fusion: Fuse modality1 and modality2\n",
    "fused_output = fusion_module(modality1, modality3)\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 1536])\n",
      "torch.Size([31, 1, 1024])\n",
      "torch.Size([31, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([31, 197, 1536])\n"
     ]
    }
   ],
   "source": [
    "for _, text_features, audio_features, _, labels in train_dataloader: \n",
    "\n",
    "    print(text_features.unsqueeze(1).shape)\n",
    "    print(audio_features.squeeze(1).shape)\n",
    "    \n",
    "    # Create an instance of the FusionModule\n",
    "    fusion_module = BiConcatFusionModule()\n",
    "\n",
    "    text_features = text_features.unsqueeze(1)\n",
    "    audio_features = audio_features.squeeze(1)\n",
    "\n",
    "    # Forward pass through the fusion module\n",
    "    combined_output = fusion_module(audio_features, text_features)\n",
    "\n",
    "    print('Fused AlphaBeta: ', combined_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiConcatClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1536, hidden_dim=768, dropout_prob=0.2):\n",
    "        super(BiConcatClassifier, self).__init__()\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Third hidden layer (slightly smaller)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_biconcat_model(model, classifier, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, text_features, audio_features, video_features, labels in dataloader:  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            combined_output = fusion_module(text_features, audio_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biconcat_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Get the combined output from the fusion module\n",
    "            combined_output = model(text_features, audio_features)  \n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            # Get logits from the classifier\n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Alpha-Beta Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}. F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_biconcat_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "            \n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Forward pass through the model\n",
    "            combined_output = model(text_features, audio_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "\n",
    "            logits = classifier(pooled_output)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to obtain probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"AlphaBeta Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test Accuracy: {accuracy:.4f},Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Biconcat Model...\n",
      "----------------------------------------\n",
      "Epoch 1/15\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0188, Recall: 0.6600, Precision: 0.3882, Accuracy: 0.6533. F1: 0.4889\n",
      "Training Loss: 1.0286, Validation Loss: 1.0188\n",
      "----------------------------------------\n",
      "Epoch 2/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.9601, Recall: 0.7800, Precision: 0.3900, Accuracy: 0.6382. F1: 0.5200\n",
      "Training Loss: 1.0042, Validation Loss: 0.9601\n",
      "----------------------------------------\n",
      "Epoch 3/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8819, Recall: 0.5600, Precision: 0.4828, Accuracy: 0.7387. F1: 0.5185\n",
      "Training Loss: 0.9452, Validation Loss: 0.8819\n",
      "----------------------------------------\n",
      "Epoch 4/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8597, Recall: 0.5600, Precision: 0.5385, Accuracy: 0.7688. F1: 0.5490\n",
      "Training Loss: 0.8861, Validation Loss: 0.8597\n",
      "----------------------------------------\n",
      "Epoch 5/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8734, Recall: 0.6000, Precision: 0.4839, Accuracy: 0.7387. F1: 0.5357\n",
      "Training Loss: 0.8923, Validation Loss: 0.8734\n",
      "----------------------------------------\n",
      "Epoch 6/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8678, Recall: 0.7600, Precision: 0.4086, Accuracy: 0.6633. F1: 0.5315\n",
      "Training Loss: 0.8454, Validation Loss: 0.8678\n",
      "----------------------------------------\n",
      "Epoch 7/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8297, Recall: 0.7400, Precision: 0.4253, Accuracy: 0.6834. F1: 0.5401\n",
      "Training Loss: 0.8270, Validation Loss: 0.8297\n",
      "----------------------------------------\n",
      "Epoch 8/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.9136, Recall: 0.5000, Precision: 0.5435, Accuracy: 0.7688. F1: 0.5208\n",
      "Training Loss: 0.8247, Validation Loss: 0.9136\n",
      "----------------------------------------\n",
      "Epoch 9/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8586, Recall: 0.8000, Precision: 0.3846, Accuracy: 0.6281. F1: 0.5195\n",
      "Training Loss: 0.8015, Validation Loss: 0.8586\n",
      "----------------------------------------\n",
      "Epoch 10/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8671, Recall: 0.6600, Precision: 0.4714, Accuracy: 0.7286. F1: 0.5500\n",
      "Training Loss: 0.7771, Validation Loss: 0.8671\n",
      "----------------------------------------\n",
      "Epoch 11/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8249, Recall: 0.7600, Precision: 0.4043, Accuracy: 0.6583. F1: 0.5278\n",
      "Training Loss: 0.7558, Validation Loss: 0.8249\n",
      "----------------------------------------\n",
      "Epoch 12/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8368, Recall: 0.7000, Precision: 0.4730, Accuracy: 0.7286. F1: 0.5645\n",
      "Training Loss: 0.7749, Validation Loss: 0.8368\n",
      "----------------------------------------\n",
      "Epoch 13/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8550, Recall: 0.7200, Precision: 0.4737, Accuracy: 0.7286. F1: 0.5714\n",
      "Training Loss: 0.7560, Validation Loss: 0.8550\n",
      "----------------------------------------\n",
      "Epoch 14/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8544, Recall: 0.8000, Precision: 0.3846, Accuracy: 0.6281. F1: 0.5195\n",
      "Training Loss: 0.7332, Validation Loss: 0.8544\n",
      "----------------------------------------\n",
      "Epoch 15/15\n",
      "Alpha-Beta Model Evaluation - Loss: 0.8331, Recall: 0.7600, Precision: 0.4270, Accuracy: 0.6834. F1: 0.5468\n",
      "Training Loss: 0.7379, Validation Loss: 0.8331\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "AlphaBeta Model Test - Loss: 0.8280, Test Recall: 0.7400, Test Precision: 0.4684, Test Accuracy: 0.7236,Test F1: 0.5736\n",
      "Test Results:\n",
      "Loss: 0.8280\n",
      "Precision: 0.4684\n",
      "Recall: 0.7400\n",
      "Accuracy: 0.7236\n",
      "F1 Score: 0.5736\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     torch.manual_seed(42)\n",
    "\n",
    "     # Initialize models\n",
    "     fusion_model = BiConcatFusionModule()\n",
    "     classifier = BiConcatClassifier()\n",
    "\n",
    "     # Define loss function and optimizer\n",
    "     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "     optimizer = optim.Adam(\n",
    "          list(classifier.parameters()),\n",
    "         lr=0.0001\n",
    "        )\n",
    "\n",
    "     # Training loop\n",
    "     num_epochs = 15\n",
    "     print(\"\\nTraining Biconcat Model...\")\n",
    "    \n",
    "     for epoch in range(num_epochs):\n",
    "         print(\"-\" * 40)\n",
    "         print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "         # Train step\n",
    "         train_loss = train_biconcat_model(\n",
    "             fusion_model, classifier, train_dataloader, \n",
    "             criterion, optimizer, device='cpu'\n",
    "         )\n",
    "\n",
    "         # Validation step\n",
    "         val_loss, precision, recall, accuracy, f1 = evaluate_biconcat_model(\n",
    "             fusion_model, classifier, val_dataloader, \n",
    "             criterion, device='cpu'\n",
    "         )\n",
    "\n",
    "         print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "     # Final testing\n",
    "     print(\"-\" * 40)\n",
    "     print(\"Testing the model on the test set...\")\n",
    "     test_loss, test_precision, test_recall, test_accuracy, test_f1_score = test_biconcat_model(\n",
    "         fusion_model, classifier, test_dataloader, \n",
    "         criterion, device='cpu'\n",
    "     )\n",
    "\n",
    "     print(f\"Test Results:\")\n",
    "     print(f\"Loss: {test_loss:.4f}\")\n",
    "     print(f\"Precision: {test_precision:.4f}\")\n",
    "     print(f\"Recall: {test_recall:.4f}\")\n",
    "     print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "     print(f\"F1 Score: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiAverageFusionModule(nn.Module):\n",
    "    def __init__(self, common_dim=768, hidden_dim=768):\n",
    "        super(BiAverageFusionModule, self).__init__()\n",
    "        \n",
    "        self.common_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define a fully connected layer to project modalities to common dimension\n",
    "        self.fc = nn.Linear(common_dim, hidden_dim)  # After averaging, only one modality's size\n",
    "\n",
    "    def project_to_common_dim(self, x, target_dim):\n",
    "        \"\"\"Project input to the common dimension if required.\"\"\"\n",
    "        if x.size(-1) != target_dim:\n",
    "            return nn.Linear(x.size(-1), target_dim)(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, modality1, modality2):\n",
    "        \"\"\"\n",
    "        Accepts two modalities and fuses them.\n",
    "        \n",
    "        Parameters:\n",
    "        modality1 (torch.Tensor): First modality with shape (batch_size, seq_len, features).\n",
    "        modality2 (torch.Tensor): Second modality with shape (batch_size, seq_len, features).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Fused representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure modalities are projected to the common dimension (768)\n",
    "        modality1 = self.project_to_common_dim(modality1, self.common_dim)\n",
    "        modality2 = self.project_to_common_dim(modality2, self.common_dim)\n",
    "        \n",
    "        # Find the maximum sequence length to handle varying sequence lengths\n",
    "        max_seq_len = max(modality1.size(1), modality2.size(1))\n",
    "\n",
    "        # Pad modalities to the same sequence length\n",
    "        if modality1.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality1.size(1)\n",
    "            modality1 = torch.nn.functional.pad(modality1, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        if modality2.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality2.size(1)\n",
    "            modality2 = torch.nn.functional.pad(modality2, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        # Average the modalities instead of concatenating them\n",
    "        averaged_output = (modality1 + modality2) / 2  # Shape: (batch_size, max_seq_len, common_dim)\n",
    "\n",
    "        # Apply fully connected layer to get the fused output\n",
    "        fused_output = self.fc(averaged_output)  # Shape: (batch_size, max_seq_len, hidden_dim)\n",
    "        \n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  torch.Size([32, 1, 1024])\n",
      "Audio:  torch.Size([32, 197, 768])\n",
      "Video:  torch.Size([32, 50, 768])\n",
      "Fused Output Shape: torch.Size([32, 197, 768])\n",
      "Fused Output Shape: torch.Size([32, 197, 768])\n",
      "Fused Output Shape: torch.Size([32, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Assume we have the following three modality inputs:\n",
    "modality1 = torch.randn(32, 1024)         # Shape: (batch_size, 1, 1024)\n",
    "modality2 = torch.randn(32, 1, 197, 768)       # Shape: (batch_size, seq_len, 768)\n",
    "modality3 = torch.randn(32, 50, 768)        # Shape: (batch_size, frames, 768)\n",
    "\n",
    "modality1 = modality1.unsqueeze(1)\n",
    "modality2 = modality2.squeeze(1)\n",
    "\n",
    "print(\"Text: \", modality1.shape)\n",
    "print(\"Audio: \", modality2.shape)\n",
    "print(\"Video: \", modality3.shape)\n",
    "\n",
    "# Initialize the fusion module\n",
    "fusion_module = BiAverageFusionModule()\n",
    "\n",
    "# Example fusion: Fuse modality2 and modality3\n",
    "fused_output = fusion_module(modality2, modality3)\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")\n",
    "\n",
    "# Example fusion: Fuse modality1 and modality2\n",
    "fused_output = fusion_module(modality1, modality2)\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")\n",
    "\n",
    "# Example fusion: Fuse modality1 and modality2\n",
    "fused_output = fusion_module(modality1, modality3)\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([31, 1, 1024])\n",
      "torch.Size([31, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([31, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "for _, text_features, audio_features, _, labels in train_dataloader: \n",
    "\n",
    "    print(text_features.unsqueeze(1).shape)\n",
    "    print(audio_features.squeeze(1).shape)\n",
    "    \n",
    "    # Create an instance of the FusionModule\n",
    "    fusion_module = BiAverageFusionModule()\n",
    "\n",
    "    text_features = text_features.unsqueeze(1)\n",
    "    audio_features = audio_features.squeeze(1)\n",
    "\n",
    "    # Forward pass through the fusion module\n",
    "    combined_output = fusion_module(audio_features, text_features)\n",
    "\n",
    "    print('Fused AlphaBeta: ', combined_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiAverageClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=382, dropout_prob=0.2):\n",
    "        super(BiAverageClassifier, self).__init__()\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Third hidden layer (slightly smaller)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_biavg_model(model, classifier, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, text_features, audio_features, video_features, labels in dataloader:  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            combined_output = fusion_module(text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biavg_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Get the combined output from the fusion module\n",
    "            combined_output = model(text_features, video_features)  \n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            # Get logits from the classifier\n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Alpha-Beta Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}. F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_biavg_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "            \n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Forward pass through the model\n",
    "            combined_output = model(text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "\n",
    "            logits = classifier(pooled_output)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to obtain probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"AlphaBeta Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test Accuracy: {accuracy:.4f},Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Biaverage Model...\n",
      "----------------------------------------\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha-Beta Model Evaluation - Loss: 1.0244, Recall: 0.0000, Precision: 0.0000, Accuracy: 0.7487. F1: 0.0000\n",
      "Training Loss: 1.0374, Validation Loss: 1.0244\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0328, Recall: 0.0400, Precision: 0.5000, Accuracy: 0.7487. F1: 0.0741\n",
      "Training Loss: 1.0306, Validation Loss: 1.0328\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0350, Recall: 0.1000, Precision: 0.3333, Accuracy: 0.7236. F1: 0.1538\n",
      "Training Loss: 0.9886, Validation Loss: 1.0350\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0281, Recall: 0.4000, Precision: 0.2597, Accuracy: 0.5628. F1: 0.3150\n",
      "Training Loss: 0.8787, Validation Loss: 1.0281\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0559, Recall: 0.4400, Precision: 0.2268, Accuracy: 0.4824. F1: 0.2993\n",
      "Training Loss: 0.7576, Validation Loss: 1.0559\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0735, Recall: 0.6600, Precision: 0.2426, Accuracy: 0.3970. F1: 0.3548\n",
      "Training Loss: 0.7303, Validation Loss: 1.0735\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.0798, Recall: 0.7400, Precision: 0.2450, Accuracy: 0.3618. F1: 0.3682\n",
      "Training Loss: 0.6657, Validation Loss: 1.0798\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.1284, Recall: 0.7600, Precision: 0.2375, Accuracy: 0.3266. F1: 0.3619\n",
      "Training Loss: 0.6365, Validation Loss: 1.1284\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.1947, Recall: 0.8800, Precision: 0.2444, Accuracy: 0.2864. F1: 0.3826\n",
      "Training Loss: 0.6021, Validation Loss: 1.1947\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Alpha-Beta Model Evaluation - Loss: 1.2674, Recall: 0.9200, Precision: 0.2434, Accuracy: 0.2613. F1: 0.3849\n",
      "Training Loss: 0.6252, Validation Loss: 1.2674\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "AlphaBeta Model Test - Loss: 1.2035, Test Recall: 0.9200, Test Precision: 0.2486, Test Accuracy: 0.2814,Test F1: 0.3915\n",
      "Test Results:\n",
      "Loss: 1.2035\n",
      "Precision: 0.2486\n",
      "Recall: 0.9200\n",
      "Accuracy: 0.2814\n",
      "F1 Score: 0.3915\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     torch.manual_seed(42)\n",
    "\n",
    "     # Initialize models\n",
    "     fusion_model = BiAverageFusionModule()\n",
    "     classifier = BiAverageClassifier()\n",
    "\n",
    "     # Define loss function and optimizer\n",
    "     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "     optimizer = optim.Adam(\n",
    "          list(classifier.parameters()),\n",
    "         lr=0.001\n",
    "        )\n",
    "\n",
    "     # Training loop\n",
    "     num_epochs = 10\n",
    "     print(\"\\nTraining Biaverage Model...\")\n",
    "    \n",
    "     for epoch in range(num_epochs):\n",
    "         print(\"-\" * 40)\n",
    "         print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "         # Train step\n",
    "         train_loss = train_biavg_model(\n",
    "             fusion_model, classifier, train_dataloader, \n",
    "             criterion, optimizer, device='cpu'\n",
    "         )\n",
    "\n",
    "         # Validation step\n",
    "         val_loss, precision, recall, accuracy, f1 = evaluate_biavg_model(\n",
    "             fusion_model, classifier, val_dataloader, \n",
    "             criterion, device='cpu'\n",
    "         )\n",
    "\n",
    "         print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "     # Final testing\n",
    "     print(\"-\" * 40)\n",
    "     print(\"Testing the model on the test set...\")\n",
    "     test_loss, test_precision, test_recall, test_accuracy, test_f1_score = test_biavg_model(\n",
    "         fusion_model, classifier, test_dataloader, \n",
    "         criterion, device='cpu'\n",
    "     )\n",
    "\n",
    "     print(f\"Test Results:\")\n",
    "     print(f\"Loss: {test_loss:.4f}\")\n",
    "     print(f\"Precision: {test_precision:.4f}\")\n",
    "     print(f\"Recall: {test_recall:.4f}\")\n",
    "     print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "     print(f\"F1 Score: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TriConcatFusionModule(nn.Module):\n",
    "    def __init__(self, common_dim=768, hidden_dim=768):\n",
    "        super(TriConcatFusionModule, self).__init__()\n",
    "        \n",
    "        self.common_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define a fully connected layer to project modalities to common dimension\n",
    "        self.fc = nn.Linear(common_dim * 3, hidden_dim)  # Concatenated output of three modalities\n",
    "    \n",
    "    def project_to_common_dim(self, x, target_dim):\n",
    "        \"\"\"Project input to the common dimension if required.\"\"\"\n",
    "        if x.size(-1) != target_dim:\n",
    "            return nn.Linear(x.size(-1), target_dim)(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, modality1, modality2, modality3):\n",
    "        \"\"\"\n",
    "        Accepts three modalities and fuses them.\n",
    "        \n",
    "        Parameters:\n",
    "        modality1 (torch.Tensor): First modality with shape (batch_size, seq_len, features).\n",
    "        modality2 (torch.Tensor): Second modality with shape (batch_size, seq_len, features).\n",
    "        modality3 (torch.Tensor): Third modality with shape (batch_size, seq_len, features).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Fused representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure modalities are projected to the common dimension (768)\n",
    "        modality1 = self.project_to_common_dim(modality1, self.common_dim)\n",
    "        modality2 = self.project_to_common_dim(modality2, self.common_dim)\n",
    "        modality3 = self.project_to_common_dim(modality3, self.common_dim)\n",
    "        \n",
    "        # Find the maximum sequence length to handle varying sequence lengths\n",
    "        max_seq_len = max(modality1.size(1), modality2.size(1), modality3.size(1))\n",
    "\n",
    "        # Pad modalities to the same sequence length\n",
    "        if modality1.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality1.size(1)\n",
    "            modality1 = torch.nn.functional.pad(modality1, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        if modality2.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality2.size(1)\n",
    "            modality2 = torch.nn.functional.pad(modality2, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        if modality3.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality3.size(1)\n",
    "            modality3 = torch.nn.functional.pad(modality3, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        # Concatenate the modalities along the feature dimension\n",
    "        combined_output = torch.cat((modality1, modality2, modality3), dim=-1)  # Shape: (batch_size, max_seq_len, common_dim * 3)\n",
    "\n",
    "        # Apply fully connected layer to get the fused output\n",
    "        fused_output = self.fc(combined_output)  # Shape: (batch_size, max_seq_len, hidden_dim)\n",
    "        \n",
    "        return combined_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused Output Shape: torch.Size([32, 12, 2304])\n"
     ]
    }
   ],
   "source": [
    "# Example input tensors for modality1 (e.g., text), modality2 (e.g., audio), and modality3 (e.g., video)\n",
    "batch_size = 32\n",
    "seq_len_1 = 10  # Length of sequence for modality 1\n",
    "seq_len_2 = 12  # Length of sequence for modality 2\n",
    "seq_len_3 = 8   # Length of sequence for modality 3\n",
    "common_dim = 768  # Common feature dimension for all modalities\n",
    "\n",
    "# Creating random example tensors for the modalities\n",
    "modality1 = torch.randn(batch_size, seq_len_1, common_dim)  # Shape: (batch_size, seq_len_1, common_dim)\n",
    "modality2 = torch.randn(batch_size, seq_len_2, common_dim)  # Shape: (batch_size, seq_len_2, common_dim)\n",
    "modality3 = torch.randn(batch_size, seq_len_3, common_dim)  # Shape: (batch_size, seq_len_3, common_dim)\n",
    "\n",
    "# Instantiate the fusion module\n",
    "fusion_module = TriConcatFusionModule()\n",
    "\n",
    "# Forward pass through the fusion module\n",
    "fused_output = fusion_module(modality1, modality2, modality3)\n",
    "\n",
    "# Print the shape of the fused output\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 2304])\n",
      "torch.Size([31, 1, 1024])\n",
      "torch.Size([31, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([31, 197, 2304])\n"
     ]
    }
   ],
   "source": [
    "for _, text_features, audio_features, video_features, labels in train_dataloader: \n",
    "\n",
    "    print(text_features.unsqueeze(1).shape)\n",
    "    print(audio_features.squeeze(1).shape)\n",
    "    \n",
    "    # Create an instance of the FusionModule\n",
    "    fusion_module = TriConcatFusionModule()\n",
    "\n",
    "    text_features = text_features.unsqueeze(1)\n",
    "    audio_features = audio_features.squeeze(1)\n",
    "\n",
    "    # Forward pass through the fusion module\n",
    "    combined_output = fusion_module(audio_features, text_features, video_features)\n",
    "\n",
    "    print('Fused AlphaBeta: ', combined_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TriConcatClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=2304, hidden_dim=1152, dropout_prob=0.2):\n",
    "        super(TriConcatClassifier, self).__init__()\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Third hidden layer (slightly smaller)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triconcat_model(model, classifier, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, text_features, audio_features, video_features, labels in dataloader:  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            combined_output = fusion_module(audio_features, text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_triconcat_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Get the combined output from the fusion module\n",
    "            combined_output = fusion_module(audio_features, text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            # Get logits from the classifier\n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Alpha-Beta Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}. F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_triconcat_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "            \n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Get the combined output from the fusion module\n",
    "            combined_output = fusion_module(audio_features, text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "\n",
    "            logits = classifier(pooled_output)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to obtain probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"AlphaBeta Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test Accuracy: {accuracy:.4f},Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Biaverage Model...\n",
      "----------------------------------------\n",
      "Epoch 1/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.7425, Recall: 0.7800, Precision: 0.5065, Accuracy: 0.7538. F1: 0.6142\n",
      "Training Loss: 0.9662, Validation Loss: 0.7425\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.5955, Recall: 0.6800, Precision: 0.7727, Accuracy: 0.8693. F1: 0.7234\n",
      "Training Loss: 0.7961, Validation Loss: 0.5955\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.6330, Recall: 0.9400, Precision: 0.4563, Accuracy: 0.7035. F1: 0.6144\n",
      "Training Loss: 0.7048, Validation Loss: 0.6330\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.5815, Recall: 0.9400, Precision: 0.5000, Accuracy: 0.7487. F1: 0.6528\n",
      "Training Loss: 0.6464, Validation Loss: 0.5815\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.7178, Recall: 0.6000, Precision: 0.9091, Accuracy: 0.8844. F1: 0.7229\n",
      "Training Loss: 0.5620, Validation Loss: 0.7178\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.5748, Recall: 0.7400, Precision: 0.7115, Accuracy: 0.8593. F1: 0.7255\n",
      "Training Loss: 0.5493, Validation Loss: 0.5748\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.5587, Recall: 0.9200, Precision: 0.5823, Accuracy: 0.8141. F1: 0.7132\n",
      "Training Loss: 0.5004, Validation Loss: 0.5587\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.5564, Recall: 0.7600, Precision: 0.8085, Accuracy: 0.8945. F1: 0.7835\n",
      "Training Loss: 0.5462, Validation Loss: 0.5564\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.5247, Recall: 0.8200, Precision: 0.6833, Accuracy: 0.8593. F1: 0.7455\n",
      "Training Loss: 0.4159, Validation Loss: 0.5247\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Alpha-Beta Model Evaluation - Loss: 0.7017, Recall: 0.8200, Precision: 0.7593, Accuracy: 0.8894. F1: 0.7885\n",
      "Training Loss: 0.3650, Validation Loss: 0.7017\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "AlphaBeta Model Test - Loss: 0.7017, Test Recall: 0.7600, Test Precision: 0.6552, Test Accuracy: 0.8392,Test F1: 0.7037\n",
      "Test Results:\n",
      "Loss: 0.7017\n",
      "Precision: 0.6552\n",
      "Recall: 0.7600\n",
      "Accuracy: 0.8392\n",
      "F1 Score: 0.7037\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     torch.manual_seed(42)\n",
    "\n",
    "     # Initialize models\n",
    "     fusion_model = TriConcatFusionModule()\n",
    "     classifier = TriConcatClassifier()\n",
    "\n",
    "     # Define loss function and optimizer\n",
    "     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "     optimizer = optim.Adam(\n",
    "          list(classifier.parameters()),\n",
    "         lr=0.001\n",
    "        )\n",
    "\n",
    "     # Training loop\n",
    "     num_epochs = 10\n",
    "     print(\"\\nTraining Biaverage Model...\")\n",
    "    \n",
    "     for epoch in range(num_epochs):\n",
    "         print(\"-\" * 40)\n",
    "         print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "         # Train step\n",
    "         train_loss = train_triconcat_model(\n",
    "             fusion_model, classifier, train_dataloader, \n",
    "             criterion, optimizer, device='cpu'\n",
    "         )\n",
    "\n",
    "         # Validation step\n",
    "         val_loss, precision, recall, accuracy, f1 = evaluate_triconcat_model(\n",
    "             fusion_model, classifier, val_dataloader, \n",
    "             criterion, device='cpu'\n",
    "         )\n",
    "\n",
    "         print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "     # Final testing\n",
    "     print(\"-\" * 40)\n",
    "     print(\"Testing the model on the test set...\")\n",
    "     test_loss, test_precision, test_recall, test_accuracy, test_f1_score = test_triconcat_model(\n",
    "         fusion_model, classifier, test_dataloader, \n",
    "         criterion, device='cpu'\n",
    "     )\n",
    "\n",
    "     print(f\"Test Results:\")\n",
    "     print(f\"Loss: {test_loss:.4f}\")\n",
    "     print(f\"Precision: {test_precision:.4f}\")\n",
    "     print(f\"Recall: {test_recall:.4f}\")\n",
    "     print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "     print(f\"F1 Score: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TriAverageFusionModule(nn.Module):\n",
    "    def __init__(self, common_dim=768, hidden_dim=768):\n",
    "        super(TriAverageFusionModule, self).__init__()\n",
    "        \n",
    "        self.common_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define a fully connected layer to project modalities to the hidden dimension\n",
    "        self.fc = nn.Linear(common_dim, hidden_dim)  # Output will be projected to hidden_dim\n",
    "    \n",
    "    def project_to_common_dim(self, x, target_dim):\n",
    "        \"\"\"Project input to the common dimension if required.\"\"\"\n",
    "        if x.size(-1) != target_dim:\n",
    "            return nn.Linear(x.size(-1), target_dim)(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, modality1, modality2, modality3):\n",
    "        \"\"\"\n",
    "        Accepts three modalities and fuses them by averaging.\n",
    "        \n",
    "        Parameters:\n",
    "        modality1 (torch.Tensor): First modality with shape (batch_size, seq_len, features).\n",
    "        modality2 (torch.Tensor): Second modality with shape (batch_size, seq_len, features).\n",
    "        modality3 (torch.Tensor): Third modality with shape (batch_size, seq_len, features).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Fused representation after averaging and projection.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure modalities are projected to the common dimension (768)\n",
    "        modality1 = self.project_to_common_dim(modality1, self.common_dim)\n",
    "        modality2 = self.project_to_common_dim(modality2, self.common_dim)\n",
    "        modality3 = self.project_to_common_dim(modality3, self.common_dim)\n",
    "        \n",
    "        # Find the maximum sequence length to handle varying sequence lengths\n",
    "        max_seq_len = max(modality1.size(1), modality2.size(1), modality3.size(1))\n",
    "        \n",
    "        # Pad modalities to the same sequence length\n",
    "        if modality1.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality1.size(1)\n",
    "            modality1 = torch.nn.functional.pad(modality1, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        if modality2.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality2.size(1)\n",
    "            modality2 = torch.nn.functional.pad(modality2, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        if modality3.size(1) < max_seq_len:\n",
    "            pad_size = max_seq_len - modality3.size(1)\n",
    "            modality3 = torch.nn.functional.pad(modality3, (0, 0, 0, pad_size))  # Pad on the sequence dimension\n",
    "        \n",
    "        # Average the three modalities along the feature dimension\n",
    "        combined_output = (modality1 + modality2 + modality3) / 3  # Shape: (batch_size, max_seq_len, common_dim)\n",
    "        \n",
    "        # Apply fully connected layer to get the fused output\n",
    "        fused_output = self.fc(combined_output)  # Shape: (batch_size, max_seq_len, hidden_dim)\n",
    "        \n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused Output Shape: torch.Size([32, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example input tensors for modality1 (e.g., text), modality2 (e.g., audio), and modality3 (e.g., video)\n",
    "batch_size = 32\n",
    "seq_len_1 = 10  # Length of sequence for modality 1\n",
    "seq_len_2 = 12  # Length of sequence for modality 2\n",
    "seq_len_3 = 8   # Length of sequence for modality 3\n",
    "common_dim = 768  # Common feature dimension for all modalities\n",
    "\n",
    "# Creating random example tensors for the modalities\n",
    "modality1 = torch.randn(batch_size, seq_len_1, common_dim)  # Shape: (batch_size, seq_len_1, common_dim)\n",
    "modality2 = torch.randn(batch_size, seq_len_2, common_dim)  # Shape: (batch_size, seq_len_2, common_dim)\n",
    "modality3 = torch.randn(batch_size, seq_len_3, common_dim)  # Shape: (batch_size, seq_len_3, common_dim)\n",
    "\n",
    "# Instantiate the fusion module\n",
    "fusion_module = TriAverageFusionModule()\n",
    "\n",
    "# Forward pass through the fusion module\n",
    "fused_output = fusion_module(modality1, modality2, modality3)\n",
    "\n",
    "# Print the shape of the fused output\n",
    "print(f\"Fused Output Shape: {fused_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([32, 1, 1024])\n",
      "torch.Size([32, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([32, 197, 768])\n",
      "torch.Size([31, 1, 1024])\n",
      "torch.Size([31, 197, 768])\n",
      "Fused AlphaBeta:  torch.Size([31, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "for _, text_features, audio_features, video_features, labels in train_dataloader: \n",
    "\n",
    "    print(text_features.unsqueeze(1).shape)\n",
    "    print(audio_features.squeeze(1).shape)\n",
    "    \n",
    "    # Create an instance of the FusionModule\n",
    "    fusion_module = TriAverageFusionModule()\n",
    "\n",
    "    text_features = text_features.unsqueeze(1)\n",
    "    audio_features = audio_features.squeeze(1)\n",
    "\n",
    "    # Forward pass through the fusion module\n",
    "    combined_output = fusion_module(audio_features, text_features, video_features)\n",
    "\n",
    "    print('Fused AlphaBeta: ', combined_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TriAverageClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, dropout_prob=0.2):\n",
    "        super(TriAverageClassifier, self).__init__()\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Third hidden layer (slightly smaller)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triavg_model(model, classifier, dataloader, criterion, optimizer, device='cpu'):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, text_features, audio_features, video_features, labels in dataloader:  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            combined_output = fusion_module(audio_features, text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    return avg_loss  # Optionally, return the last average loss if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_triavg_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "\n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Get the combined output from the fusion module\n",
    "            combined_output = model(audio_features, text_features, video_features)  \n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "            \n",
    "            # Get logits from the classifier\n",
    "            logits = classifier(pooled_output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"AlphaBetaGamma Model Evaluation - Loss: {avg_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}. F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_triavg_model(model, classifier, dataloader, criterion, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for _, text_features, audio_features, video_features, labels in dataloader:\n",
    "            \n",
    "            text_features = text_features.unsqueeze(1)\n",
    "            audio_features = audio_features.squeeze(1)\n",
    "            video_features = video_features\n",
    "\n",
    "            # Forward pass through the model\n",
    "            combined_output = model(audio_features, text_features, video_features)\n",
    "\n",
    "            pooled_output = torch.mean(combined_output, dim=1) \n",
    "\n",
    "            logits = classifier(pooled_output)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to obtain probabilities\n",
    "            outputs = torch.sigmoid(logits)\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            all_preds.extend((outputs > 0.5).int().tolist())\n",
    "            all_labels.extend(labels.int().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"AlphaBetaGamma Model Test - Loss: {avg_loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}, Test Accuracy: {accuracy:.4f},Test F1: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training TriAverage Model...\n",
      "----------------------------------------\n",
      "Epoch 1/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.0178, Recall: 1.0000, Precision: 0.2577, Accuracy: 0.2764. F1: 0.4098\n",
      "Training Loss: 1.0283, Validation Loss: 1.0178\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.0571, Recall: 1.0000, Precision: 0.2513, Accuracy: 0.2513. F1: 0.4016\n",
      "Training Loss: 0.9603, Validation Loss: 1.0571\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.0963, Recall: 1.0000, Precision: 0.2525, Accuracy: 0.2563. F1: 0.4032\n",
      "Training Loss: 0.8844, Validation Loss: 1.0963\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.0744, Recall: 1.0000, Precision: 0.2538, Accuracy: 0.2613. F1: 0.4049\n",
      "Training Loss: 0.9005, Validation Loss: 1.0744\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.1240, Recall: 1.0000, Precision: 0.2525, Accuracy: 0.2563. F1: 0.4032\n",
      "Training Loss: 0.8176, Validation Loss: 1.1240\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.1740, Recall: 1.0000, Precision: 0.2564, Accuracy: 0.2714. F1: 0.4082\n",
      "Training Loss: 0.7703, Validation Loss: 1.1740\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.1921, Recall: 1.0000, Precision: 0.2591, Accuracy: 0.2814. F1: 0.4115\n",
      "Training Loss: 0.7120, Validation Loss: 1.1921\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.2652, Recall: 0.9600, Precision: 0.2513, Accuracy: 0.2714. F1: 0.3983\n",
      "Training Loss: 0.6699, Validation Loss: 1.2652\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.2851, Recall: 0.9600, Precision: 0.2513, Accuracy: 0.2714. F1: 0.3983\n",
      "Training Loss: 0.6998, Validation Loss: 1.2851\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "AlphaBetaGamma Model Evaluation - Loss: 1.4204, Recall: 0.9600, Precision: 0.2513, Accuracy: 0.2714. F1: 0.3983\n",
      "Training Loss: 0.6339, Validation Loss: 1.4204\n",
      "----------------------------------------\n",
      "Testing the model on the test set...\n",
      "AlphaBetaGamma Model Test - Loss: 1.4255, Test Recall: 0.9800, Test Precision: 0.2526, Test Accuracy: 0.2663,Test F1: 0.4016\n",
      "Test Results:\n",
      "Loss: 1.4255\n",
      "Precision: 0.2526\n",
      "Recall: 0.9800\n",
      "Accuracy: 0.2663\n",
      "F1 Score: 0.4016\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     torch.manual_seed(42)\n",
    "\n",
    "     # Initialize models\n",
    "     fusion_model = TriAverageFusionModule()\n",
    "     classifier = TriAverageClassifier()\n",
    "\n",
    "     # Define loss function and optimizer\n",
    "     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(2.94))\n",
    "     optimizer = optim.Adam(\n",
    "          list(classifier.parameters()),\n",
    "         lr=0.001\n",
    "        )\n",
    "\n",
    "     # Training loop\n",
    "     num_epochs = 10\n",
    "     print(\"\\nTraining TriAverage Model...\")\n",
    "    \n",
    "     for epoch in range(num_epochs):\n",
    "         print(\"-\" * 40)\n",
    "         print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "         # Train step\n",
    "         train_loss = train_triavg_model(\n",
    "             fusion_model, classifier, train_dataloader, \n",
    "             criterion, optimizer, device='cpu'\n",
    "         )\n",
    "\n",
    "         # Validation step\n",
    "         val_loss, precision, recall, accuracy, f1 = evaluate_triavg_model(\n",
    "             fusion_model, classifier, val_dataloader, \n",
    "             criterion, device='cpu'\n",
    "         )\n",
    "\n",
    "         print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "     # Final testing\n",
    "     print(\"-\" * 40)\n",
    "     print(\"Testing the model on the test set...\")\n",
    "     test_loss, test_precision, test_recall, test_accuracy, test_f1_score = test_triavg_model(\n",
    "         fusion_model, classifier, test_dataloader, \n",
    "         criterion, device='cpu'\n",
    "     )\n",
    "\n",
    "     print(f\"Test Results:\")\n",
    "     print(f\"Loss: {test_loss:.4f}\")\n",
    "     print(f\"Precision: {test_precision:.4f}\")\n",
    "     print(f\"Recall: {test_recall:.4f}\")\n",
    "     print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "     print(f\"F1 Score: {test_f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
